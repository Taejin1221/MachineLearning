{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ResNet_46layers_500Epochs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taejin1221/MachineLearning/blob/master/ResNet/FashionMNIST/ResNet_46layers_500Epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1F9jG3-0TPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "c0b4ada4-d08b-4c5a-97be-13423bdeea5a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmZbJkvP0ckx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('drive/My Drive/Colab Notebooks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iJg6AkLyQ-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        " \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh2roVZhyQ-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKRNI2q2yQ-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input( shape = ( 32, 32, 3 ), name = 'input' )\n",
        "\n",
        "identity = layers.Conv2D( filters = 16, kernel_size = [ 7, 7 ], padding = 'Same', activation = 'relu' )(inputs)\n",
        "\n",
        "# block 1\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 16, kernel_size = [ 3, 3 ], padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 16, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_84Z42LyQ-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block 2\n",
        "identity = layers.ZeroPadding2D( [ 0, 8 ], 'channels_first' )(identity)\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 32, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 32, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18FDc_f9yQ-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block 3\n",
        "identity = layers.ZeroPadding2D( [ 0, 16 ], 'channels_first' )(identity)\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 64, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 64, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgZp3nGByQ-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = layers.GlobalAveragePooling2D()(identity)\n",
        "output = layers.Dense( 128, activation = 'relu' )(output)\n",
        "output = layers.Dense( 128, activation = 'relu' )(output)\n",
        "output = layers.Dense( 10, activation = 'softmax' )(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xeulYwgYyQ-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13a6bf7d-7b75-4d7d-ae07-e12eddf508e2"
      },
      "source": [
        "model = keras.Model( inputs = inputs, outputs = output, name = 'resnet' )\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   2368        input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "                                                                 conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
            "                                                                 activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 16)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 16)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2320        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
            "                                                                 activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 16)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 16)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 16)   2320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 32, 32, 16)   0           batch_normalization_13[0][0]     \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 16)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 16)   0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 16, 16, 32)   0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 32)   9248        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 32)   128         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 32)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 32)   9248        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 32)   128         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 16, 16, 32)   0           batch_normalization_15[0][0]     \n",
            "                                                                 zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 32)   0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 32)   9248        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 32)   128         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 32)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 32)   9248        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 32)   128         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 16, 16, 32)   0           batch_normalization_17[0][0]     \n",
            "                                                                 activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 32)   0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 32)   9248        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 32)   128         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 32)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 32)   9248        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 16, 32)   0           batch_normalization_19[0][0]     \n",
            "                                                                 activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 32)   0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 32)   9248        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 32)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 32)   9248        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 32)   128         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 16, 16, 32)   0           batch_normalization_21[0][0]     \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 32)   0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 32)   9248        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 32)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9248        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 32)   128         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 16, 16, 32)   0           batch_normalization_23[0][0]     \n",
            "                                                                 activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 32)   0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 32)   9248        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 32)   128         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 32)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 32)   9248        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 32)   128         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 16, 16, 32)   0           batch_normalization_25[0][0]     \n",
            "                                                                 activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 32)   0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 32)   9248        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 32)   128         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 32)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 32)   9248        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 32)   128         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 32)   0           batch_normalization_27[0][0]     \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 32)   0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 32)     0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 8, 8, 64)     0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 64)     36928       zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 64)     256         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 64)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 64)     36928       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 64)     256         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 8, 8, 64)     0           batch_normalization_29[0][0]     \n",
            "                                                                 zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 64)     0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 64)     36928       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 64)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 64)     36928       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 64)     256         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 8, 8, 64)     0           batch_normalization_31[0][0]     \n",
            "                                                                 activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 64)     0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 64)     36928       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 64)     256         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 64)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 64)     36928       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 64)     256         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 8, 8, 64)     0           batch_normalization_33[0][0]     \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 64)     0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 64)     36928       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 64)     256         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 64)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 64)     36928       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 64)     256         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 8, 8, 64)     0           batch_normalization_35[0][0]     \n",
            "                                                                 activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 64)     0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 64)     36928       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 64)     256         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 64)     0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 64)     36928       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 64)     256         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 8, 8, 64)     0           batch_normalization_37[0][0]     \n",
            "                                                                 activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 64)     0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 64)     36928       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 64)     256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 64)     0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 64)     36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 64)     256         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 8, 8, 64)     0           batch_normalization_39[0][0]     \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 64)     0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 64)     36928       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 8, 64)     256         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 8, 64)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 64)     36928       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 8, 64)     256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 8, 8, 64)     0           batch_normalization_41[0][0]     \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 64)     0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 64)     0           activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 64)           0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          8320        global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          16512       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           1290        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 713,706\n",
            "Trainable params: 710,570\n",
            "Non-trainable params: 3,136\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxH_h9i2yQ-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-3\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 500\n",
        "model_name = 'ResNet_46Layers(500Epochs, Modified1)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z52y4GRPyQ-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "00f3ec36-81cc-442c-e0cf-cf3fcbc270e4"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = keras.utils.to_categorical( y_train, 10 )\n",
        "y_test = keras.utils.to_categorical( y_test, 10 )\n",
        "\n",
        "model.compile( optimizer = keras.optimizers.RMSprop( lr, 0.9 ),\n",
        "             loss = keras.losses.CategoricalCrossentropy( from_logits = True ),\n",
        "              metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "z9BjknwUyQ-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58d2334e-a691-4877-a9b1-6a9d688c5b3d"
      },
      "source": [
        "history = model.fit( x_train, y_train, batch_size = BATCH_SIZE,\n",
        "                    epochs = EPOCHS, validation_split = 0.2 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "79/79 [==============================] - 41s 524ms/step - loss: 2.2659 - acc: 0.1877 - val_loss: 2.3044 - val_acc: 0.1468\n",
            "Epoch 2/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 2.1714 - acc: 0.2812 - val_loss: 2.2601 - val_acc: 0.1836\n",
            "Epoch 3/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 2.1136 - acc: 0.3411 - val_loss: 2.3524 - val_acc: 0.1080\n",
            "Epoch 4/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 2.0757 - acc: 0.3802 - val_loss: 2.2113 - val_acc: 0.2397\n",
            "Epoch 5/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 2.0227 - acc: 0.4335 - val_loss: 2.2575 - val_acc: 0.1957\n",
            "Epoch 6/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.9838 - acc: 0.4726 - val_loss: 2.2040 - val_acc: 0.2517\n",
            "Epoch 7/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.9558 - acc: 0.5014 - val_loss: 2.1278 - val_acc: 0.3281\n",
            "Epoch 8/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.9327 - acc: 0.5243 - val_loss: 2.0158 - val_acc: 0.4402\n",
            "Epoch 9/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.9149 - acc: 0.5435 - val_loss: 2.1235 - val_acc: 0.3307\n",
            "Epoch 10/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.9025 - acc: 0.5565 - val_loss: 2.1075 - val_acc: 0.3484\n",
            "Epoch 11/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.8841 - acc: 0.5743 - val_loss: 2.1306 - val_acc: 0.3243\n",
            "Epoch 12/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.8380 - acc: 0.6205 - val_loss: 2.0823 - val_acc: 0.3742\n",
            "Epoch 13/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.8162 - acc: 0.6426 - val_loss: 2.1199 - val_acc: 0.3370\n",
            "Epoch 14/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.7965 - acc: 0.6623 - val_loss: 2.0522 - val_acc: 0.4057\n",
            "Epoch 15/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.7803 - acc: 0.6789 - val_loss: 2.0732 - val_acc: 0.3838\n",
            "Epoch 16/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.7708 - acc: 0.6881 - val_loss: 2.0330 - val_acc: 0.4237\n",
            "Epoch 17/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.7597 - acc: 0.6999 - val_loss: 1.9705 - val_acc: 0.4846\n",
            "Epoch 18/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.7510 - acc: 0.7082 - val_loss: 1.9437 - val_acc: 0.5141\n",
            "Epoch 19/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.7437 - acc: 0.7162 - val_loss: 1.8923 - val_acc: 0.5656\n",
            "Epoch 20/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.7362 - acc: 0.7239 - val_loss: 1.9987 - val_acc: 0.4588\n",
            "Epoch 21/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.7333 - acc: 0.7261 - val_loss: 2.0254 - val_acc: 0.4313\n",
            "Epoch 22/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.7276 - acc: 0.7317 - val_loss: 1.8631 - val_acc: 0.5944\n",
            "Epoch 23/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.7144 - acc: 0.7456 - val_loss: 1.8756 - val_acc: 0.5843\n",
            "Epoch 24/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.7135 - acc: 0.7463 - val_loss: 2.2238 - val_acc: 0.2345\n",
            "Epoch 25/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.7100 - acc: 0.7495 - val_loss: 1.9674 - val_acc: 0.4911\n",
            "Epoch 26/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.7036 - acc: 0.7559 - val_loss: 1.9771 - val_acc: 0.4802\n",
            "Epoch 27/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.7011 - acc: 0.7588 - val_loss: 1.8854 - val_acc: 0.5738\n",
            "Epoch 28/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6984 - acc: 0.7616 - val_loss: 1.8081 - val_acc: 0.6511\n",
            "Epoch 29/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6922 - acc: 0.7678 - val_loss: 1.9916 - val_acc: 0.4666\n",
            "Epoch 30/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6887 - acc: 0.7719 - val_loss: 1.8138 - val_acc: 0.6453\n",
            "Epoch 31/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6872 - acc: 0.7724 - val_loss: 1.8889 - val_acc: 0.5701\n",
            "Epoch 32/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6836 - acc: 0.7766 - val_loss: 1.8291 - val_acc: 0.6311\n",
            "Epoch 33/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6779 - acc: 0.7823 - val_loss: 1.8760 - val_acc: 0.5850\n",
            "Epoch 34/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6764 - acc: 0.7834 - val_loss: 1.7581 - val_acc: 0.7007\n",
            "Epoch 35/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6721 - acc: 0.7880 - val_loss: 1.8750 - val_acc: 0.5852\n",
            "Epoch 36/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6709 - acc: 0.7888 - val_loss: 1.8634 - val_acc: 0.5949\n",
            "Epoch 37/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6656 - acc: 0.7943 - val_loss: 2.0011 - val_acc: 0.4582\n",
            "Epoch 38/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6617 - acc: 0.7983 - val_loss: 1.9041 - val_acc: 0.5542\n",
            "Epoch 39/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6620 - acc: 0.7983 - val_loss: 1.8114 - val_acc: 0.6479\n",
            "Epoch 40/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6594 - acc: 0.8009 - val_loss: 1.8840 - val_acc: 0.5752\n",
            "Epoch 41/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6554 - acc: 0.8045 - val_loss: 1.7934 - val_acc: 0.6658\n",
            "Epoch 42/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6553 - acc: 0.8046 - val_loss: 2.0917 - val_acc: 0.3680\n",
            "Epoch 43/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6538 - acc: 0.8067 - val_loss: 2.0696 - val_acc: 0.3902\n",
            "Epoch 44/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6504 - acc: 0.8104 - val_loss: 1.7759 - val_acc: 0.6828\n",
            "Epoch 45/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6476 - acc: 0.8128 - val_loss: 1.8422 - val_acc: 0.6184\n",
            "Epoch 46/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6475 - acc: 0.8130 - val_loss: 1.7783 - val_acc: 0.6802\n",
            "Epoch 47/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6408 - acc: 0.8192 - val_loss: 1.7540 - val_acc: 0.7054\n",
            "Epoch 48/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6423 - acc: 0.8181 - val_loss: 1.9357 - val_acc: 0.5235\n",
            "Epoch 49/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.6418 - acc: 0.8184 - val_loss: 2.0057 - val_acc: 0.4539\n",
            "Epoch 50/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.6476 - acc: 0.8126 - val_loss: 1.8347 - val_acc: 0.6245\n",
            "Epoch 51/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6373 - acc: 0.8230 - val_loss: 1.8416 - val_acc: 0.6182\n",
            "Epoch 52/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6396 - acc: 0.8207 - val_loss: 1.9021 - val_acc: 0.5581\n",
            "Epoch 53/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.6393 - acc: 0.8212 - val_loss: 1.7957 - val_acc: 0.6645\n",
            "Epoch 54/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6446 - acc: 0.8157 - val_loss: 1.8413 - val_acc: 0.6188\n",
            "Epoch 55/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6349 - acc: 0.8257 - val_loss: 1.7768 - val_acc: 0.6826\n",
            "Epoch 56/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6353 - acc: 0.8255 - val_loss: 1.8947 - val_acc: 0.5644\n",
            "Epoch 57/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6342 - acc: 0.8262 - val_loss: 1.8055 - val_acc: 0.6541\n",
            "Epoch 58/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6341 - acc: 0.8264 - val_loss: 1.8635 - val_acc: 0.5964\n",
            "Epoch 59/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6316 - acc: 0.8289 - val_loss: 1.8259 - val_acc: 0.6338\n",
            "Epoch 60/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.6305 - acc: 0.8296 - val_loss: 1.7643 - val_acc: 0.6950\n",
            "Epoch 61/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6277 - acc: 0.8329 - val_loss: 1.7635 - val_acc: 0.6972\n",
            "Epoch 62/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6285 - acc: 0.8323 - val_loss: 1.7473 - val_acc: 0.7124\n",
            "Epoch 63/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6265 - acc: 0.8340 - val_loss: 1.7646 - val_acc: 0.6955\n",
            "Epoch 64/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6233 - acc: 0.8368 - val_loss: 1.8214 - val_acc: 0.6393\n",
            "Epoch 65/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6283 - acc: 0.8324 - val_loss: 1.7883 - val_acc: 0.6711\n",
            "Epoch 66/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6209 - acc: 0.8400 - val_loss: 1.8307 - val_acc: 0.6296\n",
            "Epoch 67/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6168 - acc: 0.8438 - val_loss: 1.8191 - val_acc: 0.6408\n",
            "Epoch 68/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.6227 - acc: 0.8375 - val_loss: 1.7414 - val_acc: 0.7185\n",
            "Epoch 69/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6155 - acc: 0.8453 - val_loss: 1.7894 - val_acc: 0.6708\n",
            "Epoch 70/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6139 - acc: 0.8466 - val_loss: 1.8987 - val_acc: 0.5612\n",
            "Epoch 71/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6167 - acc: 0.8440 - val_loss: 1.8506 - val_acc: 0.6103\n",
            "Epoch 72/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6160 - acc: 0.8447 - val_loss: 1.7451 - val_acc: 0.7153\n",
            "Epoch 73/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6176 - acc: 0.8432 - val_loss: 1.8367 - val_acc: 0.6237\n",
            "Epoch 74/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6136 - acc: 0.8470 - val_loss: 1.8191 - val_acc: 0.6419\n",
            "Epoch 75/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6168 - acc: 0.8442 - val_loss: 1.7793 - val_acc: 0.6812\n",
            "Epoch 76/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6123 - acc: 0.8484 - val_loss: 1.7827 - val_acc: 0.6774\n",
            "Epoch 77/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6136 - acc: 0.8471 - val_loss: 1.8694 - val_acc: 0.5897\n",
            "Epoch 78/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6147 - acc: 0.8460 - val_loss: 1.7348 - val_acc: 0.7256\n",
            "Epoch 79/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6108 - acc: 0.8499 - val_loss: 1.7777 - val_acc: 0.6832\n",
            "Epoch 80/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6065 - acc: 0.8541 - val_loss: 1.7948 - val_acc: 0.6655\n",
            "Epoch 81/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6116 - acc: 0.8490 - val_loss: 1.8832 - val_acc: 0.5774\n",
            "Epoch 82/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.6062 - acc: 0.8543 - val_loss: 1.8397 - val_acc: 0.6201\n",
            "Epoch 83/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6093 - acc: 0.8514 - val_loss: 1.7240 - val_acc: 0.7367\n",
            "Epoch 84/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6079 - acc: 0.8529 - val_loss: 1.7695 - val_acc: 0.6909\n",
            "Epoch 85/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6101 - acc: 0.8504 - val_loss: 1.7326 - val_acc: 0.7277\n",
            "Epoch 86/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6081 - acc: 0.8527 - val_loss: 1.7920 - val_acc: 0.6683\n",
            "Epoch 87/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6073 - acc: 0.8533 - val_loss: 1.8310 - val_acc: 0.6294\n",
            "Epoch 88/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6055 - acc: 0.8552 - val_loss: 1.7448 - val_acc: 0.7156\n",
            "Epoch 89/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6059 - acc: 0.8547 - val_loss: 1.7629 - val_acc: 0.6978\n",
            "Epoch 90/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6021 - acc: 0.8584 - val_loss: 1.7149 - val_acc: 0.7451\n",
            "Epoch 91/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5999 - acc: 0.8609 - val_loss: 1.8204 - val_acc: 0.6396\n",
            "Epoch 92/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6057 - acc: 0.8551 - val_loss: 1.7128 - val_acc: 0.7479\n",
            "Epoch 93/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6023 - acc: 0.8585 - val_loss: 1.7528 - val_acc: 0.7074\n",
            "Epoch 94/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6024 - acc: 0.8585 - val_loss: 1.8145 - val_acc: 0.6454\n",
            "Epoch 95/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.6009 - acc: 0.8596 - val_loss: 1.6972 - val_acc: 0.7636\n",
            "Epoch 96/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5995 - acc: 0.8612 - val_loss: 1.7385 - val_acc: 0.7222\n",
            "Epoch 97/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5985 - acc: 0.8623 - val_loss: 1.8367 - val_acc: 0.6239\n",
            "Epoch 98/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5984 - acc: 0.8626 - val_loss: 1.7951 - val_acc: 0.6653\n",
            "Epoch 99/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5993 - acc: 0.8614 - val_loss: 1.7304 - val_acc: 0.7305\n",
            "Epoch 100/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5993 - acc: 0.8614 - val_loss: 1.7430 - val_acc: 0.7170\n",
            "Epoch 101/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.6004 - acc: 0.8604 - val_loss: 1.7859 - val_acc: 0.6750\n",
            "Epoch 102/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5936 - acc: 0.8669 - val_loss: 1.7286 - val_acc: 0.7310\n",
            "Epoch 103/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5902 - acc: 0.8706 - val_loss: 1.7335 - val_acc: 0.7265\n",
            "Epoch 104/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5947 - acc: 0.8660 - val_loss: 1.7550 - val_acc: 0.7057\n",
            "Epoch 105/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5950 - acc: 0.8660 - val_loss: 1.7590 - val_acc: 0.7014\n",
            "Epoch 106/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5975 - acc: 0.8636 - val_loss: 1.7198 - val_acc: 0.7411\n",
            "Epoch 107/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5973 - acc: 0.8634 - val_loss: 1.7579 - val_acc: 0.7023\n",
            "Epoch 108/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5954 - acc: 0.8654 - val_loss: 1.6937 - val_acc: 0.7665\n",
            "Epoch 109/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5952 - acc: 0.8656 - val_loss: 1.6860 - val_acc: 0.7741\n",
            "Epoch 110/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5951 - acc: 0.8655 - val_loss: 1.7208 - val_acc: 0.7396\n",
            "Epoch 111/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5921 - acc: 0.8687 - val_loss: 1.7619 - val_acc: 0.6982\n",
            "Epoch 112/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5899 - acc: 0.8709 - val_loss: 1.7393 - val_acc: 0.7214\n",
            "Epoch 113/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5881 - acc: 0.8726 - val_loss: 1.7710 - val_acc: 0.6888\n",
            "Epoch 114/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5903 - acc: 0.8703 - val_loss: 1.7219 - val_acc: 0.7383\n",
            "Epoch 115/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5906 - acc: 0.8704 - val_loss: 1.8050 - val_acc: 0.6551\n",
            "Epoch 116/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5884 - acc: 0.8723 - val_loss: 1.7266 - val_acc: 0.7338\n",
            "Epoch 117/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5879 - acc: 0.8729 - val_loss: 1.7767 - val_acc: 0.6832\n",
            "Epoch 118/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5900 - acc: 0.8708 - val_loss: 1.7340 - val_acc: 0.7263\n",
            "Epoch 119/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5859 - acc: 0.8748 - val_loss: 1.7223 - val_acc: 0.7388\n",
            "Epoch 120/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5856 - acc: 0.8753 - val_loss: 1.7448 - val_acc: 0.7156\n",
            "Epoch 121/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5812 - acc: 0.8798 - val_loss: 1.6987 - val_acc: 0.7614\n",
            "Epoch 122/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5841 - acc: 0.8766 - val_loss: 1.7617 - val_acc: 0.6991\n",
            "Epoch 123/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5865 - acc: 0.8743 - val_loss: 1.6841 - val_acc: 0.7763\n",
            "Epoch 124/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5851 - acc: 0.8756 - val_loss: 1.8474 - val_acc: 0.6133\n",
            "Epoch 125/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5828 - acc: 0.8781 - val_loss: 1.7671 - val_acc: 0.6935\n",
            "Epoch 126/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5809 - acc: 0.8799 - val_loss: 1.7398 - val_acc: 0.7203\n",
            "Epoch 127/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5831 - acc: 0.8778 - val_loss: 1.8042 - val_acc: 0.6555\n",
            "Epoch 128/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5856 - acc: 0.8752 - val_loss: 1.7535 - val_acc: 0.7073\n",
            "Epoch 129/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5864 - acc: 0.8745 - val_loss: 1.7745 - val_acc: 0.6861\n",
            "Epoch 130/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5839 - acc: 0.8769 - val_loss: 1.7576 - val_acc: 0.7027\n",
            "Epoch 131/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5866 - acc: 0.8742 - val_loss: 1.7366 - val_acc: 0.7244\n",
            "Epoch 132/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5859 - acc: 0.8749 - val_loss: 1.7661 - val_acc: 0.6943\n",
            "Epoch 133/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5877 - acc: 0.8732 - val_loss: 1.8017 - val_acc: 0.6588\n",
            "Epoch 134/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5890 - acc: 0.8719 - val_loss: 1.6922 - val_acc: 0.7687\n",
            "Epoch 135/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5839 - acc: 0.8774 - val_loss: 1.7908 - val_acc: 0.6700\n",
            "Epoch 136/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5825 - acc: 0.8782 - val_loss: 1.7284 - val_acc: 0.7317\n",
            "Epoch 137/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5806 - acc: 0.8803 - val_loss: 1.7459 - val_acc: 0.7149\n",
            "Epoch 138/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5836 - acc: 0.8771 - val_loss: 1.7274 - val_acc: 0.7337\n",
            "Epoch 139/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5882 - acc: 0.8726 - val_loss: 1.7789 - val_acc: 0.6820\n",
            "Epoch 140/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5822 - acc: 0.8785 - val_loss: 1.7078 - val_acc: 0.7529\n",
            "Epoch 141/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5780 - acc: 0.8829 - val_loss: 1.7537 - val_acc: 0.7072\n",
            "Epoch 142/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5812 - acc: 0.8798 - val_loss: 1.7417 - val_acc: 0.7194\n",
            "Epoch 143/500\n",
            "79/79 [==============================] - 40s 511ms/step - loss: 1.5826 - acc: 0.8780 - val_loss: 1.6950 - val_acc: 0.7661\n",
            "Epoch 144/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5805 - acc: 0.8804 - val_loss: 1.6926 - val_acc: 0.7680\n",
            "Epoch 145/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5795 - acc: 0.8815 - val_loss: 1.7241 - val_acc: 0.7365\n",
            "Epoch 146/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5790 - acc: 0.8818 - val_loss: 1.7262 - val_acc: 0.7343\n",
            "Epoch 147/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5779 - acc: 0.8829 - val_loss: 1.7385 - val_acc: 0.7220\n",
            "Epoch 148/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5794 - acc: 0.8814 - val_loss: 1.7934 - val_acc: 0.6672\n",
            "Epoch 149/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5778 - acc: 0.8827 - val_loss: 1.7347 - val_acc: 0.7259\n",
            "Epoch 150/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5786 - acc: 0.8825 - val_loss: 1.6907 - val_acc: 0.7697\n",
            "Epoch 151/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5804 - acc: 0.8805 - val_loss: 1.7575 - val_acc: 0.7032\n",
            "Epoch 152/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5801 - acc: 0.8808 - val_loss: 1.8114 - val_acc: 0.6494\n",
            "Epoch 153/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5791 - acc: 0.8820 - val_loss: 1.6787 - val_acc: 0.7820\n",
            "Epoch 154/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5779 - acc: 0.8832 - val_loss: 1.6853 - val_acc: 0.7752\n",
            "Epoch 155/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5804 - acc: 0.8806 - val_loss: 1.7544 - val_acc: 0.7061\n",
            "Epoch 156/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5834 - acc: 0.8775 - val_loss: 1.7223 - val_acc: 0.7387\n",
            "Epoch 157/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5773 - acc: 0.8838 - val_loss: 1.8013 - val_acc: 0.6599\n",
            "Epoch 158/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5821 - acc: 0.8787 - val_loss: 1.7811 - val_acc: 0.6794\n",
            "Epoch 159/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5769 - acc: 0.8839 - val_loss: 1.7165 - val_acc: 0.7441\n",
            "Epoch 160/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5781 - acc: 0.8827 - val_loss: 1.6780 - val_acc: 0.7828\n",
            "Epoch 161/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5807 - acc: 0.8801 - val_loss: 1.7521 - val_acc: 0.7092\n",
            "Epoch 162/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5758 - acc: 0.8850 - val_loss: 1.7268 - val_acc: 0.7341\n",
            "Epoch 163/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5782 - acc: 0.8828 - val_loss: 1.7848 - val_acc: 0.6753\n",
            "Epoch 164/500\n",
            "79/79 [==============================] - 40s 512ms/step - loss: 1.5785 - acc: 0.8827 - val_loss: 1.7081 - val_acc: 0.7530\n",
            "Epoch 165/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5760 - acc: 0.8850 - val_loss: 1.7251 - val_acc: 0.7355\n",
            "Epoch 166/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5783 - acc: 0.8826 - val_loss: 1.7567 - val_acc: 0.7038\n",
            "Epoch 167/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5753 - acc: 0.8854 - val_loss: 1.7858 - val_acc: 0.6748\n",
            "Epoch 168/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5789 - acc: 0.8820 - val_loss: 1.8240 - val_acc: 0.6370\n",
            "Epoch 169/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5780 - acc: 0.8827 - val_loss: 1.7285 - val_acc: 0.7326\n",
            "Epoch 170/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5718 - acc: 0.8892 - val_loss: 1.6803 - val_acc: 0.7800\n",
            "Epoch 171/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5734 - acc: 0.8874 - val_loss: 1.7127 - val_acc: 0.7481\n",
            "Epoch 172/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5767 - acc: 0.8844 - val_loss: 1.7194 - val_acc: 0.7410\n",
            "Epoch 173/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5746 - acc: 0.8863 - val_loss: 1.8454 - val_acc: 0.6150\n",
            "Epoch 174/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5778 - acc: 0.8830 - val_loss: 1.7595 - val_acc: 0.7011\n",
            "Epoch 175/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5777 - acc: 0.8832 - val_loss: 1.7025 - val_acc: 0.7583\n",
            "Epoch 176/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5774 - acc: 0.8835 - val_loss: 1.7520 - val_acc: 0.7088\n",
            "Epoch 177/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5749 - acc: 0.8860 - val_loss: 1.7554 - val_acc: 0.7057\n",
            "Epoch 178/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5724 - acc: 0.8886 - val_loss: 1.7115 - val_acc: 0.7493\n",
            "Epoch 179/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5813 - acc: 0.8796 - val_loss: 1.7391 - val_acc: 0.7217\n",
            "Epoch 180/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5742 - acc: 0.8867 - val_loss: 1.7412 - val_acc: 0.7196\n",
            "Epoch 181/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5760 - acc: 0.8850 - val_loss: 1.6695 - val_acc: 0.7912\n",
            "Epoch 182/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5754 - acc: 0.8857 - val_loss: 1.7299 - val_acc: 0.7308\n",
            "Epoch 183/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5737 - acc: 0.8873 - val_loss: 1.7335 - val_acc: 0.7276\n",
            "Epoch 184/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5771 - acc: 0.8840 - val_loss: 1.7465 - val_acc: 0.7138\n",
            "Epoch 185/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5742 - acc: 0.8867 - val_loss: 1.6921 - val_acc: 0.7685\n",
            "Epoch 186/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5723 - acc: 0.8888 - val_loss: 1.7750 - val_acc: 0.6861\n",
            "Epoch 187/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5758 - acc: 0.8849 - val_loss: 1.8100 - val_acc: 0.6510\n",
            "Epoch 188/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5755 - acc: 0.8853 - val_loss: 1.7190 - val_acc: 0.7419\n",
            "Epoch 189/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5752 - acc: 0.8858 - val_loss: 1.7737 - val_acc: 0.6872\n",
            "Epoch 190/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5739 - acc: 0.8869 - val_loss: 1.7570 - val_acc: 0.7032\n",
            "Epoch 191/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5725 - acc: 0.8884 - val_loss: 1.7208 - val_acc: 0.7398\n",
            "Epoch 192/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5778 - acc: 0.8833 - val_loss: 1.7044 - val_acc: 0.7560\n",
            "Epoch 193/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5765 - acc: 0.8845 - val_loss: 1.6987 - val_acc: 0.7622\n",
            "Epoch 194/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5740 - acc: 0.8866 - val_loss: 1.7094 - val_acc: 0.7517\n",
            "Epoch 195/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5776 - acc: 0.8835 - val_loss: 1.7846 - val_acc: 0.6763\n",
            "Epoch 196/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5810 - acc: 0.8799 - val_loss: 1.7074 - val_acc: 0.7538\n",
            "Epoch 197/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5784 - acc: 0.8825 - val_loss: 1.7049 - val_acc: 0.7561\n",
            "Epoch 198/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5754 - acc: 0.8857 - val_loss: 1.7248 - val_acc: 0.7358\n",
            "Epoch 199/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5718 - acc: 0.8892 - val_loss: 1.6839 - val_acc: 0.7770\n",
            "Epoch 200/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5709 - acc: 0.8899 - val_loss: 1.7368 - val_acc: 0.7236\n",
            "Epoch 201/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5723 - acc: 0.8888 - val_loss: 1.7075 - val_acc: 0.7531\n",
            "Epoch 202/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5682 - acc: 0.8929 - val_loss: 1.7696 - val_acc: 0.6910\n",
            "Epoch 203/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5716 - acc: 0.8893 - val_loss: 1.7172 - val_acc: 0.7433\n",
            "Epoch 204/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5681 - acc: 0.8928 - val_loss: 1.6933 - val_acc: 0.7674\n",
            "Epoch 205/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5715 - acc: 0.8895 - val_loss: 1.6900 - val_acc: 0.7714\n",
            "Epoch 206/500\n",
            "79/79 [==============================] - 40s 505ms/step - loss: 1.5645 - acc: 0.8964 - val_loss: 1.6941 - val_acc: 0.7665\n",
            "Epoch 207/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5672 - acc: 0.8938 - val_loss: 1.6929 - val_acc: 0.7680\n",
            "Epoch 208/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5664 - acc: 0.8946 - val_loss: 1.7012 - val_acc: 0.7599\n",
            "Epoch 209/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5684 - acc: 0.8924 - val_loss: 1.6793 - val_acc: 0.7816\n",
            "Epoch 210/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5702 - acc: 0.8910 - val_loss: 1.7057 - val_acc: 0.7552\n",
            "Epoch 211/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5678 - acc: 0.8930 - val_loss: 1.6877 - val_acc: 0.7733\n",
            "Epoch 212/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5679 - acc: 0.8929 - val_loss: 1.7205 - val_acc: 0.7404\n",
            "Epoch 213/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5700 - acc: 0.8910 - val_loss: 1.6960 - val_acc: 0.7644\n",
            "Epoch 214/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5669 - acc: 0.8939 - val_loss: 1.6968 - val_acc: 0.7638\n",
            "Epoch 215/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5710 - acc: 0.8900 - val_loss: 1.6993 - val_acc: 0.7616\n",
            "Epoch 216/500\n",
            "79/79 [==============================] - 40s 505ms/step - loss: 1.5680 - acc: 0.8929 - val_loss: 1.6968 - val_acc: 0.7643\n",
            "Epoch 217/500\n",
            "79/79 [==============================] - 40s 505ms/step - loss: 1.5685 - acc: 0.8925 - val_loss: 1.6833 - val_acc: 0.7774\n",
            "Epoch 218/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5657 - acc: 0.8954 - val_loss: 1.7021 - val_acc: 0.7586\n",
            "Epoch 219/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5700 - acc: 0.8910 - val_loss: 1.7476 - val_acc: 0.7135\n",
            "Epoch 220/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5680 - acc: 0.8929 - val_loss: 1.7020 - val_acc: 0.7588\n",
            "Epoch 221/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5672 - acc: 0.8939 - val_loss: 1.6937 - val_acc: 0.7671\n",
            "Epoch 222/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5668 - acc: 0.8941 - val_loss: 1.7060 - val_acc: 0.7550\n",
            "Epoch 223/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5670 - acc: 0.8941 - val_loss: 1.6919 - val_acc: 0.7693\n",
            "Epoch 224/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5649 - acc: 0.8959 - val_loss: 1.6881 - val_acc: 0.7729\n",
            "Epoch 225/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5746 - acc: 0.8863 - val_loss: 1.7175 - val_acc: 0.7431\n",
            "Epoch 226/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5713 - acc: 0.8896 - val_loss: 1.6861 - val_acc: 0.7748\n",
            "Epoch 227/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5661 - acc: 0.8949 - val_loss: 1.6989 - val_acc: 0.7617\n",
            "Epoch 228/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5670 - acc: 0.8941 - val_loss: 1.7117 - val_acc: 0.7493\n",
            "Epoch 229/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5669 - acc: 0.8941 - val_loss: 1.6948 - val_acc: 0.7661\n",
            "Epoch 230/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5665 - acc: 0.8945 - val_loss: 1.7089 - val_acc: 0.7518\n",
            "Epoch 231/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5636 - acc: 0.8973 - val_loss: 1.6798 - val_acc: 0.7815\n",
            "Epoch 232/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5630 - acc: 0.8978 - val_loss: 1.7431 - val_acc: 0.7179\n",
            "Epoch 233/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5707 - acc: 0.8900 - val_loss: 1.6835 - val_acc: 0.7773\n",
            "Epoch 234/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5658 - acc: 0.8952 - val_loss: 1.7491 - val_acc: 0.7116\n",
            "Epoch 235/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5652 - acc: 0.8957 - val_loss: 1.6968 - val_acc: 0.7641\n",
            "Epoch 236/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5623 - acc: 0.8987 - val_loss: 1.7719 - val_acc: 0.6888\n",
            "Epoch 237/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5653 - acc: 0.8957 - val_loss: 1.7782 - val_acc: 0.6824\n",
            "Epoch 238/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5681 - acc: 0.8929 - val_loss: 1.7016 - val_acc: 0.7596\n",
            "Epoch 239/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5672 - acc: 0.8937 - val_loss: 1.8102 - val_acc: 0.6506\n",
            "Epoch 240/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5640 - acc: 0.8969 - val_loss: 1.7283 - val_acc: 0.7328\n",
            "Epoch 241/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5670 - acc: 0.8940 - val_loss: 1.7072 - val_acc: 0.7538\n",
            "Epoch 242/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5730 - acc: 0.8880 - val_loss: 1.7684 - val_acc: 0.6929\n",
            "Epoch 243/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5691 - acc: 0.8920 - val_loss: 1.6788 - val_acc: 0.7820\n",
            "Epoch 244/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5688 - acc: 0.8922 - val_loss: 1.7338 - val_acc: 0.7265\n",
            "Epoch 245/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5668 - acc: 0.8942 - val_loss: 1.6771 - val_acc: 0.7837\n",
            "Epoch 246/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5651 - acc: 0.8960 - val_loss: 1.7185 - val_acc: 0.7422\n",
            "Epoch 247/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5641 - acc: 0.8968 - val_loss: 1.6945 - val_acc: 0.7665\n",
            "Epoch 248/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5610 - acc: 0.9000 - val_loss: 1.7087 - val_acc: 0.7520\n",
            "Epoch 249/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5668 - acc: 0.8942 - val_loss: 1.6720 - val_acc: 0.7891\n",
            "Epoch 250/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5648 - acc: 0.8963 - val_loss: 1.6853 - val_acc: 0.7753\n",
            "Epoch 251/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5654 - acc: 0.8957 - val_loss: 1.7013 - val_acc: 0.7599\n",
            "Epoch 252/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5578 - acc: 0.9033 - val_loss: 1.6948 - val_acc: 0.7659\n",
            "Epoch 253/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5659 - acc: 0.8949 - val_loss: 1.7054 - val_acc: 0.7558\n",
            "Epoch 254/500\n",
            "79/79 [==============================] - 40s 505ms/step - loss: 1.5639 - acc: 0.8971 - val_loss: 1.7337 - val_acc: 0.7271\n",
            "Epoch 255/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5677 - acc: 0.8932 - val_loss: 1.6791 - val_acc: 0.7818\n",
            "Epoch 256/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5611 - acc: 0.9000 - val_loss: 1.6820 - val_acc: 0.7791\n",
            "Epoch 257/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5610 - acc: 0.9000 - val_loss: 1.7061 - val_acc: 0.7546\n",
            "Epoch 258/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5662 - acc: 0.8948 - val_loss: 1.6969 - val_acc: 0.7642\n",
            "Epoch 259/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5687 - acc: 0.8924 - val_loss: 1.7567 - val_acc: 0.7043\n",
            "Epoch 260/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5693 - acc: 0.8918 - val_loss: 1.7196 - val_acc: 0.7412\n",
            "Epoch 261/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5642 - acc: 0.8967 - val_loss: 1.6910 - val_acc: 0.7699\n",
            "Epoch 262/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5613 - acc: 0.8997 - val_loss: 1.7032 - val_acc: 0.7579\n",
            "Epoch 263/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5633 - acc: 0.8976 - val_loss: 1.7022 - val_acc: 0.7587\n",
            "Epoch 264/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5607 - acc: 0.9002 - val_loss: 1.6881 - val_acc: 0.7725\n",
            "Epoch 265/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5615 - acc: 0.8996 - val_loss: 1.6671 - val_acc: 0.7936\n",
            "Epoch 266/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5590 - acc: 0.9020 - val_loss: 1.7016 - val_acc: 0.7592\n",
            "Epoch 267/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5584 - acc: 0.9026 - val_loss: 1.7406 - val_acc: 0.7204\n",
            "Epoch 268/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5581 - acc: 0.9028 - val_loss: 1.6955 - val_acc: 0.7651\n",
            "Epoch 269/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5606 - acc: 0.9005 - val_loss: 1.6878 - val_acc: 0.7733\n",
            "Epoch 270/500\n",
            "79/79 [==============================] - 40s 505ms/step - loss: 1.5645 - acc: 0.8964 - val_loss: 1.6941 - val_acc: 0.7668\n",
            "Epoch 271/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5611 - acc: 0.9000 - val_loss: 1.6921 - val_acc: 0.7687\n",
            "Epoch 272/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5601 - acc: 0.9008 - val_loss: 1.6695 - val_acc: 0.7915\n",
            "Epoch 273/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5640 - acc: 0.8971 - val_loss: 1.7078 - val_acc: 0.7534\n",
            "Epoch 274/500\n",
            "79/79 [==============================] - 40s 512ms/step - loss: 1.5616 - acc: 0.8994 - val_loss: 1.6703 - val_acc: 0.7909\n",
            "Epoch 275/500\n",
            "79/79 [==============================] - 40s 512ms/step - loss: 1.5659 - acc: 0.8951 - val_loss: 1.7121 - val_acc: 0.7488\n",
            "Epoch 276/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5640 - acc: 0.8970 - val_loss: 1.6625 - val_acc: 0.7986\n",
            "Epoch 277/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5625 - acc: 0.8985 - val_loss: 1.6966 - val_acc: 0.7639\n",
            "Epoch 278/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5651 - acc: 0.8959 - val_loss: 1.6773 - val_acc: 0.7840\n",
            "Epoch 279/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5652 - acc: 0.8956 - val_loss: 1.6962 - val_acc: 0.7645\n",
            "Epoch 280/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5573 - acc: 0.9036 - val_loss: 1.6651 - val_acc: 0.7955\n",
            "Epoch 281/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5622 - acc: 0.8990 - val_loss: 1.7122 - val_acc: 0.7488\n",
            "Epoch 282/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5596 - acc: 0.9013 - val_loss: 1.7394 - val_acc: 0.7218\n",
            "Epoch 283/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5625 - acc: 0.8985 - val_loss: 1.7222 - val_acc: 0.7386\n",
            "Epoch 284/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5615 - acc: 0.8996 - val_loss: 1.6834 - val_acc: 0.7773\n",
            "Epoch 285/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5577 - acc: 0.9036 - val_loss: 1.7301 - val_acc: 0.7306\n",
            "Epoch 286/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5596 - acc: 0.9014 - val_loss: 1.7011 - val_acc: 0.7599\n",
            "Epoch 287/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5672 - acc: 0.8938 - val_loss: 1.6774 - val_acc: 0.7837\n",
            "Epoch 288/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5614 - acc: 0.8996 - val_loss: 1.6959 - val_acc: 0.7649\n",
            "Epoch 289/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5623 - acc: 0.8986 - val_loss: 1.7138 - val_acc: 0.7472\n",
            "Epoch 290/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5596 - acc: 0.9015 - val_loss: 1.7193 - val_acc: 0.7416\n",
            "Epoch 291/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5615 - acc: 0.8995 - val_loss: 1.7054 - val_acc: 0.7553\n",
            "Epoch 292/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5604 - acc: 0.9007 - val_loss: 1.7592 - val_acc: 0.7018\n",
            "Epoch 293/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5578 - acc: 0.9032 - val_loss: 1.6898 - val_acc: 0.7712\n",
            "Epoch 294/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5596 - acc: 0.9014 - val_loss: 1.6695 - val_acc: 0.7913\n",
            "Epoch 295/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5575 - acc: 0.9035 - val_loss: 1.7632 - val_acc: 0.6977\n",
            "Epoch 296/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5600 - acc: 0.9010 - val_loss: 1.7013 - val_acc: 0.7596\n",
            "Epoch 297/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5639 - acc: 0.8972 - val_loss: 1.6984 - val_acc: 0.7621\n",
            "Epoch 298/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5613 - acc: 0.8997 - val_loss: 1.7739 - val_acc: 0.6873\n",
            "Epoch 299/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5611 - acc: 0.8998 - val_loss: 1.7126 - val_acc: 0.7485\n",
            "Epoch 300/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5597 - acc: 0.9013 - val_loss: 1.6815 - val_acc: 0.7798\n",
            "Epoch 301/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5630 - acc: 0.8981 - val_loss: 1.6973 - val_acc: 0.7635\n",
            "Epoch 302/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5650 - acc: 0.8960 - val_loss: 1.6973 - val_acc: 0.7637\n",
            "Epoch 303/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5623 - acc: 0.8988 - val_loss: 1.7055 - val_acc: 0.7558\n",
            "Epoch 304/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5604 - acc: 0.9006 - val_loss: 1.6941 - val_acc: 0.7670\n",
            "Epoch 305/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5575 - acc: 0.9035 - val_loss: 1.6967 - val_acc: 0.7640\n",
            "Epoch 306/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5606 - acc: 0.9005 - val_loss: 1.7189 - val_acc: 0.7419\n",
            "Epoch 307/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5621 - acc: 0.8989 - val_loss: 1.6930 - val_acc: 0.7676\n",
            "Epoch 308/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5660 - acc: 0.8949 - val_loss: 1.6793 - val_acc: 0.7814\n",
            "Epoch 309/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5577 - acc: 0.9034 - val_loss: 1.6960 - val_acc: 0.7647\n",
            "Epoch 310/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5587 - acc: 0.9021 - val_loss: 1.7211 - val_acc: 0.7399\n",
            "Epoch 311/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5595 - acc: 0.9015 - val_loss: 1.7066 - val_acc: 0.7543\n",
            "Epoch 312/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5613 - acc: 0.8996 - val_loss: 1.6822 - val_acc: 0.7789\n",
            "Epoch 313/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5600 - acc: 0.9009 - val_loss: 1.6878 - val_acc: 0.7732\n",
            "Epoch 314/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5577 - acc: 0.9034 - val_loss: 1.7015 - val_acc: 0.7596\n",
            "Epoch 315/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5519 - acc: 0.9093 - val_loss: 1.7036 - val_acc: 0.7570\n",
            "Epoch 316/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5580 - acc: 0.9030 - val_loss: 1.6771 - val_acc: 0.7837\n",
            "Epoch 317/500\n",
            "79/79 [==============================] - 40s 512ms/step - loss: 1.5623 - acc: 0.8989 - val_loss: 1.6823 - val_acc: 0.7786\n",
            "Epoch 318/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5550 - acc: 0.9061 - val_loss: 1.6820 - val_acc: 0.7790\n",
            "Epoch 319/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5564 - acc: 0.9047 - val_loss: 1.6776 - val_acc: 0.7835\n",
            "Epoch 320/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5578 - acc: 0.9033 - val_loss: 1.6862 - val_acc: 0.7745\n",
            "Epoch 321/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5600 - acc: 0.9011 - val_loss: 1.6968 - val_acc: 0.7642\n",
            "Epoch 322/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5616 - acc: 0.8995 - val_loss: 1.7491 - val_acc: 0.7121\n",
            "Epoch 323/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5593 - acc: 0.9018 - val_loss: 1.6946 - val_acc: 0.7664\n",
            "Epoch 324/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5592 - acc: 0.9018 - val_loss: 1.6875 - val_acc: 0.7731\n",
            "Epoch 325/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5596 - acc: 0.9014 - val_loss: 1.6775 - val_acc: 0.7834\n",
            "Epoch 326/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5558 - acc: 0.9054 - val_loss: 1.6763 - val_acc: 0.7846\n",
            "Epoch 327/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5593 - acc: 0.9018 - val_loss: 1.6761 - val_acc: 0.7850\n",
            "Epoch 328/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5577 - acc: 0.9034 - val_loss: 1.6697 - val_acc: 0.7914\n",
            "Epoch 329/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5565 - acc: 0.9043 - val_loss: 1.6618 - val_acc: 0.7992\n",
            "Epoch 330/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5553 - acc: 0.9058 - val_loss: 1.6744 - val_acc: 0.7870\n",
            "Epoch 331/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5561 - acc: 0.9051 - val_loss: 1.6957 - val_acc: 0.7651\n",
            "Epoch 332/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5553 - acc: 0.9057 - val_loss: 1.7282 - val_acc: 0.7327\n",
            "Epoch 333/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5503 - acc: 0.9108 - val_loss: 1.6610 - val_acc: 0.8001\n",
            "Epoch 334/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5538 - acc: 0.9073 - val_loss: 1.6804 - val_acc: 0.7806\n",
            "Epoch 335/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5584 - acc: 0.9028 - val_loss: 1.7050 - val_acc: 0.7560\n",
            "Epoch 336/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5545 - acc: 0.9065 - val_loss: 1.6914 - val_acc: 0.7692\n",
            "Epoch 337/500\n",
            "79/79 [==============================] - 40s 511ms/step - loss: 1.5546 - acc: 0.9064 - val_loss: 1.7332 - val_acc: 0.7280\n",
            "Epoch 338/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5606 - acc: 0.9003 - val_loss: 1.6968 - val_acc: 0.7641\n",
            "Epoch 339/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5602 - acc: 0.9011 - val_loss: 1.7209 - val_acc: 0.7398\n",
            "Epoch 340/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5594 - acc: 0.9018 - val_loss: 1.7189 - val_acc: 0.7423\n",
            "Epoch 341/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5633 - acc: 0.8978 - val_loss: 1.7325 - val_acc: 0.7282\n",
            "Epoch 342/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5581 - acc: 0.9030 - val_loss: 1.6876 - val_acc: 0.7734\n",
            "Epoch 343/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5615 - acc: 0.8995 - val_loss: 1.6684 - val_acc: 0.7927\n",
            "Epoch 344/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5538 - acc: 0.9074 - val_loss: 1.7180 - val_acc: 0.7429\n",
            "Epoch 345/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5574 - acc: 0.9035 - val_loss: 1.7246 - val_acc: 0.7361\n",
            "Epoch 346/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5570 - acc: 0.9040 - val_loss: 1.7668 - val_acc: 0.6941\n",
            "Epoch 347/500\n",
            "79/79 [==============================] - 40s 511ms/step - loss: 1.5568 - acc: 0.9042 - val_loss: 1.6807 - val_acc: 0.7802\n",
            "Epoch 348/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5541 - acc: 0.9069 - val_loss: 1.6939 - val_acc: 0.7670\n",
            "Epoch 349/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5594 - acc: 0.9017 - val_loss: 1.7362 - val_acc: 0.7248\n",
            "Epoch 350/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5596 - acc: 0.9014 - val_loss: 1.7100 - val_acc: 0.7510\n",
            "Epoch 351/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5658 - acc: 0.8953 - val_loss: 1.6620 - val_acc: 0.7990\n",
            "Epoch 352/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5556 - acc: 0.9055 - val_loss: 1.7118 - val_acc: 0.7493\n",
            "Epoch 353/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5552 - acc: 0.9058 - val_loss: 1.6725 - val_acc: 0.7886\n",
            "Epoch 354/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5553 - acc: 0.9057 - val_loss: 1.7477 - val_acc: 0.7132\n",
            "Epoch 355/500\n",
            "79/79 [==============================] - 40s 509ms/step - loss: 1.5558 - acc: 0.9054 - val_loss: 1.6670 - val_acc: 0.7940\n",
            "Epoch 356/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5550 - acc: 0.9061 - val_loss: 1.6704 - val_acc: 0.7908\n",
            "Epoch 357/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5577 - acc: 0.9033 - val_loss: 1.6720 - val_acc: 0.7887\n",
            "Epoch 358/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5587 - acc: 0.9024 - val_loss: 1.6973 - val_acc: 0.7640\n",
            "Epoch 359/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5562 - acc: 0.9049 - val_loss: 1.7065 - val_acc: 0.7543\n",
            "Epoch 360/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5577 - acc: 0.9034 - val_loss: 1.6889 - val_acc: 0.7720\n",
            "Epoch 361/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5600 - acc: 0.9011 - val_loss: 1.7812 - val_acc: 0.6798\n",
            "Epoch 362/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5575 - acc: 0.9036 - val_loss: 1.7109 - val_acc: 0.7504\n",
            "Epoch 363/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5536 - acc: 0.9074 - val_loss: 1.7208 - val_acc: 0.7402\n",
            "Epoch 364/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5529 - acc: 0.9081 - val_loss: 1.6847 - val_acc: 0.7763\n",
            "Epoch 365/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5522 - acc: 0.9088 - val_loss: 1.6814 - val_acc: 0.7795\n",
            "Epoch 366/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5529 - acc: 0.9082 - val_loss: 1.6778 - val_acc: 0.7834\n",
            "Epoch 367/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5555 - acc: 0.9055 - val_loss: 1.6734 - val_acc: 0.7873\n",
            "Epoch 368/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5583 - acc: 0.9026 - val_loss: 1.6711 - val_acc: 0.7899\n",
            "Epoch 369/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5553 - acc: 0.9056 - val_loss: 1.6744 - val_acc: 0.7866\n",
            "Epoch 370/500\n",
            "79/79 [==============================] - 40s 511ms/step - loss: 1.5567 - acc: 0.9042 - val_loss: 1.6814 - val_acc: 0.7798\n",
            "Epoch 371/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5593 - acc: 0.9017 - val_loss: 1.6864 - val_acc: 0.7744\n",
            "Epoch 372/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5564 - acc: 0.9046 - val_loss: 1.6831 - val_acc: 0.7774\n",
            "Epoch 373/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5571 - acc: 0.9039 - val_loss: 1.6883 - val_acc: 0.7726\n",
            "Epoch 374/500\n",
            "79/79 [==============================] - 40s 511ms/step - loss: 1.5536 - acc: 0.9072 - val_loss: 1.6698 - val_acc: 0.7912\n",
            "Epoch 375/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5489 - acc: 0.9122 - val_loss: 1.6702 - val_acc: 0.7909\n",
            "Epoch 376/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5538 - acc: 0.9074 - val_loss: 1.7027 - val_acc: 0.7582\n",
            "Epoch 377/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5538 - acc: 0.9072 - val_loss: 1.6549 - val_acc: 0.8060\n",
            "Epoch 378/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5554 - acc: 0.9057 - val_loss: 1.6772 - val_acc: 0.7837\n",
            "Epoch 379/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5551 - acc: 0.9061 - val_loss: 1.6998 - val_acc: 0.7608\n",
            "Epoch 380/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5548 - acc: 0.9062 - val_loss: 1.7176 - val_acc: 0.7434\n",
            "Epoch 381/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5519 - acc: 0.9091 - val_loss: 1.6848 - val_acc: 0.7762\n",
            "Epoch 382/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5546 - acc: 0.9067 - val_loss: 1.6582 - val_acc: 0.8028\n",
            "Epoch 383/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5522 - acc: 0.9087 - val_loss: 1.7056 - val_acc: 0.7557\n",
            "Epoch 384/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5520 - acc: 0.9092 - val_loss: 1.6901 - val_acc: 0.7708\n",
            "Epoch 385/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5576 - acc: 0.9034 - val_loss: 1.6929 - val_acc: 0.7681\n",
            "Epoch 386/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5585 - acc: 0.9025 - val_loss: 1.7007 - val_acc: 0.7602\n",
            "Epoch 387/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5528 - acc: 0.9082 - val_loss: 1.6731 - val_acc: 0.7881\n",
            "Epoch 388/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5515 - acc: 0.9096 - val_loss: 1.6930 - val_acc: 0.7676\n",
            "Epoch 389/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5545 - acc: 0.9066 - val_loss: 1.6799 - val_acc: 0.7810\n",
            "Epoch 390/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5538 - acc: 0.9072 - val_loss: 1.6794 - val_acc: 0.7815\n",
            "Epoch 391/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5536 - acc: 0.9075 - val_loss: 1.6780 - val_acc: 0.7829\n",
            "Epoch 392/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5564 - acc: 0.9046 - val_loss: 1.6824 - val_acc: 0.7786\n",
            "Epoch 393/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5606 - acc: 0.9004 - val_loss: 1.6984 - val_acc: 0.7624\n",
            "Epoch 394/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5568 - acc: 0.9043 - val_loss: 1.7033 - val_acc: 0.7577\n",
            "Epoch 395/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5529 - acc: 0.9082 - val_loss: 1.6806 - val_acc: 0.7807\n",
            "Epoch 396/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5517 - acc: 0.9094 - val_loss: 1.6606 - val_acc: 0.8002\n",
            "Epoch 397/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5540 - acc: 0.9071 - val_loss: 1.6698 - val_acc: 0.7912\n",
            "Epoch 398/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5567 - acc: 0.9044 - val_loss: 1.6852 - val_acc: 0.7761\n",
            "Epoch 399/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5535 - acc: 0.9075 - val_loss: 1.6742 - val_acc: 0.7870\n",
            "Epoch 400/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5562 - acc: 0.9049 - val_loss: 1.6575 - val_acc: 0.8030\n",
            "Epoch 401/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5538 - acc: 0.9072 - val_loss: 1.6617 - val_acc: 0.7994\n",
            "Epoch 402/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5496 - acc: 0.9115 - val_loss: 1.6752 - val_acc: 0.7861\n",
            "Epoch 403/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5511 - acc: 0.9100 - val_loss: 1.6880 - val_acc: 0.7729\n",
            "Epoch 404/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5534 - acc: 0.9075 - val_loss: 1.6771 - val_acc: 0.7839\n",
            "Epoch 405/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5523 - acc: 0.9087 - val_loss: 1.7192 - val_acc: 0.7413\n",
            "Epoch 406/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5556 - acc: 0.9056 - val_loss: 1.7257 - val_acc: 0.7353\n",
            "Epoch 407/500\n",
            "79/79 [==============================] - 40s 510ms/step - loss: 1.5584 - acc: 0.9027 - val_loss: 1.6943 - val_acc: 0.7666\n",
            "Epoch 408/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5581 - acc: 0.9029 - val_loss: 1.7084 - val_acc: 0.7527\n",
            "Epoch 409/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5580 - acc: 0.9032 - val_loss: 1.7366 - val_acc: 0.7243\n",
            "Epoch 410/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5549 - acc: 0.9062 - val_loss: 1.6989 - val_acc: 0.7622\n",
            "Epoch 411/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5591 - acc: 0.9019 - val_loss: 1.6575 - val_acc: 0.8033\n",
            "Epoch 412/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5513 - acc: 0.9097 - val_loss: 1.6721 - val_acc: 0.7891\n",
            "Epoch 413/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5542 - acc: 0.9068 - val_loss: 1.7320 - val_acc: 0.7290\n",
            "Epoch 414/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5514 - acc: 0.9097 - val_loss: 1.6603 - val_acc: 0.8005\n",
            "Epoch 415/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5585 - acc: 0.9025 - val_loss: 1.6780 - val_acc: 0.7829\n",
            "Epoch 416/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5543 - acc: 0.9067 - val_loss: 1.6732 - val_acc: 0.7879\n",
            "Epoch 417/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5617 - acc: 0.8994 - val_loss: 1.6771 - val_acc: 0.7841\n",
            "Epoch 418/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5557 - acc: 0.9055 - val_loss: 1.6797 - val_acc: 0.7813\n",
            "Epoch 419/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5568 - acc: 0.9043 - val_loss: 1.6529 - val_acc: 0.8080\n",
            "Epoch 420/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5524 - acc: 0.9086 - val_loss: 1.6793 - val_acc: 0.7814\n",
            "Epoch 421/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5530 - acc: 0.9081 - val_loss: 1.6991 - val_acc: 0.7618\n",
            "Epoch 422/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5540 - acc: 0.9071 - val_loss: 1.6804 - val_acc: 0.7810\n",
            "Epoch 423/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5546 - acc: 0.9064 - val_loss: 1.6889 - val_acc: 0.7725\n",
            "Epoch 424/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5543 - acc: 0.9068 - val_loss: 1.6888 - val_acc: 0.7725\n",
            "Epoch 425/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5556 - acc: 0.9054 - val_loss: 1.6651 - val_acc: 0.7958\n",
            "Epoch 426/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5542 - acc: 0.9070 - val_loss: 1.7167 - val_acc: 0.7444\n",
            "Epoch 427/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5593 - acc: 0.9019 - val_loss: 1.6676 - val_acc: 0.7934\n",
            "Epoch 428/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5508 - acc: 0.9103 - val_loss: 1.6904 - val_acc: 0.7708\n",
            "Epoch 429/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5546 - acc: 0.9063 - val_loss: 1.6926 - val_acc: 0.7683\n",
            "Epoch 430/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5494 - acc: 0.9118 - val_loss: 1.7050 - val_acc: 0.7559\n",
            "Epoch 431/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5509 - acc: 0.9103 - val_loss: 1.6629 - val_acc: 0.7982\n",
            "Epoch 432/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5556 - acc: 0.9054 - val_loss: 1.6789 - val_acc: 0.7823\n",
            "Epoch 433/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5492 - acc: 0.9117 - val_loss: 1.6821 - val_acc: 0.7789\n",
            "Epoch 434/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5573 - acc: 0.9038 - val_loss: 1.6650 - val_acc: 0.7960\n",
            "Epoch 435/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5562 - acc: 0.9049 - val_loss: 1.6677 - val_acc: 0.7935\n",
            "Epoch 436/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5569 - acc: 0.9043 - val_loss: 1.6786 - val_acc: 0.7826\n",
            "Epoch 437/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5533 - acc: 0.9079 - val_loss: 1.6707 - val_acc: 0.7903\n",
            "Epoch 438/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5544 - acc: 0.9067 - val_loss: 1.7569 - val_acc: 0.7041\n",
            "Epoch 439/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5583 - acc: 0.9028 - val_loss: 1.6603 - val_acc: 0.8006\n",
            "Epoch 440/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5541 - acc: 0.9069 - val_loss: 1.6978 - val_acc: 0.7633\n",
            "Epoch 441/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5531 - acc: 0.9079 - val_loss: 1.6800 - val_acc: 0.7812\n",
            "Epoch 442/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5545 - acc: 0.9065 - val_loss: 1.6781 - val_acc: 0.7830\n",
            "Epoch 443/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5556 - acc: 0.9054 - val_loss: 1.6586 - val_acc: 0.8025\n",
            "Epoch 444/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5589 - acc: 0.9021 - val_loss: 1.6882 - val_acc: 0.7728\n",
            "Epoch 445/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5524 - acc: 0.9087 - val_loss: 1.6675 - val_acc: 0.7936\n",
            "Epoch 446/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5484 - acc: 0.9128 - val_loss: 1.6686 - val_acc: 0.7925\n",
            "Epoch 447/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5482 - acc: 0.9129 - val_loss: 1.6909 - val_acc: 0.7699\n",
            "Epoch 448/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5532 - acc: 0.9079 - val_loss: 1.6777 - val_acc: 0.7833\n",
            "Epoch 449/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5551 - acc: 0.9060 - val_loss: 1.6876 - val_acc: 0.7731\n",
            "Epoch 450/500\n",
            "79/79 [==============================] - 40s 506ms/step - loss: 1.5551 - acc: 0.9060 - val_loss: 1.6621 - val_acc: 0.7989\n",
            "Epoch 451/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5523 - acc: 0.9089 - val_loss: 1.7098 - val_acc: 0.7514\n",
            "Epoch 452/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5535 - acc: 0.9076 - val_loss: 1.6785 - val_acc: 0.7827\n",
            "Epoch 453/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5522 - acc: 0.9088 - val_loss: 1.6694 - val_acc: 0.7917\n",
            "Epoch 454/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5525 - acc: 0.9085 - val_loss: 1.6852 - val_acc: 0.7758\n",
            "Epoch 455/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5521 - acc: 0.9089 - val_loss: 1.6698 - val_acc: 0.7914\n",
            "Epoch 456/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5573 - acc: 0.9037 - val_loss: 1.6971 - val_acc: 0.7640\n",
            "Epoch 457/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5532 - acc: 0.9079 - val_loss: 1.7226 - val_acc: 0.7382\n",
            "Epoch 458/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5494 - acc: 0.9116 - val_loss: 1.6764 - val_acc: 0.7846\n",
            "Epoch 459/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5519 - acc: 0.9093 - val_loss: 1.7062 - val_acc: 0.7547\n",
            "Epoch 460/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5517 - acc: 0.9094 - val_loss: 1.6810 - val_acc: 0.7802\n",
            "Epoch 461/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5508 - acc: 0.9103 - val_loss: 1.7030 - val_acc: 0.7581\n",
            "Epoch 462/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5487 - acc: 0.9123 - val_loss: 1.6609 - val_acc: 0.8002\n",
            "Epoch 463/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5468 - acc: 0.9143 - val_loss: 1.6502 - val_acc: 0.8108\n",
            "Epoch 464/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5516 - acc: 0.9095 - val_loss: 1.7007 - val_acc: 0.7602\n",
            "Epoch 465/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5497 - acc: 0.9114 - val_loss: 1.6513 - val_acc: 0.8099\n",
            "Epoch 466/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5533 - acc: 0.9078 - val_loss: 1.6790 - val_acc: 0.7820\n",
            "Epoch 467/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5541 - acc: 0.9069 - val_loss: 1.6884 - val_acc: 0.7725\n",
            "Epoch 468/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5482 - acc: 0.9130 - val_loss: 1.6612 - val_acc: 0.7997\n",
            "Epoch 469/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5519 - acc: 0.9092 - val_loss: 1.6651 - val_acc: 0.7960\n",
            "Epoch 470/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5497 - acc: 0.9114 - val_loss: 1.6795 - val_acc: 0.7814\n",
            "Epoch 471/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5479 - acc: 0.9132 - val_loss: 1.6775 - val_acc: 0.7833\n",
            "Epoch 472/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5485 - acc: 0.9126 - val_loss: 1.6855 - val_acc: 0.7756\n",
            "Epoch 473/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5451 - acc: 0.9160 - val_loss: 1.6635 - val_acc: 0.7973\n",
            "Epoch 474/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5462 - acc: 0.9149 - val_loss: 1.6696 - val_acc: 0.7913\n",
            "Epoch 475/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5499 - acc: 0.9112 - val_loss: 1.6823 - val_acc: 0.7788\n",
            "Epoch 476/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5544 - acc: 0.9067 - val_loss: 1.6766 - val_acc: 0.7845\n",
            "Epoch 477/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5560 - acc: 0.9051 - val_loss: 1.6754 - val_acc: 0.7855\n",
            "Epoch 478/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5509 - acc: 0.9102 - val_loss: 1.6720 - val_acc: 0.7890\n",
            "Epoch 479/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5496 - acc: 0.9114 - val_loss: 1.7112 - val_acc: 0.7498\n",
            "Epoch 480/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5484 - acc: 0.9127 - val_loss: 1.6593 - val_acc: 0.8019\n",
            "Epoch 481/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5494 - acc: 0.9117 - val_loss: 1.7017 - val_acc: 0.7590\n",
            "Epoch 482/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5467 - acc: 0.9143 - val_loss: 1.6569 - val_acc: 0.8042\n",
            "Epoch 483/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5470 - acc: 0.9140 - val_loss: 1.6610 - val_acc: 0.8000\n",
            "Epoch 484/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5499 - acc: 0.9112 - val_loss: 1.6667 - val_acc: 0.7945\n",
            "Epoch 485/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5538 - acc: 0.9072 - val_loss: 1.6691 - val_acc: 0.7918\n",
            "Epoch 486/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5489 - acc: 0.9123 - val_loss: 1.6959 - val_acc: 0.7649\n",
            "Epoch 487/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5490 - acc: 0.9120 - val_loss: 1.6720 - val_acc: 0.7890\n",
            "Epoch 488/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5521 - acc: 0.9089 - val_loss: 1.6684 - val_acc: 0.7925\n",
            "Epoch 489/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5490 - acc: 0.9121 - val_loss: 1.7167 - val_acc: 0.7443\n",
            "Epoch 490/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5511 - acc: 0.9100 - val_loss: 1.6729 - val_acc: 0.7878\n",
            "Epoch 491/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5499 - acc: 0.9112 - val_loss: 1.6896 - val_acc: 0.7714\n",
            "Epoch 492/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5464 - acc: 0.9147 - val_loss: 1.6661 - val_acc: 0.7945\n",
            "Epoch 493/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5497 - acc: 0.9114 - val_loss: 1.7075 - val_acc: 0.7534\n",
            "Epoch 494/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5496 - acc: 0.9115 - val_loss: 1.6593 - val_acc: 0.8017\n",
            "Epoch 495/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5487 - acc: 0.9124 - val_loss: 1.6704 - val_acc: 0.7907\n",
            "Epoch 496/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5475 - acc: 0.9137 - val_loss: 1.6772 - val_acc: 0.7838\n",
            "Epoch 497/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5484 - acc: 0.9128 - val_loss: 1.6645 - val_acc: 0.7966\n",
            "Epoch 498/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5501 - acc: 0.9110 - val_loss: 1.6742 - val_acc: 0.7868\n",
            "Epoch 499/500\n",
            "79/79 [==============================] - 40s 508ms/step - loss: 1.5492 - acc: 0.9118 - val_loss: 1.6629 - val_acc: 0.7981\n",
            "Epoch 500/500\n",
            "79/79 [==============================] - 40s 507ms/step - loss: 1.5520 - acc: 0.9091 - val_loss: 1.7027 - val_acc: 0.7584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I27AdU-kyQ_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "07bde22a-e5ca-449c-90d9-d3877db4074a"
      },
      "source": [
        "evaluation = model.evaluate( x_test, y_test )\n",
        "print( f'loss: {evaluation[0]:.2f}, acc: {evaluation[1]*100:.2f}%' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 5s 15ms/step - loss: 1.7126 - acc: 0.7483\n",
            "loss: 1.71, acc: 74.83%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GOphUGLyQ_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save( model_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0WnDgZSyQ_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "98c0b014-9ed6-43b2-9f04-f6c0346714c0"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot( history.history['acc'] )\n",
        "plt.plot( history.history['val_acc'])\n",
        "plt.xlabel( 'epochs' )\n",
        "plt.ylabel( 'acc' )\n",
        "plt.legend( ['acc', 'val_acc'] )\n",
        "plt.title( model_name )\n",
        "\n",
        "plt.savefig( model_name + '.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}