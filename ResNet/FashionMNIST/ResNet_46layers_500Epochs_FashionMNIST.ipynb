{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ResNet_46layers_500Epochs_FashionMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taejin1221/MachineLearning/blob/master/ResNet/FashionMNIST/ResNet_46layers_500Epochs_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1F9jG3-0TPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "558667c4-2ebc-4f9e-eff9-bd069b510dde"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmZbJkvP0ckx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('drive/My Drive/Colab Notebooks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iJg6AkLyQ-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        " \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh2roVZhyQ-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKRNI2q2yQ-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input( shape = ( 28, 28, 1 ), name = 'input' )\n",
        "\n",
        "identity = layers.Conv2D( filters = 16, kernel_size = [ 7, 7 ], padding = 'Same', activation = 'relu' )(inputs)\n",
        "\n",
        "# block 1\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 16, kernel_size = [ 3, 3 ], padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 16, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_84Z42LyQ-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block 2\n",
        "identity = layers.ZeroPadding2D( [ 0, 8 ], 'channels_first' )(identity)\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 32, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 32, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18FDc_f9yQ-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block 3\n",
        "identity = layers.ZeroPadding2D( [ 0, 16 ], 'channels_first' )(identity)\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 64, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 64, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgZp3nGByQ-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = layers.GlobalAveragePooling2D()(identity)\n",
        "output = layers.Dense( 128, activation = 'relu' )(output)\n",
        "output = layers.Dense( 128, activation = 'relu' )(output)\n",
        "output = layers.Dense( 10, activation = 'softmax' )(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xeulYwgYyQ-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b564bee-36ba-44c2-a557-8075cfa8fd1c"
      },
      "source": [
        "model = keras.Model( inputs = inputs, outputs = output, name = 'resnet' )\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 28, 28, 16)   800         input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 28, 28, 16)   2320        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 28, 28, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 28, 28, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 28, 28, 16)   2320        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 28, 28, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 28, 28, 16)   0           batch_normalization_1[0][0]      \n",
            "                                                                 conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 28, 28, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 28, 28, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 28, 28, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 28, 28, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 28, 28, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 28, 28, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 28, 28, 16)   0           batch_normalization_3[0][0]      \n",
            "                                                                 activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 28, 28, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 28, 28, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 28, 28, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 28, 28, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 28, 28, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 28, 28, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 28, 28, 16)   0           batch_normalization_5[0][0]      \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 28, 28, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 28, 28, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 28, 28, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 28, 28, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 28, 28, 16)   2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 28, 28, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 28, 28, 16)   0           batch_normalization_7[0][0]      \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 28, 28, 16)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 28, 28, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 28, 28, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 28, 28, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 28, 28, 16)   2320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 28, 28, 16)   64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 16)   0           batch_normalization_9[0][0]      \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 28, 28, 16)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 28, 28, 16)   2320        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 28, 28, 16)   64          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 28, 28, 16)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 28, 28, 16)   2320        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 28, 28, 16)   64          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 16)   0           batch_normalization_11[0][0]     \n",
            "                                                                 activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 28, 28, 16)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 28, 28, 16)   2320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 28, 28, 16)   64          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 28, 28, 16)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 28, 28, 16)   2320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 28, 28, 16)   64          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 16)   0           batch_normalization_13[0][0]     \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 28, 28, 16)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 14, 14, 16)   0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 14, 14, 32)   0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 14, 14, 32)   9248        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 14, 14, 32)   128         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 14, 14, 32)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 14, 14, 32)   9248        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 14, 14, 32)   128         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 14, 14, 32)   0           batch_normalization_15[0][0]     \n",
            "                                                                 zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 14, 14, 32)   0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 14, 14, 32)   9248        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 14, 14, 32)   128         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 14, 14, 32)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 14, 14, 32)   9248        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 14, 14, 32)   128         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 14, 14, 32)   0           batch_normalization_17[0][0]     \n",
            "                                                                 activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 14, 14, 32)   0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 14, 14, 32)   9248        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 14, 14, 32)   128         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 14, 14, 32)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 14, 14, 32)   9248        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 14, 14, 32)   128         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 32)   0           batch_normalization_19[0][0]     \n",
            "                                                                 activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 14, 14, 32)   0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 14, 14, 32)   9248        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 14, 14, 32)   128         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 14, 14, 32)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 14, 14, 32)   9248        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 14, 14, 32)   128         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 32)   0           batch_normalization_21[0][0]     \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 14, 14, 32)   0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 14, 14, 32)   9248        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 14, 14, 32)   128         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 14, 14, 32)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 14, 14, 32)   9248        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 14, 14, 32)   128         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 32)   0           batch_normalization_23[0][0]     \n",
            "                                                                 activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 14, 14, 32)   0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 14, 14, 32)   9248        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 14, 14, 32)   128         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 14, 14, 32)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 14, 14, 32)   9248        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 14, 14, 32)   128         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 32)   0           batch_normalization_25[0][0]     \n",
            "                                                                 activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 14, 14, 32)   0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 14, 14, 32)   9248        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 14, 14, 32)   128         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 14, 14, 32)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 14, 14, 32)   9248        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 14, 14, 32)   128         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 32)   0           batch_normalization_27[0][0]     \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 14, 14, 32)   0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 32)     0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 7, 7, 64)     0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 7, 7, 64)     36928       zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 7, 7, 64)     256         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 7, 7, 64)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 7, 7, 64)     36928       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 7, 7, 64)     256         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 7, 7, 64)     0           batch_normalization_29[0][0]     \n",
            "                                                                 zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 7, 7, 64)     0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 7, 7, 64)     36928       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 7, 7, 64)     256         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 7, 7, 64)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 7, 7, 64)     36928       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 7, 7, 64)     256         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 7, 7, 64)     0           batch_normalization_31[0][0]     \n",
            "                                                                 activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 7, 7, 64)     0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 7, 7, 64)     36928       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 7, 7, 64)     256         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 7, 7, 64)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 7, 7, 64)     36928       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 7, 7, 64)     256         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 7, 7, 64)     0           batch_normalization_33[0][0]     \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 7, 7, 64)     0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 7, 7, 64)     36928       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 7, 7, 64)     256         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 7, 7, 64)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 7, 7, 64)     36928       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 7, 7, 64)     256         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 7, 7, 64)     0           batch_normalization_35[0][0]     \n",
            "                                                                 activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 7, 7, 64)     0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 7, 7, 64)     36928       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 7, 7, 64)     256         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 7, 7, 64)     0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 7, 7, 64)     36928       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 7, 7, 64)     256         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 7, 7, 64)     0           batch_normalization_37[0][0]     \n",
            "                                                                 activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 7, 7, 64)     0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 7, 7, 64)     36928       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 7, 7, 64)     256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 7, 7, 64)     0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 7, 7, 64)     36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 7, 7, 64)     256         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 7, 7, 64)     0           batch_normalization_39[0][0]     \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 7, 7, 64)     0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 7, 7, 64)     36928       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 7, 7, 64)     256         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 7, 7, 64)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 7, 7, 64)     36928       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 7, 7, 64)     256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 7, 7, 64)     0           batch_normalization_41[0][0]     \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 64)     0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 64)     0           activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 64)           0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          8320        global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          16512       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           1290        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 712,138\n",
            "Trainable params: 709,002\n",
            "Non-trainable params: 3,136\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxH_h9i2yQ-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-3\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 500\n",
        "model_name = 'ResNet_46Layers(500Epochs, Modified1, FashionMNIST)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z52y4GRPyQ-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "be333bac-49af-4fc5-cfc9-93ff0a78ad2b"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = keras.utils.to_categorical( y_train, 10 )\n",
        "y_test = keras.utils.to_categorical( y_test, 10 )\n",
        "\n",
        "model.compile( optimizer = keras.optimizers.RMSprop( lr, 0.9 ),\n",
        "             loss = keras.losses.CategoricalCrossentropy( from_logits = True ),\n",
        "              metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "z9BjknwUyQ-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bb5940d-32b8-4dbb-bc4d-1e2c64b3cd21"
      },
      "source": [
        "history = model.fit( x_train, y_train, batch_size = BATCH_SIZE,\n",
        "                    epochs = EPOCHS, validation_split = 0.2 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "94/94 [==============================] - 36s 380ms/step - loss: 2.2426 - acc: 0.2164 - val_loss: 2.3571 - val_acc: 0.1028\n",
            "Epoch 2/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 2.1811 - acc: 0.2780 - val_loss: 2.2176 - val_acc: 0.2372\n",
            "Epoch 3/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 2.0111 - acc: 0.4472 - val_loss: 2.2801 - val_acc: 0.1802\n",
            "Epoch 4/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.8868 - acc: 0.5715 - val_loss: 1.8599 - val_acc: 0.5999\n",
            "Epoch 5/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.7914 - acc: 0.6689 - val_loss: 1.7943 - val_acc: 0.6688\n",
            "Epoch 6/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.6987 - acc: 0.7625 - val_loss: 1.7107 - val_acc: 0.7552\n",
            "Epoch 7/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.6187 - acc: 0.8424 - val_loss: 1.7804 - val_acc: 0.6785\n",
            "Epoch 8/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.6019 - acc: 0.8586 - val_loss: 1.7438 - val_acc: 0.7162\n",
            "Epoch 9/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5882 - acc: 0.8724 - val_loss: 1.6087 - val_acc: 0.8523\n",
            "Epoch 10/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5838 - acc: 0.8769 - val_loss: 1.6106 - val_acc: 0.8498\n",
            "Epoch 11/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5776 - acc: 0.8831 - val_loss: 1.6255 - val_acc: 0.8361\n",
            "Epoch 12/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5726 - acc: 0.8881 - val_loss: 1.7134 - val_acc: 0.7459\n",
            "Epoch 13/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5698 - acc: 0.8907 - val_loss: 1.6273 - val_acc: 0.8334\n",
            "Epoch 14/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5676 - acc: 0.8933 - val_loss: 1.6835 - val_acc: 0.7763\n",
            "Epoch 15/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5663 - acc: 0.8943 - val_loss: 1.6056 - val_acc: 0.8544\n",
            "Epoch 16/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5632 - acc: 0.8977 - val_loss: 1.6507 - val_acc: 0.8093\n",
            "Epoch 17/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5629 - acc: 0.8975 - val_loss: 1.6137 - val_acc: 0.8473\n",
            "Epoch 18/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5582 - acc: 0.9028 - val_loss: 1.6135 - val_acc: 0.8467\n",
            "Epoch 19/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5595 - acc: 0.9013 - val_loss: 1.6379 - val_acc: 0.8231\n",
            "Epoch 20/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5571 - acc: 0.9036 - val_loss: 1.6106 - val_acc: 0.8493\n",
            "Epoch 21/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5561 - acc: 0.9048 - val_loss: 1.5800 - val_acc: 0.8807\n",
            "Epoch 22/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5568 - acc: 0.9037 - val_loss: 1.6489 - val_acc: 0.8112\n",
            "Epoch 23/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5541 - acc: 0.9066 - val_loss: 1.6057 - val_acc: 0.8547\n",
            "Epoch 24/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5526 - acc: 0.9079 - val_loss: 1.5983 - val_acc: 0.8622\n",
            "Epoch 25/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5502 - acc: 0.9106 - val_loss: 1.5821 - val_acc: 0.8782\n",
            "Epoch 26/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5516 - acc: 0.9094 - val_loss: 1.5974 - val_acc: 0.8627\n",
            "Epoch 27/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5499 - acc: 0.9110 - val_loss: 1.5782 - val_acc: 0.8821\n",
            "Epoch 28/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5478 - acc: 0.9134 - val_loss: 1.6298 - val_acc: 0.8298\n",
            "Epoch 29/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5491 - acc: 0.9117 - val_loss: 1.5774 - val_acc: 0.8833\n",
            "Epoch 30/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5468 - acc: 0.9142 - val_loss: 1.5777 - val_acc: 0.8827\n",
            "Epoch 31/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5470 - acc: 0.9138 - val_loss: 1.6242 - val_acc: 0.8363\n",
            "Epoch 32/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5468 - acc: 0.9140 - val_loss: 1.7556 - val_acc: 0.7048\n",
            "Epoch 33/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5449 - acc: 0.9160 - val_loss: 1.6072 - val_acc: 0.8539\n",
            "Epoch 34/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5467 - acc: 0.9143 - val_loss: 1.5825 - val_acc: 0.8783\n",
            "Epoch 35/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5470 - acc: 0.9137 - val_loss: 1.6123 - val_acc: 0.8485\n",
            "Epoch 36/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5460 - acc: 0.9149 - val_loss: 1.6199 - val_acc: 0.8405\n",
            "Epoch 37/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5471 - acc: 0.9136 - val_loss: 1.5878 - val_acc: 0.8733\n",
            "Epoch 38/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5456 - acc: 0.9153 - val_loss: 1.5810 - val_acc: 0.8798\n",
            "Epoch 39/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5425 - acc: 0.9181 - val_loss: 1.6486 - val_acc: 0.8119\n",
            "Epoch 40/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5451 - acc: 0.9158 - val_loss: 1.6267 - val_acc: 0.8343\n",
            "Epoch 41/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5466 - acc: 0.9144 - val_loss: 1.5866 - val_acc: 0.8739\n",
            "Epoch 42/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5434 - acc: 0.9174 - val_loss: 1.5773 - val_acc: 0.8832\n",
            "Epoch 43/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5433 - acc: 0.9174 - val_loss: 1.5854 - val_acc: 0.8754\n",
            "Epoch 44/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5429 - acc: 0.9182 - val_loss: 1.6369 - val_acc: 0.8238\n",
            "Epoch 45/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5432 - acc: 0.9179 - val_loss: 1.6232 - val_acc: 0.8370\n",
            "Epoch 46/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5457 - acc: 0.9152 - val_loss: 1.6360 - val_acc: 0.8247\n",
            "Epoch 47/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5423 - acc: 0.9187 - val_loss: 1.5602 - val_acc: 0.9007\n",
            "Epoch 48/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5394 - acc: 0.9216 - val_loss: 1.6142 - val_acc: 0.8467\n",
            "Epoch 49/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5405 - acc: 0.9202 - val_loss: 1.6009 - val_acc: 0.8600\n",
            "Epoch 50/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5409 - acc: 0.9199 - val_loss: 1.5829 - val_acc: 0.8777\n",
            "Epoch 51/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5403 - acc: 0.9209 - val_loss: 1.5686 - val_acc: 0.8926\n",
            "Epoch 52/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5392 - acc: 0.9219 - val_loss: 1.5697 - val_acc: 0.8907\n",
            "Epoch 53/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5414 - acc: 0.9194 - val_loss: 1.6088 - val_acc: 0.8519\n",
            "Epoch 54/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5419 - acc: 0.9190 - val_loss: 1.5952 - val_acc: 0.8652\n",
            "Epoch 55/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5435 - acc: 0.9176 - val_loss: 1.6031 - val_acc: 0.8577\n",
            "Epoch 56/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5419 - acc: 0.9191 - val_loss: 1.5653 - val_acc: 0.8958\n",
            "Epoch 57/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5379 - acc: 0.9229 - val_loss: 1.6002 - val_acc: 0.8609\n",
            "Epoch 58/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5372 - acc: 0.9237 - val_loss: 1.5988 - val_acc: 0.8621\n",
            "Epoch 59/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5374 - acc: 0.9237 - val_loss: 1.5751 - val_acc: 0.8858\n",
            "Epoch 60/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5376 - acc: 0.9233 - val_loss: 1.5661 - val_acc: 0.8947\n",
            "Epoch 61/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5366 - acc: 0.9244 - val_loss: 1.5933 - val_acc: 0.8674\n",
            "Epoch 62/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5407 - acc: 0.9203 - val_loss: 1.5900 - val_acc: 0.8709\n",
            "Epoch 63/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5416 - acc: 0.9193 - val_loss: 1.6298 - val_acc: 0.8308\n",
            "Epoch 64/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5432 - acc: 0.9176 - val_loss: 1.5991 - val_acc: 0.8620\n",
            "Epoch 65/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5394 - acc: 0.9216 - val_loss: 1.5905 - val_acc: 0.8704\n",
            "Epoch 66/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5391 - acc: 0.9219 - val_loss: 1.5739 - val_acc: 0.8871\n",
            "Epoch 67/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5390 - acc: 0.9219 - val_loss: 1.5933 - val_acc: 0.8673\n",
            "Epoch 68/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5388 - acc: 0.9224 - val_loss: 1.6546 - val_acc: 0.8061\n",
            "Epoch 69/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5393 - acc: 0.9216 - val_loss: 1.5684 - val_acc: 0.8925\n",
            "Epoch 70/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5383 - acc: 0.9227 - val_loss: 1.5756 - val_acc: 0.8850\n",
            "Epoch 71/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5351 - acc: 0.9259 - val_loss: 1.5868 - val_acc: 0.8743\n",
            "Epoch 72/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5356 - acc: 0.9255 - val_loss: 1.5622 - val_acc: 0.8988\n",
            "Epoch 73/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5378 - acc: 0.9232 - val_loss: 1.5702 - val_acc: 0.8906\n",
            "Epoch 74/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5356 - acc: 0.9253 - val_loss: 1.5527 - val_acc: 0.9080\n",
            "Epoch 75/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5354 - acc: 0.9257 - val_loss: 1.5947 - val_acc: 0.8656\n",
            "Epoch 76/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5358 - acc: 0.9252 - val_loss: 1.5941 - val_acc: 0.8664\n",
            "Epoch 77/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5350 - acc: 0.9258 - val_loss: 1.6055 - val_acc: 0.8552\n",
            "Epoch 78/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5324 - acc: 0.9286 - val_loss: 1.5862 - val_acc: 0.8745\n",
            "Epoch 79/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5342 - acc: 0.9268 - val_loss: 1.5508 - val_acc: 0.9103\n",
            "Epoch 80/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5351 - acc: 0.9259 - val_loss: 1.5614 - val_acc: 0.8997\n",
            "Epoch 81/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5354 - acc: 0.9255 - val_loss: 1.5641 - val_acc: 0.8972\n",
            "Epoch 82/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5353 - acc: 0.9256 - val_loss: 1.5826 - val_acc: 0.8784\n",
            "Epoch 83/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5345 - acc: 0.9265 - val_loss: 1.5954 - val_acc: 0.8651\n",
            "Epoch 84/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5348 - acc: 0.9262 - val_loss: 1.5931 - val_acc: 0.8677\n",
            "Epoch 85/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5330 - acc: 0.9279 - val_loss: 1.5549 - val_acc: 0.9061\n",
            "Epoch 86/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5334 - acc: 0.9275 - val_loss: 1.5583 - val_acc: 0.9025\n",
            "Epoch 87/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5351 - acc: 0.9259 - val_loss: 1.5641 - val_acc: 0.8968\n",
            "Epoch 88/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5347 - acc: 0.9262 - val_loss: 1.5672 - val_acc: 0.8938\n",
            "Epoch 89/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5344 - acc: 0.9265 - val_loss: 1.5555 - val_acc: 0.9055\n",
            "Epoch 90/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5338 - acc: 0.9274 - val_loss: 1.6138 - val_acc: 0.8476\n",
            "Epoch 91/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5345 - acc: 0.9264 - val_loss: 1.6445 - val_acc: 0.8167\n",
            "Epoch 92/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5346 - acc: 0.9264 - val_loss: 1.5620 - val_acc: 0.8991\n",
            "Epoch 93/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5333 - acc: 0.9278 - val_loss: 1.5745 - val_acc: 0.8865\n",
            "Epoch 94/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5364 - acc: 0.9244 - val_loss: 1.5601 - val_acc: 0.9006\n",
            "Epoch 95/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5328 - acc: 0.9283 - val_loss: 1.5843 - val_acc: 0.8765\n",
            "Epoch 96/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5339 - acc: 0.9272 - val_loss: 1.5713 - val_acc: 0.8896\n",
            "Epoch 97/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5325 - acc: 0.9287 - val_loss: 1.5513 - val_acc: 0.9097\n",
            "Epoch 98/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5319 - acc: 0.9291 - val_loss: 1.5908 - val_acc: 0.8702\n",
            "Epoch 99/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5351 - acc: 0.9258 - val_loss: 1.5801 - val_acc: 0.8809\n",
            "Epoch 100/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5309 - acc: 0.9302 - val_loss: 1.5581 - val_acc: 0.9033\n",
            "Epoch 101/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5321 - acc: 0.9289 - val_loss: 1.5637 - val_acc: 0.8970\n",
            "Epoch 102/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5310 - acc: 0.9300 - val_loss: 1.5615 - val_acc: 0.8992\n",
            "Epoch 103/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5332 - acc: 0.9279 - val_loss: 1.5716 - val_acc: 0.8892\n",
            "Epoch 104/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5331 - acc: 0.9280 - val_loss: 1.5832 - val_acc: 0.8777\n",
            "Epoch 105/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5337 - acc: 0.9273 - val_loss: 1.5686 - val_acc: 0.8925\n",
            "Epoch 106/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5364 - acc: 0.9247 - val_loss: 1.5812 - val_acc: 0.8798\n",
            "Epoch 107/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5310 - acc: 0.9301 - val_loss: 1.5691 - val_acc: 0.8919\n",
            "Epoch 108/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5335 - acc: 0.9276 - val_loss: 1.5683 - val_acc: 0.8928\n",
            "Epoch 109/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5339 - acc: 0.9270 - val_loss: 1.5773 - val_acc: 0.8837\n",
            "Epoch 110/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5312 - acc: 0.9298 - val_loss: 1.5704 - val_acc: 0.8904\n",
            "Epoch 111/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5331 - acc: 0.9280 - val_loss: 1.5611 - val_acc: 0.9000\n",
            "Epoch 112/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5296 - acc: 0.9315 - val_loss: 1.5645 - val_acc: 0.8965\n",
            "Epoch 113/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5336 - acc: 0.9274 - val_loss: 1.5717 - val_acc: 0.8896\n",
            "Epoch 114/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5334 - acc: 0.9275 - val_loss: 1.6405 - val_acc: 0.8204\n",
            "Epoch 115/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5337 - acc: 0.9274 - val_loss: 1.5834 - val_acc: 0.8775\n",
            "Epoch 116/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5328 - acc: 0.9283 - val_loss: 1.5546 - val_acc: 0.9063\n",
            "Epoch 117/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5375 - acc: 0.9236 - val_loss: 1.5790 - val_acc: 0.8819\n",
            "Epoch 118/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5317 - acc: 0.9294 - val_loss: 1.5557 - val_acc: 0.9054\n",
            "Epoch 119/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5281 - acc: 0.9330 - val_loss: 1.5751 - val_acc: 0.8860\n",
            "Epoch 120/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5288 - acc: 0.9323 - val_loss: 1.5593 - val_acc: 0.9015\n",
            "Epoch 121/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5316 - acc: 0.9295 - val_loss: 1.5846 - val_acc: 0.8763\n",
            "Epoch 122/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5280 - acc: 0.9330 - val_loss: 1.5691 - val_acc: 0.8917\n",
            "Epoch 123/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5303 - acc: 0.9308 - val_loss: 1.5550 - val_acc: 0.9058\n",
            "Epoch 124/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5302 - acc: 0.9310 - val_loss: 1.5603 - val_acc: 0.9004\n",
            "Epoch 125/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5345 - acc: 0.9265 - val_loss: 1.5981 - val_acc: 0.8627\n",
            "Epoch 126/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5298 - acc: 0.9312 - val_loss: 1.5866 - val_acc: 0.8743\n",
            "Epoch 127/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5308 - acc: 0.9302 - val_loss: 1.5760 - val_acc: 0.8849\n",
            "Epoch 128/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5286 - acc: 0.9325 - val_loss: 1.5648 - val_acc: 0.8964\n",
            "Epoch 129/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5303 - acc: 0.9308 - val_loss: 1.5725 - val_acc: 0.8883\n",
            "Epoch 130/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5299 - acc: 0.9310 - val_loss: 1.5758 - val_acc: 0.8850\n",
            "Epoch 131/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5299 - acc: 0.9312 - val_loss: 1.5627 - val_acc: 0.8983\n",
            "Epoch 132/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5303 - acc: 0.9308 - val_loss: 1.5482 - val_acc: 0.9129\n",
            "Epoch 133/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5317 - acc: 0.9293 - val_loss: 1.5508 - val_acc: 0.9101\n",
            "Epoch 134/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5257 - acc: 0.9354 - val_loss: 1.5765 - val_acc: 0.8847\n",
            "Epoch 135/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5294 - acc: 0.9316 - val_loss: 1.5533 - val_acc: 0.9078\n",
            "Epoch 136/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5263 - acc: 0.9348 - val_loss: 1.5550 - val_acc: 0.9057\n",
            "Epoch 137/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5279 - acc: 0.9331 - val_loss: 1.5649 - val_acc: 0.8957\n",
            "Epoch 138/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5271 - acc: 0.9339 - val_loss: 1.5609 - val_acc: 0.9001\n",
            "Epoch 139/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5265 - acc: 0.9344 - val_loss: 1.5796 - val_acc: 0.8813\n",
            "Epoch 140/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5301 - acc: 0.9309 - val_loss: 1.5513 - val_acc: 0.9097\n",
            "Epoch 141/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5265 - acc: 0.9345 - val_loss: 1.5802 - val_acc: 0.8811\n",
            "Epoch 142/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5253 - acc: 0.9358 - val_loss: 1.5517 - val_acc: 0.9093\n",
            "Epoch 143/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5269 - acc: 0.9342 - val_loss: 1.5542 - val_acc: 0.9069\n",
            "Epoch 144/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5269 - acc: 0.9341 - val_loss: 1.5653 - val_acc: 0.8957\n",
            "Epoch 145/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5298 - acc: 0.9313 - val_loss: 1.5509 - val_acc: 0.9102\n",
            "Epoch 146/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5291 - acc: 0.9321 - val_loss: 1.5790 - val_acc: 0.8820\n",
            "Epoch 147/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5299 - acc: 0.9311 - val_loss: 1.5691 - val_acc: 0.8920\n",
            "Epoch 148/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5285 - acc: 0.9325 - val_loss: 1.5657 - val_acc: 0.8953\n",
            "Epoch 149/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5302 - acc: 0.9309 - val_loss: 1.5564 - val_acc: 0.9047\n",
            "Epoch 150/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5269 - acc: 0.9342 - val_loss: 1.5871 - val_acc: 0.8742\n",
            "Epoch 151/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5294 - acc: 0.9317 - val_loss: 1.5678 - val_acc: 0.8932\n",
            "Epoch 152/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5276 - acc: 0.9335 - val_loss: 1.5804 - val_acc: 0.8805\n",
            "Epoch 153/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5260 - acc: 0.9350 - val_loss: 1.5558 - val_acc: 0.9049\n",
            "Epoch 154/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5288 - acc: 0.9323 - val_loss: 1.5558 - val_acc: 0.9052\n",
            "Epoch 155/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5350 - acc: 0.9260 - val_loss: 1.5583 - val_acc: 0.9027\n",
            "Epoch 156/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5278 - acc: 0.9332 - val_loss: 1.5714 - val_acc: 0.8898\n",
            "Epoch 157/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5311 - acc: 0.9300 - val_loss: 1.5551 - val_acc: 0.9056\n",
            "Epoch 158/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5328 - acc: 0.9283 - val_loss: 1.5679 - val_acc: 0.8932\n",
            "Epoch 159/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5281 - acc: 0.9330 - val_loss: 1.5581 - val_acc: 0.9032\n",
            "Epoch 160/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5270 - acc: 0.9340 - val_loss: 1.5743 - val_acc: 0.8867\n",
            "Epoch 161/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5327 - acc: 0.9285 - val_loss: 1.5607 - val_acc: 0.9002\n",
            "Epoch 162/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5275 - acc: 0.9336 - val_loss: 1.5688 - val_acc: 0.8918\n",
            "Epoch 163/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5270 - acc: 0.9341 - val_loss: 1.5692 - val_acc: 0.8918\n",
            "Epoch 164/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5290 - acc: 0.9322 - val_loss: 1.6029 - val_acc: 0.8584\n",
            "Epoch 165/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5318 - acc: 0.9292 - val_loss: 1.5568 - val_acc: 0.9042\n",
            "Epoch 166/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5274 - acc: 0.9338 - val_loss: 1.5847 - val_acc: 0.8763\n",
            "Epoch 167/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5320 - acc: 0.9290 - val_loss: 1.5783 - val_acc: 0.8828\n",
            "Epoch 168/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5269 - acc: 0.9342 - val_loss: 1.5612 - val_acc: 0.8998\n",
            "Epoch 169/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5308 - acc: 0.9302 - val_loss: 1.5585 - val_acc: 0.9025\n",
            "Epoch 170/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5353 - acc: 0.9258 - val_loss: 1.5491 - val_acc: 0.9120\n",
            "Epoch 171/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5290 - acc: 0.9321 - val_loss: 1.5485 - val_acc: 0.9124\n",
            "Epoch 172/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5276 - acc: 0.9334 - val_loss: 1.5766 - val_acc: 0.8845\n",
            "Epoch 173/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5290 - acc: 0.9321 - val_loss: 1.6158 - val_acc: 0.8452\n",
            "Epoch 174/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5279 - acc: 0.9332 - val_loss: 1.5644 - val_acc: 0.8967\n",
            "Epoch 175/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5283 - acc: 0.9329 - val_loss: 1.5656 - val_acc: 0.8956\n",
            "Epoch 176/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5293 - acc: 0.9317 - val_loss: 1.6038 - val_acc: 0.8573\n",
            "Epoch 177/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5306 - acc: 0.9304 - val_loss: 1.5844 - val_acc: 0.8767\n",
            "Epoch 178/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5297 - acc: 0.9314 - val_loss: 1.6340 - val_acc: 0.8269\n",
            "Epoch 179/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5284 - acc: 0.9327 - val_loss: 1.5549 - val_acc: 0.9063\n",
            "Epoch 180/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5330 - acc: 0.9281 - val_loss: 1.5739 - val_acc: 0.8872\n",
            "Epoch 181/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5308 - acc: 0.9302 - val_loss: 1.5609 - val_acc: 0.9000\n",
            "Epoch 182/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5310 - acc: 0.9300 - val_loss: 1.5680 - val_acc: 0.8931\n",
            "Epoch 183/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5309 - acc: 0.9301 - val_loss: 1.5736 - val_acc: 0.8875\n",
            "Epoch 184/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5276 - acc: 0.9334 - val_loss: 1.5730 - val_acc: 0.8881\n",
            "Epoch 185/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5275 - acc: 0.9335 - val_loss: 1.5706 - val_acc: 0.8905\n",
            "Epoch 186/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5280 - acc: 0.9331 - val_loss: 1.5711 - val_acc: 0.8898\n",
            "Epoch 187/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5251 - acc: 0.9360 - val_loss: 1.5518 - val_acc: 0.9090\n",
            "Epoch 188/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5296 - acc: 0.9314 - val_loss: 1.6423 - val_acc: 0.8189\n",
            "Epoch 189/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5341 - acc: 0.9271 - val_loss: 1.5572 - val_acc: 0.9038\n",
            "Epoch 190/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5260 - acc: 0.9351 - val_loss: 1.5698 - val_acc: 0.8913\n",
            "Epoch 191/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5291 - acc: 0.9321 - val_loss: 1.5524 - val_acc: 0.9087\n",
            "Epoch 192/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5254 - acc: 0.9357 - val_loss: 1.5558 - val_acc: 0.9053\n",
            "Epoch 193/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5295 - acc: 0.9317 - val_loss: 1.5521 - val_acc: 0.9090\n",
            "Epoch 194/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5247 - acc: 0.9365 - val_loss: 1.5469 - val_acc: 0.9141\n",
            "Epoch 195/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5261 - acc: 0.9350 - val_loss: 1.5662 - val_acc: 0.8947\n",
            "Epoch 196/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5280 - acc: 0.9330 - val_loss: 1.5591 - val_acc: 0.9020\n",
            "Epoch 197/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5263 - acc: 0.9348 - val_loss: 1.5604 - val_acc: 0.9003\n",
            "Epoch 198/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5248 - acc: 0.9363 - val_loss: 1.5458 - val_acc: 0.9152\n",
            "Epoch 199/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5265 - acc: 0.9346 - val_loss: 1.5648 - val_acc: 0.8963\n",
            "Epoch 200/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5261 - acc: 0.9349 - val_loss: 1.5569 - val_acc: 0.9044\n",
            "Epoch 201/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5255 - acc: 0.9356 - val_loss: 1.5562 - val_acc: 0.9051\n",
            "Epoch 202/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5265 - acc: 0.9346 - val_loss: 1.5610 - val_acc: 0.9000\n",
            "Epoch 203/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5232 - acc: 0.9379 - val_loss: 1.5625 - val_acc: 0.8985\n",
            "Epoch 204/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5252 - acc: 0.9359 - val_loss: 1.5850 - val_acc: 0.8758\n",
            "Epoch 205/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5237 - acc: 0.9374 - val_loss: 1.5427 - val_acc: 0.9185\n",
            "Epoch 206/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5247 - acc: 0.9364 - val_loss: 1.5788 - val_acc: 0.8822\n",
            "Epoch 207/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5352 - acc: 0.9258 - val_loss: 1.5565 - val_acc: 0.9047\n",
            "Epoch 208/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5268 - acc: 0.9342 - val_loss: 1.5553 - val_acc: 0.9059\n",
            "Epoch 209/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5236 - acc: 0.9375 - val_loss: 1.5568 - val_acc: 0.9044\n",
            "Epoch 210/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5274 - acc: 0.9337 - val_loss: 1.5639 - val_acc: 0.8972\n",
            "Epoch 211/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5251 - acc: 0.9360 - val_loss: 1.5472 - val_acc: 0.9139\n",
            "Epoch 212/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5268 - acc: 0.9341 - val_loss: 1.5542 - val_acc: 0.9068\n",
            "Epoch 213/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5293 - acc: 0.9318 - val_loss: 1.5559 - val_acc: 0.9050\n",
            "Epoch 214/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5259 - acc: 0.9350 - val_loss: 1.5430 - val_acc: 0.9178\n",
            "Epoch 215/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5242 - acc: 0.9369 - val_loss: 1.5502 - val_acc: 0.9107\n",
            "Epoch 216/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5208 - acc: 0.9403 - val_loss: 1.6013 - val_acc: 0.8602\n",
            "Epoch 217/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5236 - acc: 0.9374 - val_loss: 1.5557 - val_acc: 0.9058\n",
            "Epoch 218/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5246 - acc: 0.9365 - val_loss: 1.5550 - val_acc: 0.9061\n",
            "Epoch 219/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5249 - acc: 0.9361 - val_loss: 1.5834 - val_acc: 0.8776\n",
            "Epoch 220/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5229 - acc: 0.9381 - val_loss: 1.5570 - val_acc: 0.9043\n",
            "Epoch 221/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5216 - acc: 0.9396 - val_loss: 1.5498 - val_acc: 0.9111\n",
            "Epoch 222/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5256 - acc: 0.9355 - val_loss: 1.5574 - val_acc: 0.9034\n",
            "Epoch 223/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5225 - acc: 0.9385 - val_loss: 1.5808 - val_acc: 0.8801\n",
            "Epoch 224/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5239 - acc: 0.9371 - val_loss: 1.5651 - val_acc: 0.8961\n",
            "Epoch 225/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5248 - acc: 0.9365 - val_loss: 1.5453 - val_acc: 0.9158\n",
            "Epoch 226/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5257 - acc: 0.9354 - val_loss: 1.5515 - val_acc: 0.9093\n",
            "Epoch 227/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5241 - acc: 0.9371 - val_loss: 1.5536 - val_acc: 0.9073\n",
            "Epoch 228/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5471 - val_acc: 0.9143\n",
            "Epoch 229/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5257 - acc: 0.9355 - val_loss: 1.5546 - val_acc: 0.9064\n",
            "Epoch 230/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5215 - acc: 0.9397 - val_loss: 1.5524 - val_acc: 0.9086\n",
            "Epoch 231/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5216 - acc: 0.9394 - val_loss: 1.5446 - val_acc: 0.9163\n",
            "Epoch 232/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5210 - acc: 0.9401 - val_loss: 1.5460 - val_acc: 0.9152\n",
            "Epoch 233/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5248 - acc: 0.9363 - val_loss: 1.5578 - val_acc: 0.9031\n",
            "Epoch 234/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5259 - acc: 0.9352 - val_loss: 1.5599 - val_acc: 0.9012\n",
            "Epoch 235/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5227 - acc: 0.9384 - val_loss: 1.5495 - val_acc: 0.9113\n",
            "Epoch 236/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5225 - acc: 0.9385 - val_loss: 1.5460 - val_acc: 0.9151\n",
            "Epoch 237/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5236 - acc: 0.9374 - val_loss: 1.5630 - val_acc: 0.8980\n",
            "Epoch 238/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5226 - acc: 0.9385 - val_loss: 1.5537 - val_acc: 0.9074\n",
            "Epoch 239/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5198 - acc: 0.9412 - val_loss: 1.5478 - val_acc: 0.9133\n",
            "Epoch 240/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5239 - acc: 0.9372 - val_loss: 1.5517 - val_acc: 0.9093\n",
            "Epoch 241/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5248 - acc: 0.9362 - val_loss: 1.5622 - val_acc: 0.8988\n",
            "Epoch 242/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5250 - acc: 0.9361 - val_loss: 1.5583 - val_acc: 0.9027\n",
            "Epoch 243/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5223 - acc: 0.9388 - val_loss: 1.5487 - val_acc: 0.9125\n",
            "Epoch 244/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5205 - acc: 0.9407 - val_loss: 1.5474 - val_acc: 0.9137\n",
            "Epoch 245/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5206 - acc: 0.9404 - val_loss: 1.5570 - val_acc: 0.9041\n",
            "Epoch 246/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5220 - acc: 0.9392 - val_loss: 1.5599 - val_acc: 0.9013\n",
            "Epoch 247/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5256 - acc: 0.9355 - val_loss: 1.5642 - val_acc: 0.8968\n",
            "Epoch 248/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5232 - acc: 0.9379 - val_loss: 1.5574 - val_acc: 0.9037\n",
            "Epoch 249/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5239 - acc: 0.9371 - val_loss: 1.5779 - val_acc: 0.8832\n",
            "Epoch 250/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5261 - acc: 0.9351 - val_loss: 1.5651 - val_acc: 0.8960\n",
            "Epoch 251/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5226 - acc: 0.9385 - val_loss: 1.5587 - val_acc: 0.9025\n",
            "Epoch 252/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5214 - acc: 0.9397 - val_loss: 1.5478 - val_acc: 0.9135\n",
            "Epoch 253/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5258 - acc: 0.9352 - val_loss: 1.5695 - val_acc: 0.8914\n",
            "Epoch 254/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5246 - acc: 0.9366 - val_loss: 1.5512 - val_acc: 0.9098\n",
            "Epoch 255/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5237 - acc: 0.9375 - val_loss: 1.5608 - val_acc: 0.9002\n",
            "Epoch 256/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5198 - acc: 0.9413 - val_loss: 1.5627 - val_acc: 0.8983\n",
            "Epoch 257/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5203 - acc: 0.9407 - val_loss: 1.5460 - val_acc: 0.9150\n",
            "Epoch 258/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5188 - acc: 0.9424 - val_loss: 1.5466 - val_acc: 0.9145\n",
            "Epoch 259/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5228 - acc: 0.9383 - val_loss: 1.5542 - val_acc: 0.9069\n",
            "Epoch 260/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5215 - acc: 0.9396 - val_loss: 1.5677 - val_acc: 0.8932\n",
            "Epoch 261/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5209 - acc: 0.9401 - val_loss: 1.5559 - val_acc: 0.9053\n",
            "Epoch 262/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5212 - acc: 0.9400 - val_loss: 1.5491 - val_acc: 0.9119\n",
            "Epoch 263/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5220 - acc: 0.9391 - val_loss: 1.5400 - val_acc: 0.9212\n",
            "Epoch 264/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5260 - acc: 0.9351 - val_loss: 1.5490 - val_acc: 0.9121\n",
            "Epoch 265/500\n",
            "94/94 [==============================] - 34s 359ms/step - loss: 1.5269 - acc: 0.9342 - val_loss: 1.5500 - val_acc: 0.9111\n",
            "Epoch 266/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5250 - acc: 0.9361 - val_loss: 1.5484 - val_acc: 0.9128\n",
            "Epoch 267/500\n",
            "94/94 [==============================] - 34s 359ms/step - loss: 1.5224 - acc: 0.9387 - val_loss: 1.5565 - val_acc: 0.9047\n",
            "Epoch 268/500\n",
            "94/94 [==============================] - 34s 358ms/step - loss: 1.5234 - acc: 0.9377 - val_loss: 1.5544 - val_acc: 0.9068\n",
            "Epoch 269/500\n",
            "94/94 [==============================] - 34s 358ms/step - loss: 1.5215 - acc: 0.9396 - val_loss: 1.5621 - val_acc: 0.8991\n",
            "Epoch 270/500\n",
            "94/94 [==============================] - 34s 357ms/step - loss: 1.5222 - acc: 0.9390 - val_loss: 1.5443 - val_acc: 0.9168\n",
            "Epoch 271/500\n",
            "94/94 [==============================] - 33s 356ms/step - loss: 1.5209 - acc: 0.9402 - val_loss: 1.5501 - val_acc: 0.9110\n",
            "Epoch 272/500\n",
            "94/94 [==============================] - 34s 356ms/step - loss: 1.5215 - acc: 0.9396 - val_loss: 1.5533 - val_acc: 0.9078\n",
            "Epoch 273/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5231 - acc: 0.9380 - val_loss: 1.5515 - val_acc: 0.9094\n",
            "Epoch 274/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5242 - acc: 0.9370 - val_loss: 1.5461 - val_acc: 0.9151\n",
            "Epoch 275/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5208 - acc: 0.9404 - val_loss: 1.5539 - val_acc: 0.9072\n",
            "Epoch 276/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5217 - acc: 0.9394 - val_loss: 1.5689 - val_acc: 0.8923\n",
            "Epoch 277/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5220 - acc: 0.9391 - val_loss: 1.5478 - val_acc: 0.9133\n",
            "Epoch 278/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5220 - acc: 0.9392 - val_loss: 1.5629 - val_acc: 0.8982\n",
            "Epoch 279/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5231 - acc: 0.9380 - val_loss: 1.5494 - val_acc: 0.9118\n",
            "Epoch 280/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5470 - val_acc: 0.9141\n",
            "Epoch 281/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5234 - acc: 0.9377 - val_loss: 1.5597 - val_acc: 0.9013\n",
            "Epoch 282/500\n",
            "94/94 [==============================] - 34s 366ms/step - loss: 1.5211 - acc: 0.9399 - val_loss: 1.5658 - val_acc: 0.8953\n",
            "Epoch 283/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5217 - acc: 0.9394 - val_loss: 1.5557 - val_acc: 0.9056\n",
            "Epoch 284/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5192 - acc: 0.9418 - val_loss: 1.5406 - val_acc: 0.9205\n",
            "Epoch 285/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5221 - acc: 0.9389 - val_loss: 1.5526 - val_acc: 0.9085\n",
            "Epoch 286/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5211 - acc: 0.9400 - val_loss: 1.5626 - val_acc: 0.8983\n",
            "Epoch 287/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5225 - acc: 0.9385 - val_loss: 1.5508 - val_acc: 0.9105\n",
            "Epoch 288/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5188 - acc: 0.9423 - val_loss: 1.5535 - val_acc: 0.9077\n",
            "Epoch 289/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5616 - val_acc: 0.8997\n",
            "Epoch 290/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5458 - val_acc: 0.9154\n",
            "Epoch 291/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5191 - acc: 0.9420 - val_loss: 1.5501 - val_acc: 0.9108\n",
            "Epoch 292/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5199 - acc: 0.9412 - val_loss: 1.5675 - val_acc: 0.8937\n",
            "Epoch 293/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5187 - acc: 0.9424 - val_loss: 1.5567 - val_acc: 0.9044\n",
            "Epoch 294/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5217 - acc: 0.9394 - val_loss: 1.5420 - val_acc: 0.9191\n",
            "Epoch 295/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5182 - acc: 0.9429 - val_loss: 1.5465 - val_acc: 0.9144\n",
            "Epoch 296/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5208 - acc: 0.9402 - val_loss: 1.5577 - val_acc: 0.9032\n",
            "Epoch 297/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5213 - acc: 0.9399 - val_loss: 1.5520 - val_acc: 0.9092\n",
            "Epoch 298/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5240 - acc: 0.9371 - val_loss: 1.5492 - val_acc: 0.9119\n",
            "Epoch 299/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5474 - val_acc: 0.9137\n",
            "Epoch 300/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5208 - acc: 0.9404 - val_loss: 1.5498 - val_acc: 0.9114\n",
            "Epoch 301/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5234 - acc: 0.9378 - val_loss: 1.5599 - val_acc: 0.9012\n",
            "Epoch 302/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5188 - acc: 0.9423 - val_loss: 1.5474 - val_acc: 0.9137\n",
            "Epoch 303/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5208 - acc: 0.9404 - val_loss: 1.5520 - val_acc: 0.9093\n",
            "Epoch 304/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5222 - acc: 0.9388 - val_loss: 1.5485 - val_acc: 0.9128\n",
            "Epoch 305/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5188 - acc: 0.9423 - val_loss: 1.5468 - val_acc: 0.9144\n",
            "Epoch 306/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5190 - acc: 0.9422 - val_loss: 1.5557 - val_acc: 0.9053\n",
            "Epoch 307/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5192 - acc: 0.9420 - val_loss: 1.5574 - val_acc: 0.9034\n",
            "Epoch 308/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5205 - acc: 0.9406 - val_loss: 1.5531 - val_acc: 0.9080\n",
            "Epoch 309/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5249 - acc: 0.9362 - val_loss: 1.5714 - val_acc: 0.8897\n",
            "Epoch 310/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5203 - acc: 0.9407 - val_loss: 1.5558 - val_acc: 0.9053\n",
            "Epoch 311/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5172 - acc: 0.9440 - val_loss: 1.5470 - val_acc: 0.9143\n",
            "Epoch 312/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5217 - acc: 0.9393 - val_loss: 1.5499 - val_acc: 0.9112\n",
            "Epoch 313/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5185 - acc: 0.9426 - val_loss: 1.5476 - val_acc: 0.9134\n",
            "Epoch 314/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5193 - acc: 0.9417 - val_loss: 1.5584 - val_acc: 0.9027\n",
            "Epoch 315/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5204 - acc: 0.9407 - val_loss: 1.5481 - val_acc: 0.9128\n",
            "Epoch 316/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5186 - acc: 0.9425 - val_loss: 1.5404 - val_acc: 0.9208\n",
            "Epoch 317/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5176 - acc: 0.9435 - val_loss: 1.5687 - val_acc: 0.8925\n",
            "Epoch 318/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5171 - acc: 0.9440 - val_loss: 1.5492 - val_acc: 0.9119\n",
            "Epoch 319/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5214 - acc: 0.9397 - val_loss: 1.5422 - val_acc: 0.9189\n",
            "Epoch 320/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5208 - acc: 0.9403 - val_loss: 1.5551 - val_acc: 0.9061\n",
            "Epoch 321/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5190 - acc: 0.9422 - val_loss: 1.5519 - val_acc: 0.9092\n",
            "Epoch 322/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5166 - acc: 0.9445 - val_loss: 1.5478 - val_acc: 0.9134\n",
            "Epoch 323/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5196 - acc: 0.9416 - val_loss: 1.5481 - val_acc: 0.9129\n",
            "Epoch 324/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5150 - acc: 0.9461 - val_loss: 1.5609 - val_acc: 0.9003\n",
            "Epoch 325/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5197 - acc: 0.9414 - val_loss: 1.5524 - val_acc: 0.9087\n",
            "Epoch 326/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5154 - acc: 0.9457 - val_loss: 1.5595 - val_acc: 0.9016\n",
            "Epoch 327/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5168 - acc: 0.9443 - val_loss: 1.5553 - val_acc: 0.9057\n",
            "Epoch 328/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5172 - acc: 0.9440 - val_loss: 1.5492 - val_acc: 0.9119\n",
            "Epoch 329/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5176 - acc: 0.9435 - val_loss: 1.5457 - val_acc: 0.9155\n",
            "Epoch 330/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5227 - acc: 0.9383 - val_loss: 1.5448 - val_acc: 0.9163\n",
            "Epoch 331/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5198 - acc: 0.9412 - val_loss: 1.5581 - val_acc: 0.9030\n",
            "Epoch 332/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5171 - acc: 0.9440 - val_loss: 1.5536 - val_acc: 0.9076\n",
            "Epoch 333/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5170 - acc: 0.9441 - val_loss: 1.5505 - val_acc: 0.9103\n",
            "Epoch 334/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5162 - acc: 0.9449 - val_loss: 1.5519 - val_acc: 0.9092\n",
            "Epoch 335/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5172 - acc: 0.9439 - val_loss: 1.5600 - val_acc: 0.9012\n",
            "Epoch 336/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5215 - acc: 0.9396 - val_loss: 1.5414 - val_acc: 0.9197\n",
            "Epoch 337/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5148 - acc: 0.9463 - val_loss: 1.5453 - val_acc: 0.9160\n",
            "Epoch 338/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5215 - acc: 0.9396 - val_loss: 1.5506 - val_acc: 0.9104\n",
            "Epoch 339/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5217 - acc: 0.9394 - val_loss: 1.5544 - val_acc: 0.9064\n",
            "Epoch 340/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5198 - acc: 0.9413 - val_loss: 1.5442 - val_acc: 0.9167\n",
            "Epoch 341/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5207 - acc: 0.9405 - val_loss: 1.5546 - val_acc: 0.9064\n",
            "Epoch 342/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5189 - acc: 0.9422 - val_loss: 1.5586 - val_acc: 0.9023\n",
            "Epoch 343/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5173 - acc: 0.9438 - val_loss: 1.5481 - val_acc: 0.9129\n",
            "Epoch 344/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5166 - acc: 0.9445 - val_loss: 1.5540 - val_acc: 0.9070\n",
            "Epoch 345/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5605 - val_acc: 0.9003\n",
            "Epoch 346/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5207 - acc: 0.9404 - val_loss: 1.5490 - val_acc: 0.9121\n",
            "Epoch 347/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5193 - acc: 0.9417 - val_loss: 1.5485 - val_acc: 0.9125\n",
            "Epoch 348/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5173 - acc: 0.9439 - val_loss: 1.5650 - val_acc: 0.8962\n",
            "Epoch 349/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5178 - acc: 0.9433 - val_loss: 1.5605 - val_acc: 0.9007\n",
            "Epoch 350/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5212 - acc: 0.9399 - val_loss: 1.5927 - val_acc: 0.8683\n",
            "Epoch 351/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5157 - acc: 0.9455 - val_loss: 1.5415 - val_acc: 0.9196\n",
            "Epoch 352/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5189 - acc: 0.9422 - val_loss: 1.5431 - val_acc: 0.9179\n",
            "Epoch 353/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5159 - acc: 0.9452 - val_loss: 1.5533 - val_acc: 0.9078\n",
            "Epoch 354/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5146 - acc: 0.9465 - val_loss: 1.5501 - val_acc: 0.9107\n",
            "Epoch 355/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5171 - acc: 0.9440 - val_loss: 1.5446 - val_acc: 0.9166\n",
            "Epoch 356/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5187 - acc: 0.9424 - val_loss: 1.5516 - val_acc: 0.9096\n",
            "Epoch 357/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5155 - acc: 0.9456 - val_loss: 1.5473 - val_acc: 0.9137\n",
            "Epoch 358/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5158 - acc: 0.9453 - val_loss: 1.5413 - val_acc: 0.9197\n",
            "Epoch 359/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5154 - acc: 0.9457 - val_loss: 1.5405 - val_acc: 0.9207\n",
            "Epoch 360/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5142 - acc: 0.9469 - val_loss: 1.5446 - val_acc: 0.9165\n",
            "Epoch 361/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5158 - acc: 0.9453 - val_loss: 1.5607 - val_acc: 0.9004\n",
            "Epoch 362/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5163 - acc: 0.9449 - val_loss: 1.5482 - val_acc: 0.9130\n",
            "Epoch 363/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5175 - acc: 0.9436 - val_loss: 1.5416 - val_acc: 0.9195\n",
            "Epoch 364/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5161 - acc: 0.9450 - val_loss: 1.5522 - val_acc: 0.9088\n",
            "Epoch 365/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5157 - acc: 0.9454 - val_loss: 1.5417 - val_acc: 0.9193\n",
            "Epoch 366/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5143 - acc: 0.9468 - val_loss: 1.5476 - val_acc: 0.9136\n",
            "Epoch 367/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5167 - acc: 0.9444 - val_loss: 1.5376 - val_acc: 0.9237\n",
            "Epoch 368/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5144 - acc: 0.9467 - val_loss: 1.5547 - val_acc: 0.9063\n",
            "Epoch 369/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5162 - acc: 0.9449 - val_loss: 1.5448 - val_acc: 0.9163\n",
            "Epoch 370/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5164 - acc: 0.9447 - val_loss: 1.5480 - val_acc: 0.9130\n",
            "Epoch 371/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5180 - acc: 0.9432 - val_loss: 1.5465 - val_acc: 0.9143\n",
            "Epoch 372/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5169 - acc: 0.9443 - val_loss: 1.5729 - val_acc: 0.8882\n",
            "Epoch 373/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5154 - acc: 0.9458 - val_loss: 1.5546 - val_acc: 0.9063\n",
            "Epoch 374/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5156 - acc: 0.9455 - val_loss: 1.5495 - val_acc: 0.9116\n",
            "Epoch 375/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5179 - acc: 0.9432 - val_loss: 1.5557 - val_acc: 0.9054\n",
            "Epoch 376/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5183 - acc: 0.9428 - val_loss: 1.5429 - val_acc: 0.9181\n",
            "Epoch 377/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5160 - acc: 0.9450 - val_loss: 1.5492 - val_acc: 0.9118\n",
            "Epoch 378/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5153 - acc: 0.9458 - val_loss: 1.5444 - val_acc: 0.9168\n",
            "Epoch 379/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5164 - acc: 0.9448 - val_loss: 1.5559 - val_acc: 0.9051\n",
            "Epoch 380/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5156 - acc: 0.9456 - val_loss: 1.5595 - val_acc: 0.9018\n",
            "Epoch 381/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5174 - acc: 0.9438 - val_loss: 1.5453 - val_acc: 0.9158\n",
            "Epoch 382/500\n",
            "94/94 [==============================] - 34s 365ms/step - loss: 1.5170 - acc: 0.9442 - val_loss: 1.5444 - val_acc: 0.9168\n",
            "Epoch 383/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5161 - acc: 0.9451 - val_loss: 1.5573 - val_acc: 0.9039\n",
            "Epoch 384/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5197 - acc: 0.9414 - val_loss: 1.5583 - val_acc: 0.9029\n",
            "Epoch 385/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5179 - acc: 0.9433 - val_loss: 1.5450 - val_acc: 0.9162\n",
            "Epoch 386/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5200 - acc: 0.9411 - val_loss: 1.5607 - val_acc: 0.9003\n",
            "Epoch 387/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5171 - acc: 0.9440 - val_loss: 1.5432 - val_acc: 0.9179\n",
            "Epoch 388/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5151 - acc: 0.9460 - val_loss: 1.5403 - val_acc: 0.9208\n",
            "Epoch 389/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5169 - acc: 0.9442 - val_loss: 1.5463 - val_acc: 0.9145\n",
            "Epoch 390/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5188 - acc: 0.9423 - val_loss: 1.5477 - val_acc: 0.9134\n",
            "Epoch 391/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5184 - acc: 0.9426 - val_loss: 1.5655 - val_acc: 0.8957\n",
            "Epoch 392/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5182 - acc: 0.9429 - val_loss: 1.5461 - val_acc: 0.9150\n",
            "Epoch 393/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5165 - acc: 0.9446 - val_loss: 1.5443 - val_acc: 0.9169\n",
            "Epoch 394/500\n",
            "94/94 [==============================] - 34s 364ms/step - loss: 1.5167 - acc: 0.9444 - val_loss: 1.5437 - val_acc: 0.9173\n",
            "Epoch 395/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5132 - acc: 0.9479 - val_loss: 1.5723 - val_acc: 0.8889\n",
            "Epoch 396/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5130 - acc: 0.9482 - val_loss: 1.5394 - val_acc: 0.9217\n",
            "Epoch 397/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5131 - acc: 0.9481 - val_loss: 1.5426 - val_acc: 0.9187\n",
            "Epoch 398/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5135 - acc: 0.9477 - val_loss: 1.5411 - val_acc: 0.9199\n",
            "Epoch 399/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5164 - acc: 0.9447 - val_loss: 1.5474 - val_acc: 0.9136\n",
            "Epoch 400/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5145 - acc: 0.9467 - val_loss: 1.5405 - val_acc: 0.9205\n",
            "Epoch 401/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5177 - acc: 0.9435 - val_loss: 1.5482 - val_acc: 0.9128\n",
            "Epoch 402/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5160 - acc: 0.9451 - val_loss: 1.5547 - val_acc: 0.9065\n",
            "Epoch 403/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5144 - acc: 0.9467 - val_loss: 1.5443 - val_acc: 0.9168\n",
            "Epoch 404/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5176 - acc: 0.9434 - val_loss: 1.5514 - val_acc: 0.9097\n",
            "Epoch 405/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5168 - acc: 0.9443 - val_loss: 1.5475 - val_acc: 0.9136\n",
            "Epoch 406/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5187 - acc: 0.9424 - val_loss: 1.5457 - val_acc: 0.9153\n",
            "Epoch 407/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5185 - acc: 0.9426 - val_loss: 1.5648 - val_acc: 0.8963\n",
            "Epoch 408/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5205 - acc: 0.9405 - val_loss: 1.5519 - val_acc: 0.9090\n",
            "Epoch 409/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5130 - acc: 0.9480 - val_loss: 1.5563 - val_acc: 0.9048\n",
            "Epoch 410/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5139 - acc: 0.9473 - val_loss: 1.5513 - val_acc: 0.9096\n",
            "Epoch 411/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5132 - acc: 0.9479 - val_loss: 1.5488 - val_acc: 0.9124\n",
            "Epoch 412/500\n",
            "94/94 [==============================] - 34s 363ms/step - loss: 1.5154 - acc: 0.9457 - val_loss: 1.5532 - val_acc: 0.9080\n",
            "Epoch 413/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5181 - acc: 0.9430 - val_loss: 1.5672 - val_acc: 0.8938\n",
            "Epoch 414/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5167 - acc: 0.9444 - val_loss: 1.5459 - val_acc: 0.9151\n",
            "Epoch 415/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5140 - acc: 0.9471 - val_loss: 1.5480 - val_acc: 0.9133\n",
            "Epoch 416/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5160 - acc: 0.9451 - val_loss: 1.5486 - val_acc: 0.9126\n",
            "Epoch 417/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5186 - acc: 0.9426 - val_loss: 1.5487 - val_acc: 0.9124\n",
            "Epoch 418/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5166 - acc: 0.9445 - val_loss: 1.5487 - val_acc: 0.9124\n",
            "Epoch 419/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5147 - acc: 0.9465 - val_loss: 1.5440 - val_acc: 0.9171\n",
            "Epoch 420/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5170 - acc: 0.9442 - val_loss: 1.5432 - val_acc: 0.9179\n",
            "Epoch 421/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5117 - acc: 0.9494 - val_loss: 1.5468 - val_acc: 0.9143\n",
            "Epoch 422/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5173 - acc: 0.9438 - val_loss: 1.5794 - val_acc: 0.8815\n",
            "Epoch 423/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5195 - acc: 0.9416 - val_loss: 1.5580 - val_acc: 0.9032\n",
            "Epoch 424/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5162 - acc: 0.9449 - val_loss: 1.5482 - val_acc: 0.9128\n",
            "Epoch 425/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5178 - acc: 0.9433 - val_loss: 1.5428 - val_acc: 0.9182\n",
            "Epoch 426/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5128 - acc: 0.9482 - val_loss: 1.5434 - val_acc: 0.9178\n",
            "Epoch 427/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5178 - acc: 0.9433 - val_loss: 1.5491 - val_acc: 0.9120\n",
            "Epoch 428/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5157 - acc: 0.9454 - val_loss: 1.5450 - val_acc: 0.9162\n",
            "Epoch 429/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5150 - acc: 0.9462 - val_loss: 1.5485 - val_acc: 0.9125\n",
            "Epoch 430/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5146 - acc: 0.9465 - val_loss: 1.5387 - val_acc: 0.9223\n",
            "Epoch 431/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5144 - acc: 0.9468 - val_loss: 1.5442 - val_acc: 0.9168\n",
            "Epoch 432/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5181 - acc: 0.9431 - val_loss: 1.5487 - val_acc: 0.9122\n",
            "Epoch 433/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5152 - acc: 0.9459 - val_loss: 1.5443 - val_acc: 0.9168\n",
            "Epoch 434/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5168 - acc: 0.9444 - val_loss: 1.5560 - val_acc: 0.9051\n",
            "Epoch 435/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5182 - acc: 0.9429 - val_loss: 1.5404 - val_acc: 0.9208\n",
            "Epoch 436/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5126 - acc: 0.9485 - val_loss: 1.5443 - val_acc: 0.9169\n",
            "Epoch 437/500\n",
            "94/94 [==============================] - 34s 360ms/step - loss: 1.5124 - acc: 0.9488 - val_loss: 1.5550 - val_acc: 0.9061\n",
            "Epoch 438/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5150 - acc: 0.9461 - val_loss: 1.5449 - val_acc: 0.9162\n",
            "Epoch 439/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5138 - acc: 0.9474 - val_loss: 1.5557 - val_acc: 0.9054\n",
            "Epoch 440/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5158 - acc: 0.9454 - val_loss: 1.5472 - val_acc: 0.9139\n",
            "Epoch 441/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5158 - acc: 0.9452 - val_loss: 1.5435 - val_acc: 0.9175\n",
            "Epoch 442/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5162 - acc: 0.9450 - val_loss: 1.5427 - val_acc: 0.9182\n",
            "Epoch 443/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5121 - acc: 0.9490 - val_loss: 1.5443 - val_acc: 0.9168\n",
            "Epoch 444/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5142 - acc: 0.9469 - val_loss: 1.5473 - val_acc: 0.9137\n",
            "Epoch 445/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5189 - acc: 0.9422 - val_loss: 1.5589 - val_acc: 0.9022\n",
            "Epoch 446/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5187 - acc: 0.9424 - val_loss: 1.5510 - val_acc: 0.9101\n",
            "Epoch 447/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5186 - acc: 0.9425 - val_loss: 1.5501 - val_acc: 0.9111\n",
            "Epoch 448/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5167 - acc: 0.9445 - val_loss: 1.5682 - val_acc: 0.8926\n",
            "Epoch 449/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5171 - acc: 0.9440 - val_loss: 1.5535 - val_acc: 0.9075\n",
            "Epoch 450/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5128 - acc: 0.9483 - val_loss: 1.5419 - val_acc: 0.9192\n",
            "Epoch 451/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5190 - acc: 0.9421 - val_loss: 1.5561 - val_acc: 0.9051\n",
            "Epoch 452/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5178 - acc: 0.9433 - val_loss: 1.5428 - val_acc: 0.9182\n",
            "Epoch 453/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5139 - acc: 0.9472 - val_loss: 1.5772 - val_acc: 0.8840\n",
            "Epoch 454/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5160 - acc: 0.9451 - val_loss: 1.5452 - val_acc: 0.9160\n",
            "Epoch 455/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5174 - acc: 0.9436 - val_loss: 1.5439 - val_acc: 0.9173\n",
            "Epoch 456/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5161 - acc: 0.9450 - val_loss: 1.5543 - val_acc: 0.9069\n",
            "Epoch 457/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5131 - acc: 0.9480 - val_loss: 1.5446 - val_acc: 0.9166\n",
            "Epoch 458/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5142 - acc: 0.9470 - val_loss: 1.5431 - val_acc: 0.9180\n",
            "Epoch 459/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5156 - acc: 0.9454 - val_loss: 1.5475 - val_acc: 0.9136\n",
            "Epoch 460/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5165 - acc: 0.9446 - val_loss: 1.5501 - val_acc: 0.9111\n",
            "Epoch 461/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5104 - acc: 0.9507 - val_loss: 1.5426 - val_acc: 0.9184\n",
            "Epoch 462/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5142 - acc: 0.9469 - val_loss: 1.5473 - val_acc: 0.9138\n",
            "Epoch 463/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5168 - acc: 0.9444 - val_loss: 1.5487 - val_acc: 0.9125\n",
            "Epoch 464/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5163 - acc: 0.9448 - val_loss: 1.5465 - val_acc: 0.9146\n",
            "Epoch 465/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5178 - acc: 0.9433 - val_loss: 1.5575 - val_acc: 0.9036\n",
            "Epoch 466/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5205 - acc: 0.9407 - val_loss: 1.5754 - val_acc: 0.8856\n",
            "Epoch 467/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5180 - acc: 0.9432 - val_loss: 1.5437 - val_acc: 0.9176\n",
            "Epoch 468/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5159 - acc: 0.9452 - val_loss: 1.5444 - val_acc: 0.9166\n",
            "Epoch 469/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5141 - acc: 0.9471 - val_loss: 1.5510 - val_acc: 0.9101\n",
            "Epoch 470/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5171 - acc: 0.9440 - val_loss: 1.5689 - val_acc: 0.8922\n",
            "Epoch 471/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5168 - acc: 0.9443 - val_loss: 1.5572 - val_acc: 0.9038\n",
            "Epoch 472/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5215 - acc: 0.9396 - val_loss: 1.5509 - val_acc: 0.9103\n",
            "Epoch 473/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5197 - acc: 0.9414 - val_loss: 1.5506 - val_acc: 0.9103\n",
            "Epoch 474/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5493 - val_acc: 0.9118\n",
            "Epoch 475/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5153 - acc: 0.9458 - val_loss: 1.5450 - val_acc: 0.9162\n",
            "Epoch 476/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5159 - acc: 0.9452 - val_loss: 1.5697 - val_acc: 0.8913\n",
            "Epoch 477/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5150 - acc: 0.9461 - val_loss: 1.5543 - val_acc: 0.9067\n",
            "Epoch 478/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5169 - acc: 0.9442 - val_loss: 1.5459 - val_acc: 0.9153\n",
            "Epoch 479/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5137 - acc: 0.9473 - val_loss: 1.5393 - val_acc: 0.9218\n",
            "Epoch 480/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5138 - acc: 0.9473 - val_loss: 1.5511 - val_acc: 0.9099\n",
            "Epoch 481/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5144 - acc: 0.9467 - val_loss: 1.5589 - val_acc: 0.9022\n",
            "Epoch 482/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5141 - acc: 0.9470 - val_loss: 1.5514 - val_acc: 0.9097\n",
            "Epoch 483/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5151 - acc: 0.9461 - val_loss: 1.5448 - val_acc: 0.9163\n",
            "Epoch 484/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5135 - acc: 0.9476 - val_loss: 1.5407 - val_acc: 0.9206\n",
            "Epoch 485/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5190 - acc: 0.9421 - val_loss: 1.5647 - val_acc: 0.8964\n",
            "Epoch 486/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5161 - acc: 0.9450 - val_loss: 1.5427 - val_acc: 0.9183\n",
            "Epoch 487/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5125 - acc: 0.9486 - val_loss: 1.5539 - val_acc: 0.9070\n",
            "Epoch 488/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5146 - acc: 0.9465 - val_loss: 1.5434 - val_acc: 0.9176\n",
            "Epoch 489/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5151 - acc: 0.9460 - val_loss: 1.5536 - val_acc: 0.9077\n",
            "Epoch 490/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5123 - acc: 0.9489 - val_loss: 1.5418 - val_acc: 0.9192\n",
            "Epoch 491/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5139 - acc: 0.9472 - val_loss: 1.5549 - val_acc: 0.9062\n",
            "Epoch 492/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5149 - acc: 0.9463 - val_loss: 1.5542 - val_acc: 0.9068\n",
            "Epoch 493/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5140 - acc: 0.9472 - val_loss: 1.5510 - val_acc: 0.9100\n",
            "Epoch 494/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5126 - acc: 0.9485 - val_loss: 1.5474 - val_acc: 0.9137\n",
            "Epoch 495/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5114 - acc: 0.9496 - val_loss: 1.5458 - val_acc: 0.9153\n",
            "Epoch 496/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5139 - acc: 0.9471 - val_loss: 1.5414 - val_acc: 0.9196\n",
            "Epoch 497/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5113 - acc: 0.9498 - val_loss: 1.5514 - val_acc: 0.9095\n",
            "Epoch 498/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5145 - acc: 0.9465 - val_loss: 1.5576 - val_acc: 0.9036\n",
            "Epoch 499/500\n",
            "94/94 [==============================] - 34s 361ms/step - loss: 1.5146 - acc: 0.9465 - val_loss: 1.5581 - val_acc: 0.9031\n",
            "Epoch 500/500\n",
            "94/94 [==============================] - 34s 362ms/step - loss: 1.5145 - acc: 0.9466 - val_loss: 1.5423 - val_acc: 0.9188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I27AdU-kyQ_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8048be21-55b8-4ef6-cea5-1afc3b2b2b55"
      },
      "source": [
        "evaluation = model.evaluate( x_test, y_test )\n",
        "print( f'loss: {evaluation[0]:.2f}, acc: {evaluation[1]*100:.2f}%' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 4s 14ms/step - loss: 1.5456 - acc: 0.9155\n",
            "loss: 1.55, acc: 91.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GOphUGLyQ_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save( model_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0WnDgZSyQ_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d23f2598-2471-4f8d-e093-6f86b5d557e3"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot( history.history['acc'] )\n",
        "plt.plot( history.history['val_acc'])\n",
        "plt.xlabel( 'epochs' )\n",
        "plt.ylabel( 'acc' )\n",
        "plt.legend( ['acc', 'val_acc'] )\n",
        "plt.title( model_name )\n",
        "\n",
        "plt.savefig( model_name + '.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}