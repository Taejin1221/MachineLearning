{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ResNet_58layers_500Epochs_FashionMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taejin1221/MachineLearning/blob/master/ResNet/FashionMNIST/ResNet_58layers_500Epochs_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4RUB4HR0BYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "8aa2c7ac-9ad9-4e23-c52e-e1403b9b3978"
      },
      "source": [
        "from google.colab import drive # for google colab\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jW0OUJRA5Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('drive/My Drive/Colab Notebooks/') # Drive directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iJg6AkLyQ-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh2roVZhyQ-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKRNI2q2yQ-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input( shape = ( 28, 28, 1 ), name = 'input' )\n",
        "\n",
        "identity = layers.Conv2D( filters = 16, kernel_size = [ 7, 7 ], padding = 'Same', activation = 'relu' )(inputs)\n",
        "\n",
        "# block 1\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 16, kernel_size = [ 3, 3 ], padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 16, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_84Z42LyQ-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block 2\n",
        "identity = layers.ZeroPadding2D( [ 0, 8 ], 'channels_first' )(identity)\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 32, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 32, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18FDc_f9yQ-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# block 3\n",
        "identity = layers.ZeroPadding2D( [ 0, 16 ], 'channels_first' )(identity)\n",
        "for _ in range( n ):\n",
        "    output = layers.Conv2D( filters = 64, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(identity)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Activation('relu')(output)\n",
        "    \n",
        "    output = layers.Conv2D( filters = 64, kernel_size = [ 3, 3 ],\n",
        "                           padding = 'Same' )(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    \n",
        "    output = layers.Add()( [ output, identity ] )\n",
        "    identity = layers.Activation('relu')(output)\n",
        "\n",
        "identity = layers.MaxPooling2D( pool_size = [ 3, 3 ], padding = 'same',\n",
        "                               strides = 2 )(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgZp3nGByQ-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = layers.GlobalAveragePooling2D()(identity)\n",
        "output = layers.Dense( 128, activation = 'relu' )(output)\n",
        "output = layers.Dense( 128, activation = 'relu' )(output)\n",
        "output = layers.Dense( 10, activation = 'softmax' )(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xeulYwgYyQ-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c6178a4-24c6-4ed6-bc52-a685af6630e4"
      },
      "source": [
        "model = keras.Model( inputs = inputs, outputs = output, name = 'resnet' )\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 28, 28, 16)   800         input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 28, 28, 16)   2320        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 28, 28, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 28, 28, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 28, 28, 16)   2320        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 28, 28, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 28, 28, 16)   0           batch_normalization_1[0][0]      \n",
            "                                                                 conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 28, 28, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 28, 28, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 28, 28, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 28, 28, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 28, 28, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 28, 28, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 28, 28, 16)   0           batch_normalization_3[0][0]      \n",
            "                                                                 activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 28, 28, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 28, 28, 16)   2320        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 28, 28, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 28, 28, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 28, 28, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 28, 28, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 28, 28, 16)   0           batch_normalization_5[0][0]      \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 28, 28, 16)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 28, 28, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 28, 28, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 28, 28, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 28, 28, 16)   2320        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 28, 28, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 28, 28, 16)   0           batch_normalization_7[0][0]      \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 28, 28, 16)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 28, 28, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 28, 28, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 28, 28, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 28, 28, 16)   2320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 28, 28, 16)   64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 28, 28, 16)   0           batch_normalization_9[0][0]      \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 28, 28, 16)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 28, 28, 16)   2320        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 28, 28, 16)   64          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 28, 28, 16)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 28, 28, 16)   2320        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 28, 28, 16)   64          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 28, 28, 16)   0           batch_normalization_11[0][0]     \n",
            "                                                                 activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 28, 28, 16)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 28, 28, 16)   2320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 28, 28, 16)   64          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 28, 28, 16)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 28, 28, 16)   2320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 28, 28, 16)   64          conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 28, 28, 16)   0           batch_normalization_13[0][0]     \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 28, 28, 16)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 28, 28, 16)   2320        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 28, 28, 16)   64          conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 28, 28, 16)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 28, 28, 16)   2320        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 28, 28, 16)   64          conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 28, 28, 16)   0           batch_normalization_15[0][0]     \n",
            "                                                                 activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 28, 28, 16)   0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 28, 28, 16)   2320        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 28, 28, 16)   64          conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 28, 28, 16)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 28, 28, 16)   2320        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 28, 28, 16)   64          conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 28, 28, 16)   0           batch_normalization_17[0][0]     \n",
            "                                                                 activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 28, 28, 16)   0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 14, 14, 16)   0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 14, 14, 32)   0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 14, 14, 32)   9248        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 14, 14, 32)   128         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 14, 14, 32)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 14, 14, 32)   9248        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 14, 14, 32)   128         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 14, 14, 32)   0           batch_normalization_19[0][0]     \n",
            "                                                                 zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 14, 14, 32)   0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 14, 14, 32)   9248        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 14, 14, 32)   128         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 14, 14, 32)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 14, 14, 32)   9248        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 14, 14, 32)   128         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 14, 14, 32)   0           batch_normalization_21[0][0]     \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 14, 14, 32)   0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 14, 14, 32)   9248        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 14, 14, 32)   128         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 14, 14, 32)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 14, 14, 32)   9248        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 14, 14, 32)   128         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 14, 14, 32)   0           batch_normalization_23[0][0]     \n",
            "                                                                 activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 14, 14, 32)   0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 14, 14, 32)   9248        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 14, 14, 32)   128         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 14, 14, 32)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 14, 14, 32)   9248        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 14, 14, 32)   128         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 14, 14, 32)   0           batch_normalization_25[0][0]     \n",
            "                                                                 activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 14, 14, 32)   0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 14, 14, 32)   9248        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 14, 14, 32)   128         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 14, 14, 32)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 14, 14, 32)   9248        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 14, 14, 32)   128         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 32)   0           batch_normalization_27[0][0]     \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 14, 14, 32)   0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 14, 14, 32)   9248        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 14, 14, 32)   128         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 14, 14, 32)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 14, 14, 32)   9248        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 14, 14, 32)   128         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 14, 14, 32)   0           batch_normalization_29[0][0]     \n",
            "                                                                 activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 14, 14, 32)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 14, 14, 32)   9248        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 14, 14, 32)   128         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 14, 14, 32)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 14, 14, 32)   9248        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 14, 14, 32)   128         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 14, 14, 32)   0           batch_normalization_31[0][0]     \n",
            "                                                                 activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 14, 14, 32)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 14, 14, 32)   9248        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 14, 14, 32)   128         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 14, 14, 32)   0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 14, 14, 32)   9248        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 14, 14, 32)   128         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 14, 14, 32)   0           batch_normalization_33[0][0]     \n",
            "                                                                 activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 14, 14, 32)   0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 14, 14, 32)   9248        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 14, 14, 32)   128         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 14, 14, 32)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 14, 14, 32)   9248        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 14, 14, 32)   128         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 14, 14, 32)   0           batch_normalization_35[0][0]     \n",
            "                                                                 activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 14, 14, 32)   0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 32)     0           activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 7, 7, 64)     0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 7, 7, 64)     36928       zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 7, 7, 64)     256         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 7, 7, 64)     0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 7, 7, 64)     36928       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 7, 7, 64)     256         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 7, 7, 64)     0           batch_normalization_37[0][0]     \n",
            "                                                                 zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 7, 7, 64)     0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 7, 7, 64)     36928       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 7, 7, 64)     256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 7, 7, 64)     0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 7, 7, 64)     36928       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 7, 7, 64)     256         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 7, 7, 64)     0           batch_normalization_39[0][0]     \n",
            "                                                                 activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 7, 7, 64)     0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 7, 7, 64)     36928       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 7, 7, 64)     256         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 7, 7, 64)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 7, 7, 64)     36928       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 7, 7, 64)     256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 7, 7, 64)     0           batch_normalization_41[0][0]     \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 64)     0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 7, 7, 64)     36928       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 7, 7, 64)     256         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 64)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 7, 7, 64)     36928       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 7, 7, 64)     256         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 7, 7, 64)     0           batch_normalization_43[0][0]     \n",
            "                                                                 activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 64)     0           add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 7, 7, 64)     36928       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 7, 7, 64)     256         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 64)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 7, 7, 64)     36928       activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 7, 7, 64)     256         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 7, 7, 64)     0           batch_normalization_45[0][0]     \n",
            "                                                                 activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 64)     0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 7, 7, 64)     36928       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 7, 7, 64)     256         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 64)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 7, 7, 64)     36928       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 7, 7, 64)     256         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 7, 7, 64)     0           batch_normalization_47[0][0]     \n",
            "                                                                 activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 64)     0           add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 7, 7, 64)     36928       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 7, 7, 64)     256         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 64)     0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 7, 7, 64)     36928       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 7, 7, 64)     256         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 7, 7, 64)     0           batch_normalization_49[0][0]     \n",
            "                                                                 activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 7, 7, 64)     0           add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 7, 7, 64)     36928       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 7, 7, 64)     256         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 7, 7, 64)     0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 7, 7, 64)     36928       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 7, 7, 64)     256         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 7, 7, 64)     0           batch_normalization_51[0][0]     \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 7, 7, 64)     0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 7, 7, 64)     36928       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 7, 7, 64)     256         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 7, 7, 64)     0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 7, 7, 64)     36928       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 7, 7, 64)     256         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 7, 7, 64)     0           batch_normalization_53[0][0]     \n",
            "                                                                 activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 7, 7, 64)     0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 64)     0           activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 64)           0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          8320        global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          16512       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           1290        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 907,914\n",
            "Trainable params: 903,882\n",
            "Non-trainable params: 4,032\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxH_h9i2yQ-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters\n",
        "lr = 1e-3\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 500\n",
        "model_name = 'ResNet_58Layers(500Epochs, Modified1, FashionMNIST)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z52y4GRPyQ-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "a706e907-5fea-4ddd-f0df-e23045c1bb1a"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = keras.utils.to_categorical( y_train, 10 )\n",
        "y_test = keras.utils.to_categorical( y_test, 10 )\n",
        "\n",
        "model.compile( optimizer = keras.optimizers.RMSprop( lr, 0.9 ),\n",
        "             loss = keras.losses.CategoricalCrossentropy( from_logits = True ),\n",
        "              metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "z9BjknwUyQ-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "367c1fe1-eadc-463a-83da-63cbdae6b6e2"
      },
      "source": [
        "history = model.fit( x_train, y_train, batch_size = BATCH_SIZE,\n",
        "                    epochs = EPOCHS, validation_split = 0.2 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "94/94 [==============================] - 22s 235ms/step - loss: 2.0341 - acc: 0.4241 - val_loss: 2.3581 - val_acc: 0.1030\n",
            "Epoch 2/500\n",
            "94/94 [==============================] - 21s 225ms/step - loss: 1.8318 - acc: 0.6286 - val_loss: 2.3580 - val_acc: 0.1032\n",
            "Epoch 3/500\n",
            "94/94 [==============================] - 21s 228ms/step - loss: 1.7963 - acc: 0.6638 - val_loss: 2.2126 - val_acc: 0.2438\n",
            "Epoch 4/500\n",
            "94/94 [==============================] - 22s 232ms/step - loss: 1.7450 - acc: 0.7153 - val_loss: 1.9915 - val_acc: 0.4628\n",
            "Epoch 5/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.7266 - acc: 0.7337 - val_loss: 1.8078 - val_acc: 0.6532\n",
            "Epoch 6/500\n",
            "94/94 [==============================] - 22s 229ms/step - loss: 1.7004 - acc: 0.7598 - val_loss: 1.7896 - val_acc: 0.6693\n",
            "Epoch 7/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.6900 - acc: 0.7697 - val_loss: 1.6942 - val_acc: 0.7653\n",
            "Epoch 8/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.6205 - acc: 0.8412 - val_loss: 1.6530 - val_acc: 0.8073\n",
            "Epoch 9/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5890 - acc: 0.8719 - val_loss: 1.7290 - val_acc: 0.7327\n",
            "Epoch 10/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5847 - acc: 0.8762 - val_loss: 1.6103 - val_acc: 0.8503\n",
            "Epoch 11/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5806 - acc: 0.8798 - val_loss: 1.8042 - val_acc: 0.6546\n",
            "Epoch 12/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5754 - acc: 0.8849 - val_loss: 1.6399 - val_acc: 0.8202\n",
            "Epoch 13/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5725 - acc: 0.8884 - val_loss: 1.6267 - val_acc: 0.8333\n",
            "Epoch 14/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5727 - acc: 0.8881 - val_loss: 1.6087 - val_acc: 0.8516\n",
            "Epoch 15/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5698 - acc: 0.8908 - val_loss: 1.6781 - val_acc: 0.7821\n",
            "Epoch 16/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5685 - acc: 0.8922 - val_loss: 1.5783 - val_acc: 0.8824\n",
            "Epoch 17/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5670 - acc: 0.8940 - val_loss: 1.6484 - val_acc: 0.8123\n",
            "Epoch 18/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5649 - acc: 0.8958 - val_loss: 1.6526 - val_acc: 0.8077\n",
            "Epoch 19/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5615 - acc: 0.8993 - val_loss: 1.5963 - val_acc: 0.8643\n",
            "Epoch 20/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5633 - acc: 0.8976 - val_loss: 1.6369 - val_acc: 0.8232\n",
            "Epoch 21/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5609 - acc: 0.8997 - val_loss: 1.5855 - val_acc: 0.8756\n",
            "Epoch 22/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5601 - acc: 0.9010 - val_loss: 1.6217 - val_acc: 0.8390\n",
            "Epoch 23/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5580 - acc: 0.9028 - val_loss: 1.6275 - val_acc: 0.8337\n",
            "Epoch 24/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5569 - acc: 0.9039 - val_loss: 1.6835 - val_acc: 0.7771\n",
            "Epoch 25/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5584 - acc: 0.9022 - val_loss: 1.6147 - val_acc: 0.8456\n",
            "Epoch 26/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5579 - acc: 0.9031 - val_loss: 1.5836 - val_acc: 0.8775\n",
            "Epoch 27/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5593 - acc: 0.9014 - val_loss: 1.6574 - val_acc: 0.8032\n",
            "Epoch 28/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5555 - acc: 0.9054 - val_loss: 1.5789 - val_acc: 0.8823\n",
            "Epoch 29/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5577 - acc: 0.9031 - val_loss: 1.6266 - val_acc: 0.8342\n",
            "Epoch 30/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5550 - acc: 0.9057 - val_loss: 1.5874 - val_acc: 0.8730\n",
            "Epoch 31/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5561 - acc: 0.9049 - val_loss: 1.5937 - val_acc: 0.8673\n",
            "Epoch 32/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5549 - acc: 0.9058 - val_loss: 1.5846 - val_acc: 0.8761\n",
            "Epoch 33/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5546 - acc: 0.9062 - val_loss: 1.5884 - val_acc: 0.8722\n",
            "Epoch 34/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5569 - acc: 0.9039 - val_loss: 1.6457 - val_acc: 0.8144\n",
            "Epoch 35/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5529 - acc: 0.9081 - val_loss: 1.5721 - val_acc: 0.8886\n",
            "Epoch 36/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5532 - acc: 0.9078 - val_loss: 1.5773 - val_acc: 0.8838\n",
            "Epoch 37/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5534 - acc: 0.9078 - val_loss: 1.6846 - val_acc: 0.7763\n",
            "Epoch 38/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5526 - acc: 0.9083 - val_loss: 1.6010 - val_acc: 0.8593\n",
            "Epoch 39/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5514 - acc: 0.9095 - val_loss: 1.5799 - val_acc: 0.8802\n",
            "Epoch 40/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5533 - acc: 0.9075 - val_loss: 1.5756 - val_acc: 0.8857\n",
            "Epoch 41/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5507 - acc: 0.9102 - val_loss: 1.6268 - val_acc: 0.8334\n",
            "Epoch 42/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5541 - acc: 0.9069 - val_loss: 1.6184 - val_acc: 0.8428\n",
            "Epoch 43/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5483 - acc: 0.9124 - val_loss: 1.7212 - val_acc: 0.7390\n",
            "Epoch 44/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5523 - acc: 0.9087 - val_loss: 1.5868 - val_acc: 0.8740\n",
            "Epoch 45/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5510 - acc: 0.9099 - val_loss: 1.5674 - val_acc: 0.8934\n",
            "Epoch 46/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5479 - acc: 0.9133 - val_loss: 1.5938 - val_acc: 0.8667\n",
            "Epoch 47/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5481 - acc: 0.9128 - val_loss: 1.6116 - val_acc: 0.8496\n",
            "Epoch 48/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5490 - acc: 0.9120 - val_loss: 1.5906 - val_acc: 0.8706\n",
            "Epoch 49/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5492 - acc: 0.9116 - val_loss: 1.5767 - val_acc: 0.8839\n",
            "Epoch 50/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5460 - acc: 0.9150 - val_loss: 1.6147 - val_acc: 0.8457\n",
            "Epoch 51/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5471 - acc: 0.9137 - val_loss: 1.6084 - val_acc: 0.8521\n",
            "Epoch 52/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5443 - acc: 0.9167 - val_loss: 1.6219 - val_acc: 0.8388\n",
            "Epoch 53/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5467 - acc: 0.9141 - val_loss: 1.6115 - val_acc: 0.8491\n",
            "Epoch 54/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5439 - acc: 0.9170 - val_loss: 1.5891 - val_acc: 0.8717\n",
            "Epoch 55/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5465 - acc: 0.9147 - val_loss: 1.5949 - val_acc: 0.8659\n",
            "Epoch 56/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5457 - acc: 0.9153 - val_loss: 1.5599 - val_acc: 0.9008\n",
            "Epoch 57/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5429 - acc: 0.9181 - val_loss: 1.5665 - val_acc: 0.8943\n",
            "Epoch 58/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5452 - acc: 0.9159 - val_loss: 1.6041 - val_acc: 0.8568\n",
            "Epoch 59/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5443 - acc: 0.9167 - val_loss: 1.5835 - val_acc: 0.8773\n",
            "Epoch 60/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5481 - acc: 0.9131 - val_loss: 1.5894 - val_acc: 0.8712\n",
            "Epoch 61/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5449 - acc: 0.9162 - val_loss: 1.5993 - val_acc: 0.8617\n",
            "Epoch 62/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5467 - acc: 0.9143 - val_loss: 1.5736 - val_acc: 0.8873\n",
            "Epoch 63/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5426 - acc: 0.9185 - val_loss: 1.5611 - val_acc: 0.9000\n",
            "Epoch 64/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5485 - acc: 0.9123 - val_loss: 1.5809 - val_acc: 0.8801\n",
            "Epoch 65/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5430 - acc: 0.9178 - val_loss: 1.5751 - val_acc: 0.8857\n",
            "Epoch 66/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5482 - acc: 0.9127 - val_loss: 1.6020 - val_acc: 0.8592\n",
            "Epoch 67/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5448 - acc: 0.9161 - val_loss: 1.5640 - val_acc: 0.8972\n",
            "Epoch 68/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5459 - acc: 0.9152 - val_loss: 1.5899 - val_acc: 0.8708\n",
            "Epoch 69/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5444 - acc: 0.9166 - val_loss: 1.5860 - val_acc: 0.8750\n",
            "Epoch 70/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5446 - acc: 0.9165 - val_loss: 1.5958 - val_acc: 0.8648\n",
            "Epoch 71/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5438 - acc: 0.9172 - val_loss: 1.5608 - val_acc: 0.9002\n",
            "Epoch 72/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5481 - acc: 0.9128 - val_loss: 1.5696 - val_acc: 0.8917\n",
            "Epoch 73/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5472 - acc: 0.9138 - val_loss: 1.5856 - val_acc: 0.8748\n",
            "Epoch 74/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5408 - acc: 0.9202 - val_loss: 1.5768 - val_acc: 0.8842\n",
            "Epoch 75/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5405 - acc: 0.9206 - val_loss: 1.5749 - val_acc: 0.8863\n",
            "Epoch 76/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5393 - acc: 0.9217 - val_loss: 1.5932 - val_acc: 0.8672\n",
            "Epoch 77/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5448 - acc: 0.9162 - val_loss: 1.6020 - val_acc: 0.8588\n",
            "Epoch 78/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5458 - acc: 0.9151 - val_loss: 1.6374 - val_acc: 0.8237\n",
            "Epoch 79/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5436 - acc: 0.9175 - val_loss: 1.5894 - val_acc: 0.8715\n",
            "Epoch 80/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5418 - acc: 0.9192 - val_loss: 1.5649 - val_acc: 0.8961\n",
            "Epoch 81/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5416 - acc: 0.9195 - val_loss: 1.6097 - val_acc: 0.8509\n",
            "Epoch 82/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5425 - acc: 0.9184 - val_loss: 1.6209 - val_acc: 0.8398\n",
            "Epoch 83/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5415 - acc: 0.9195 - val_loss: 1.5704 - val_acc: 0.8902\n",
            "Epoch 84/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5417 - acc: 0.9194 - val_loss: 1.6085 - val_acc: 0.8528\n",
            "Epoch 85/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5422 - acc: 0.9187 - val_loss: 1.5682 - val_acc: 0.8928\n",
            "Epoch 86/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5459 - acc: 0.9151 - val_loss: 1.6194 - val_acc: 0.8414\n",
            "Epoch 87/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5442 - acc: 0.9169 - val_loss: 1.5682 - val_acc: 0.8926\n",
            "Epoch 88/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5382 - acc: 0.9229 - val_loss: 1.5604 - val_acc: 0.9007\n",
            "Epoch 89/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5394 - acc: 0.9218 - val_loss: 1.5718 - val_acc: 0.8890\n",
            "Epoch 90/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5438 - acc: 0.9172 - val_loss: 1.5661 - val_acc: 0.8947\n",
            "Epoch 91/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5401 - acc: 0.9209 - val_loss: 1.5636 - val_acc: 0.8970\n",
            "Epoch 92/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5426 - acc: 0.9182 - val_loss: 1.5690 - val_acc: 0.8919\n",
            "Epoch 93/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5369 - acc: 0.9241 - val_loss: 1.5698 - val_acc: 0.8908\n",
            "Epoch 94/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5406 - acc: 0.9204 - val_loss: 1.5926 - val_acc: 0.8687\n",
            "Epoch 95/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5398 - acc: 0.9215 - val_loss: 1.5557 - val_acc: 0.9054\n",
            "Epoch 96/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5448 - acc: 0.9161 - val_loss: 1.5833 - val_acc: 0.8775\n",
            "Epoch 97/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5409 - acc: 0.9202 - val_loss: 1.5758 - val_acc: 0.8854\n",
            "Epoch 98/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5379 - acc: 0.9233 - val_loss: 1.5734 - val_acc: 0.8878\n",
            "Epoch 99/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5401 - acc: 0.9209 - val_loss: 1.6008 - val_acc: 0.8605\n",
            "Epoch 100/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5407 - acc: 0.9203 - val_loss: 1.5747 - val_acc: 0.8860\n",
            "Epoch 101/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5376 - acc: 0.9235 - val_loss: 1.5678 - val_acc: 0.8932\n",
            "Epoch 102/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5428 - acc: 0.9182 - val_loss: 1.5845 - val_acc: 0.8764\n",
            "Epoch 103/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5374 - acc: 0.9238 - val_loss: 1.5717 - val_acc: 0.8895\n",
            "Epoch 104/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5431 - acc: 0.9179 - val_loss: 1.5759 - val_acc: 0.8853\n",
            "Epoch 105/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5397 - acc: 0.9214 - val_loss: 1.5877 - val_acc: 0.8733\n",
            "Epoch 106/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5404 - acc: 0.9206 - val_loss: 1.5638 - val_acc: 0.8972\n",
            "Epoch 107/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5380 - acc: 0.9230 - val_loss: 1.6171 - val_acc: 0.8442\n",
            "Epoch 108/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5418 - acc: 0.9193 - val_loss: 1.5823 - val_acc: 0.8785\n",
            "Epoch 109/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5382 - acc: 0.9229 - val_loss: 1.5492 - val_acc: 0.9118\n",
            "Epoch 110/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5362 - acc: 0.9249 - val_loss: 1.5867 - val_acc: 0.8743\n",
            "Epoch 111/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5362 - acc: 0.9248 - val_loss: 1.6283 - val_acc: 0.8325\n",
            "Epoch 112/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5408 - acc: 0.9204 - val_loss: 1.5830 - val_acc: 0.8781\n",
            "Epoch 113/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5376 - acc: 0.9235 - val_loss: 1.5534 - val_acc: 0.9078\n",
            "Epoch 114/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5376 - acc: 0.9236 - val_loss: 1.5619 - val_acc: 0.8992\n",
            "Epoch 115/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5352 - acc: 0.9258 - val_loss: 1.5808 - val_acc: 0.8805\n",
            "Epoch 116/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5366 - acc: 0.9245 - val_loss: 1.5867 - val_acc: 0.8742\n",
            "Epoch 117/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5391 - acc: 0.9220 - val_loss: 1.5920 - val_acc: 0.8690\n",
            "Epoch 118/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5386 - acc: 0.9225 - val_loss: 1.5766 - val_acc: 0.8847\n",
            "Epoch 119/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5401 - acc: 0.9210 - val_loss: 1.5780 - val_acc: 0.8830\n",
            "Epoch 120/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5389 - acc: 0.9221 - val_loss: 1.5617 - val_acc: 0.8992\n",
            "Epoch 121/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5416 - acc: 0.9195 - val_loss: 1.5904 - val_acc: 0.8707\n",
            "Epoch 122/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5368 - acc: 0.9244 - val_loss: 1.5799 - val_acc: 0.8811\n",
            "Epoch 123/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5380 - acc: 0.9230 - val_loss: 1.5547 - val_acc: 0.9063\n",
            "Epoch 124/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5354 - acc: 0.9256 - val_loss: 1.5769 - val_acc: 0.8841\n",
            "Epoch 125/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5376 - acc: 0.9234 - val_loss: 1.5705 - val_acc: 0.8905\n",
            "Epoch 126/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5392 - acc: 0.9219 - val_loss: 1.5559 - val_acc: 0.9049\n",
            "Epoch 127/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5402 - acc: 0.9209 - val_loss: 1.5580 - val_acc: 0.9029\n",
            "Epoch 128/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5345 - acc: 0.9266 - val_loss: 1.5668 - val_acc: 0.8942\n",
            "Epoch 129/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5458 - acc: 0.9154 - val_loss: 1.5738 - val_acc: 0.8871\n",
            "Epoch 130/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5363 - acc: 0.9247 - val_loss: 1.5572 - val_acc: 0.9039\n",
            "Epoch 131/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5350 - acc: 0.9261 - val_loss: 1.5625 - val_acc: 0.8986\n",
            "Epoch 132/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5420 - acc: 0.9191 - val_loss: 1.5900 - val_acc: 0.8708\n",
            "Epoch 133/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5366 - acc: 0.9245 - val_loss: 1.5537 - val_acc: 0.9073\n",
            "Epoch 134/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5359 - acc: 0.9253 - val_loss: 1.5766 - val_acc: 0.8847\n",
            "Epoch 135/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5384 - acc: 0.9227 - val_loss: 1.6753 - val_acc: 0.7857\n",
            "Epoch 136/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5376 - acc: 0.9235 - val_loss: 1.5607 - val_acc: 0.9003\n",
            "Epoch 137/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5321 - acc: 0.9290 - val_loss: 1.5759 - val_acc: 0.8850\n",
            "Epoch 138/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5362 - acc: 0.9248 - val_loss: 1.5851 - val_acc: 0.8758\n",
            "Epoch 139/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5370 - acc: 0.9241 - val_loss: 1.5561 - val_acc: 0.9050\n",
            "Epoch 140/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5414 - acc: 0.9196 - val_loss: 1.5635 - val_acc: 0.8974\n",
            "Epoch 141/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5439 - acc: 0.9172 - val_loss: 1.5688 - val_acc: 0.8923\n",
            "Epoch 142/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5374 - acc: 0.9237 - val_loss: 1.5542 - val_acc: 0.9068\n",
            "Epoch 143/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5394 - acc: 0.9216 - val_loss: 1.5794 - val_acc: 0.8817\n",
            "Epoch 144/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5410 - acc: 0.9201 - val_loss: 1.5594 - val_acc: 0.9016\n",
            "Epoch 145/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5363 - acc: 0.9249 - val_loss: 1.5520 - val_acc: 0.9092\n",
            "Epoch 146/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5381 - acc: 0.9231 - val_loss: 1.5571 - val_acc: 0.9038\n",
            "Epoch 147/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5381 - acc: 0.9230 - val_loss: 1.5842 - val_acc: 0.8767\n",
            "Epoch 148/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5384 - acc: 0.9226 - val_loss: 1.5551 - val_acc: 0.9062\n",
            "Epoch 149/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5418 - acc: 0.9193 - val_loss: 1.5517 - val_acc: 0.9093\n",
            "Epoch 150/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5432 - acc: 0.9179 - val_loss: 1.5991 - val_acc: 0.8619\n",
            "Epoch 151/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5404 - acc: 0.9206 - val_loss: 1.5792 - val_acc: 0.8817\n",
            "Epoch 152/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5527 - acc: 0.9085 - val_loss: 1.5693 - val_acc: 0.8920\n",
            "Epoch 153/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5416 - acc: 0.9195 - val_loss: 1.5607 - val_acc: 0.9003\n",
            "Epoch 154/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5365 - acc: 0.9246 - val_loss: 1.5606 - val_acc: 0.9006\n",
            "Epoch 155/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5327 - acc: 0.9284 - val_loss: 1.5661 - val_acc: 0.8951\n",
            "Epoch 156/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5363 - acc: 0.9248 - val_loss: 1.5747 - val_acc: 0.8863\n",
            "Epoch 157/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5372 - acc: 0.9239 - val_loss: 1.5547 - val_acc: 0.9066\n",
            "Epoch 158/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5343 - acc: 0.9268 - val_loss: 1.5624 - val_acc: 0.8988\n",
            "Epoch 159/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5342 - acc: 0.9269 - val_loss: 1.5837 - val_acc: 0.8772\n",
            "Epoch 160/500\n",
            "94/94 [==============================] - 22s 229ms/step - loss: 1.5356 - acc: 0.9255 - val_loss: 1.5763 - val_acc: 0.8847\n",
            "Epoch 161/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5378 - acc: 0.9233 - val_loss: 1.5526 - val_acc: 0.9085\n",
            "Epoch 162/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5328 - acc: 0.9283 - val_loss: 1.5630 - val_acc: 0.8982\n",
            "Epoch 163/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5389 - acc: 0.9222 - val_loss: 1.5804 - val_acc: 0.8808\n",
            "Epoch 164/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5400 - acc: 0.9211 - val_loss: 1.5891 - val_acc: 0.8719\n",
            "Epoch 165/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5412 - acc: 0.9199 - val_loss: 1.5618 - val_acc: 0.8991\n",
            "Epoch 166/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5400 - acc: 0.9211 - val_loss: 1.5585 - val_acc: 0.9028\n",
            "Epoch 167/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5361 - acc: 0.9250 - val_loss: 1.5692 - val_acc: 0.8920\n",
            "Epoch 168/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5357 - acc: 0.9254 - val_loss: 1.5630 - val_acc: 0.8982\n",
            "Epoch 169/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5397 - acc: 0.9214 - val_loss: 1.5530 - val_acc: 0.9082\n",
            "Epoch 170/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5417 - acc: 0.9194 - val_loss: 1.5649 - val_acc: 0.8961\n",
            "Epoch 171/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5356 - acc: 0.9255 - val_loss: 1.5558 - val_acc: 0.9051\n",
            "Epoch 172/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5357 - acc: 0.9255 - val_loss: 1.5665 - val_acc: 0.8944\n",
            "Epoch 173/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5372 - acc: 0.9240 - val_loss: 1.5736 - val_acc: 0.8875\n",
            "Epoch 174/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5384 - acc: 0.9227 - val_loss: 1.5531 - val_acc: 0.9079\n",
            "Epoch 175/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5362 - acc: 0.9250 - val_loss: 1.5683 - val_acc: 0.8928\n",
            "Epoch 176/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5359 - acc: 0.9252 - val_loss: 1.5550 - val_acc: 0.9060\n",
            "Epoch 177/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5371 - acc: 0.9240 - val_loss: 1.5522 - val_acc: 0.9088\n",
            "Epoch 178/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5331 - acc: 0.9280 - val_loss: 1.5669 - val_acc: 0.8940\n",
            "Epoch 179/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5321 - acc: 0.9290 - val_loss: 1.5491 - val_acc: 0.9120\n",
            "Epoch 180/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5328 - acc: 0.9283 - val_loss: 1.5733 - val_acc: 0.8877\n",
            "Epoch 181/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5354 - acc: 0.9257 - val_loss: 1.5568 - val_acc: 0.9045\n",
            "Epoch 182/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5345 - acc: 0.9266 - val_loss: 1.5831 - val_acc: 0.8781\n",
            "Epoch 183/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5363 - acc: 0.9248 - val_loss: 1.5502 - val_acc: 0.9108\n",
            "Epoch 184/500\n",
            "94/94 [==============================] - 22s 233ms/step - loss: 1.5314 - acc: 0.9298 - val_loss: 1.5601 - val_acc: 0.9008\n",
            "Epoch 185/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5335 - acc: 0.9276 - val_loss: 1.5594 - val_acc: 0.9017\n",
            "Epoch 186/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5356 - acc: 0.9254 - val_loss: 1.5527 - val_acc: 0.9086\n",
            "Epoch 187/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5311 - acc: 0.9300 - val_loss: 1.5634 - val_acc: 0.8976\n",
            "Epoch 188/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5344 - acc: 0.9267 - val_loss: 1.5757 - val_acc: 0.8854\n",
            "Epoch 189/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5419 - acc: 0.9192 - val_loss: 1.5564 - val_acc: 0.9048\n",
            "Epoch 190/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5419 - acc: 0.9193 - val_loss: 1.5555 - val_acc: 0.9059\n",
            "Epoch 191/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5362 - acc: 0.9250 - val_loss: 1.5619 - val_acc: 0.8992\n",
            "Epoch 192/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5314 - acc: 0.9296 - val_loss: 1.5641 - val_acc: 0.8969\n",
            "Epoch 193/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5330 - acc: 0.9282 - val_loss: 1.5670 - val_acc: 0.8938\n",
            "Epoch 194/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5321 - acc: 0.9290 - val_loss: 1.5686 - val_acc: 0.8926\n",
            "Epoch 195/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5409 - acc: 0.9202 - val_loss: 1.5508 - val_acc: 0.9103\n",
            "Epoch 196/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5395 - acc: 0.9216 - val_loss: 1.5643 - val_acc: 0.8967\n",
            "Epoch 197/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5334 - acc: 0.9277 - val_loss: 1.5924 - val_acc: 0.8687\n",
            "Epoch 198/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5366 - acc: 0.9245 - val_loss: 1.5570 - val_acc: 0.9043\n",
            "Epoch 199/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5353 - acc: 0.9258 - val_loss: 1.5615 - val_acc: 0.8995\n",
            "Epoch 200/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5332 - acc: 0.9278 - val_loss: 1.5589 - val_acc: 0.9022\n",
            "Epoch 201/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5347 - acc: 0.9263 - val_loss: 1.5611 - val_acc: 0.8999\n",
            "Epoch 202/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5353 - acc: 0.9258 - val_loss: 1.5589 - val_acc: 0.9022\n",
            "Epoch 203/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5343 - acc: 0.9268 - val_loss: 1.5602 - val_acc: 0.9008\n",
            "Epoch 204/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5330 - acc: 0.9281 - val_loss: 1.6007 - val_acc: 0.8604\n",
            "Epoch 205/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5338 - acc: 0.9273 - val_loss: 1.5483 - val_acc: 0.9128\n",
            "Epoch 206/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5334 - acc: 0.9277 - val_loss: 1.5616 - val_acc: 0.8995\n",
            "Epoch 207/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5373 - acc: 0.9237 - val_loss: 1.5769 - val_acc: 0.8842\n",
            "Epoch 208/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5343 - acc: 0.9267 - val_loss: 1.5616 - val_acc: 0.8993\n",
            "Epoch 209/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5339 - acc: 0.9272 - val_loss: 1.5485 - val_acc: 0.9127\n",
            "Epoch 210/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5329 - acc: 0.9281 - val_loss: 1.6060 - val_acc: 0.8552\n",
            "Epoch 211/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5384 - acc: 0.9227 - val_loss: 1.5573 - val_acc: 0.9039\n",
            "Epoch 212/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5390 - acc: 0.9221 - val_loss: 1.6052 - val_acc: 0.8558\n",
            "Epoch 213/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5401 - acc: 0.9210 - val_loss: 1.5604 - val_acc: 0.9007\n",
            "Epoch 214/500\n",
            "94/94 [==============================] - 22s 232ms/step - loss: 1.5374 - acc: 0.9237 - val_loss: 1.5487 - val_acc: 0.9124\n",
            "Epoch 215/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5347 - acc: 0.9264 - val_loss: 1.5680 - val_acc: 0.8930\n",
            "Epoch 216/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5357 - acc: 0.9254 - val_loss: 1.5642 - val_acc: 0.8972\n",
            "Epoch 217/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5315 - acc: 0.9296 - val_loss: 1.5511 - val_acc: 0.9100\n",
            "Epoch 218/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5311 - acc: 0.9300 - val_loss: 1.5595 - val_acc: 0.9016\n",
            "Epoch 219/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5354 - acc: 0.9258 - val_loss: 1.5595 - val_acc: 0.9016\n",
            "Epoch 220/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5345 - acc: 0.9266 - val_loss: 1.5650 - val_acc: 0.8959\n",
            "Epoch 221/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5350 - acc: 0.9261 - val_loss: 1.5763 - val_acc: 0.8847\n",
            "Epoch 222/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5366 - acc: 0.9245 - val_loss: 1.5494 - val_acc: 0.9117\n",
            "Epoch 223/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5311 - acc: 0.9299 - val_loss: 1.5814 - val_acc: 0.8798\n",
            "Epoch 224/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5364 - acc: 0.9246 - val_loss: 1.5502 - val_acc: 0.9109\n",
            "Epoch 225/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5366 - acc: 0.9245 - val_loss: 1.5651 - val_acc: 0.8960\n",
            "Epoch 226/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5312 - acc: 0.9299 - val_loss: 1.5483 - val_acc: 0.9130\n",
            "Epoch 227/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5337 - acc: 0.9274 - val_loss: 1.5496 - val_acc: 0.9114\n",
            "Epoch 228/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5325 - acc: 0.9285 - val_loss: 1.5755 - val_acc: 0.8854\n",
            "Epoch 229/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5298 - acc: 0.9313 - val_loss: 1.5483 - val_acc: 0.9128\n",
            "Epoch 230/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5297 - acc: 0.9314 - val_loss: 1.5469 - val_acc: 0.9143\n",
            "Epoch 231/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5310 - acc: 0.9301 - val_loss: 1.5497 - val_acc: 0.9112\n",
            "Epoch 232/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5306 - acc: 0.9305 - val_loss: 1.5577 - val_acc: 0.9035\n",
            "Epoch 233/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5397 - acc: 0.9215 - val_loss: 1.5793 - val_acc: 0.8817\n",
            "Epoch 234/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5356 - acc: 0.9255 - val_loss: 1.5711 - val_acc: 0.8899\n",
            "Epoch 235/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5334 - acc: 0.9276 - val_loss: 1.5571 - val_acc: 0.9039\n",
            "Epoch 236/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5335 - acc: 0.9277 - val_loss: 1.5605 - val_acc: 0.9007\n",
            "Epoch 237/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5306 - acc: 0.9305 - val_loss: 1.5649 - val_acc: 0.8962\n",
            "Epoch 238/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5311 - acc: 0.9300 - val_loss: 1.5705 - val_acc: 0.8907\n",
            "Epoch 239/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5371 - acc: 0.9240 - val_loss: 1.5513 - val_acc: 0.9097\n",
            "Epoch 240/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5353 - acc: 0.9258 - val_loss: 1.5536 - val_acc: 0.9076\n",
            "Epoch 241/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5323 - acc: 0.9289 - val_loss: 1.5622 - val_acc: 0.8988\n",
            "Epoch 242/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5307 - acc: 0.9305 - val_loss: 1.5473 - val_acc: 0.9139\n",
            "Epoch 243/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5336 - acc: 0.9275 - val_loss: 1.5528 - val_acc: 0.9084\n",
            "Epoch 244/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5324 - acc: 0.9289 - val_loss: 1.5752 - val_acc: 0.8857\n",
            "Epoch 245/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5381 - acc: 0.9230 - val_loss: 1.5855 - val_acc: 0.8756\n",
            "Epoch 246/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5326 - acc: 0.9285 - val_loss: 1.5573 - val_acc: 0.9039\n",
            "Epoch 247/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5293 - acc: 0.9318 - val_loss: 1.5896 - val_acc: 0.8715\n",
            "Epoch 248/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5293 - acc: 0.9318 - val_loss: 1.5486 - val_acc: 0.9122\n",
            "Epoch 249/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5273 - acc: 0.9338 - val_loss: 1.5549 - val_acc: 0.9062\n",
            "Epoch 250/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5361 - acc: 0.9251 - val_loss: 1.5605 - val_acc: 0.9006\n",
            "Epoch 251/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5347 - acc: 0.9264 - val_loss: 1.5680 - val_acc: 0.8931\n",
            "Epoch 252/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5358 - acc: 0.9254 - val_loss: 1.5526 - val_acc: 0.9085\n",
            "Epoch 253/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5320 - acc: 0.9291 - val_loss: 1.5450 - val_acc: 0.9162\n",
            "Epoch 254/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5325 - acc: 0.9286 - val_loss: 1.5562 - val_acc: 0.9049\n",
            "Epoch 255/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5323 - acc: 0.9289 - val_loss: 1.5635 - val_acc: 0.8976\n",
            "Epoch 256/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5305 - acc: 0.9306 - val_loss: 1.5551 - val_acc: 0.9059\n",
            "Epoch 257/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5338 - acc: 0.9272 - val_loss: 1.5499 - val_acc: 0.9111\n",
            "Epoch 258/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5330 - acc: 0.9281 - val_loss: 1.5527 - val_acc: 0.9085\n",
            "Epoch 259/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5359 - acc: 0.9252 - val_loss: 1.5603 - val_acc: 0.9008\n",
            "Epoch 260/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5342 - acc: 0.9269 - val_loss: 1.5510 - val_acc: 0.9101\n",
            "Epoch 261/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5313 - acc: 0.9298 - val_loss: 1.5848 - val_acc: 0.8761\n",
            "Epoch 262/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5332 - acc: 0.9279 - val_loss: 1.5673 - val_acc: 0.8939\n",
            "Epoch 263/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5318 - acc: 0.9293 - val_loss: 1.5573 - val_acc: 0.9038\n",
            "Epoch 264/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5294 - acc: 0.9318 - val_loss: 1.5490 - val_acc: 0.9121\n",
            "Epoch 265/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5297 - acc: 0.9314 - val_loss: 1.5504 - val_acc: 0.9105\n",
            "Epoch 266/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5289 - acc: 0.9323 - val_loss: 1.5767 - val_acc: 0.8844\n",
            "Epoch 267/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5357 - acc: 0.9254 - val_loss: 1.5633 - val_acc: 0.8978\n",
            "Epoch 268/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5317 - acc: 0.9294 - val_loss: 1.5675 - val_acc: 0.8933\n",
            "Epoch 269/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5320 - acc: 0.9291 - val_loss: 1.5569 - val_acc: 0.9042\n",
            "Epoch 270/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5306 - acc: 0.9305 - val_loss: 1.5510 - val_acc: 0.9100\n",
            "Epoch 271/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5294 - acc: 0.9317 - val_loss: 1.5535 - val_acc: 0.9075\n",
            "Epoch 272/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5319 - acc: 0.9292 - val_loss: 1.5482 - val_acc: 0.9129\n",
            "Epoch 273/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5294 - acc: 0.9317 - val_loss: 1.5582 - val_acc: 0.9028\n",
            "Epoch 274/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5333 - acc: 0.9278 - val_loss: 1.5502 - val_acc: 0.9111\n",
            "Epoch 275/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5282 - acc: 0.9329 - val_loss: 1.5577 - val_acc: 0.9034\n",
            "Epoch 276/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5346 - acc: 0.9265 - val_loss: 1.5566 - val_acc: 0.9045\n",
            "Epoch 277/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5301 - acc: 0.9311 - val_loss: 1.5570 - val_acc: 0.9041\n",
            "Epoch 278/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5345 - acc: 0.9265 - val_loss: 1.5456 - val_acc: 0.9155\n",
            "Epoch 279/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5290 - acc: 0.9321 - val_loss: 1.5557 - val_acc: 0.9054\n",
            "Epoch 280/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5302 - acc: 0.9309 - val_loss: 1.5572 - val_acc: 0.9038\n",
            "Epoch 281/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5285 - acc: 0.9327 - val_loss: 1.5681 - val_acc: 0.8929\n",
            "Epoch 282/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5296 - acc: 0.9316 - val_loss: 1.5598 - val_acc: 0.9013\n",
            "Epoch 283/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5307 - acc: 0.9304 - val_loss: 1.5494 - val_acc: 0.9117\n",
            "Epoch 284/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5331 - acc: 0.9280 - val_loss: 1.5657 - val_acc: 0.8954\n",
            "Epoch 285/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5365 - acc: 0.9245 - val_loss: 1.5575 - val_acc: 0.9037\n",
            "Epoch 286/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5290 - acc: 0.9321 - val_loss: 1.5652 - val_acc: 0.8958\n",
            "Epoch 287/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5305 - acc: 0.9306 - val_loss: 1.5493 - val_acc: 0.9118\n",
            "Epoch 288/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5290 - acc: 0.9320 - val_loss: 1.5597 - val_acc: 0.9014\n",
            "Epoch 289/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5302 - acc: 0.9309 - val_loss: 1.5604 - val_acc: 0.9007\n",
            "Epoch 290/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5303 - acc: 0.9308 - val_loss: 1.5786 - val_acc: 0.8825\n",
            "Epoch 291/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5316 - acc: 0.9295 - val_loss: 1.5589 - val_acc: 0.9022\n",
            "Epoch 292/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5275 - acc: 0.9337 - val_loss: 1.5722 - val_acc: 0.8888\n",
            "Epoch 293/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5362 - acc: 0.9249 - val_loss: 1.5520 - val_acc: 0.9090\n",
            "Epoch 294/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5332 - acc: 0.9280 - val_loss: 1.5616 - val_acc: 0.8995\n",
            "Epoch 295/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5295 - acc: 0.9316 - val_loss: 1.5624 - val_acc: 0.8988\n",
            "Epoch 296/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5285 - acc: 0.9327 - val_loss: 1.5563 - val_acc: 0.9047\n",
            "Epoch 297/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5247 - acc: 0.9365 - val_loss: 1.5448 - val_acc: 0.9164\n",
            "Epoch 298/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5291 - acc: 0.9321 - val_loss: 1.5677 - val_acc: 0.8933\n",
            "Epoch 299/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5319 - acc: 0.9293 - val_loss: 1.5671 - val_acc: 0.8939\n",
            "Epoch 300/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5307 - acc: 0.9304 - val_loss: 1.5679 - val_acc: 0.8932\n",
            "Epoch 301/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5314 - acc: 0.9296 - val_loss: 1.5521 - val_acc: 0.9090\n",
            "Epoch 302/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5269 - acc: 0.9342 - val_loss: 1.5677 - val_acc: 0.8935\n",
            "Epoch 303/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5324 - acc: 0.9287 - val_loss: 1.5444 - val_acc: 0.9168\n",
            "Epoch 304/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5324 - acc: 0.9287 - val_loss: 1.5708 - val_acc: 0.8903\n",
            "Epoch 305/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5326 - acc: 0.9285 - val_loss: 1.5654 - val_acc: 0.8956\n",
            "Epoch 306/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5325 - acc: 0.9286 - val_loss: 1.5674 - val_acc: 0.8938\n",
            "Epoch 307/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5314 - acc: 0.9297 - val_loss: 1.5646 - val_acc: 0.8963\n",
            "Epoch 308/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5278 - acc: 0.9333 - val_loss: 1.5564 - val_acc: 0.9046\n",
            "Epoch 309/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5286 - acc: 0.9326 - val_loss: 1.5606 - val_acc: 0.9003\n",
            "Epoch 310/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5292 - acc: 0.9319 - val_loss: 1.5519 - val_acc: 0.9093\n",
            "Epoch 311/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5305 - acc: 0.9306 - val_loss: 1.5566 - val_acc: 0.9046\n",
            "Epoch 312/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5334 - acc: 0.9277 - val_loss: 1.5475 - val_acc: 0.9137\n",
            "Epoch 313/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5268 - acc: 0.9344 - val_loss: 1.5443 - val_acc: 0.9168\n",
            "Epoch 314/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5264 - acc: 0.9347 - val_loss: 1.5550 - val_acc: 0.9062\n",
            "Epoch 315/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5287 - acc: 0.9324 - val_loss: 1.5557 - val_acc: 0.9055\n",
            "Epoch 316/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5275 - acc: 0.9336 - val_loss: 1.5476 - val_acc: 0.9136\n",
            "Epoch 317/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5289 - acc: 0.9322 - val_loss: 1.5513 - val_acc: 0.9100\n",
            "Epoch 318/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5312 - acc: 0.9300 - val_loss: 1.5719 - val_acc: 0.8892\n",
            "Epoch 319/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5291 - acc: 0.9320 - val_loss: 1.5468 - val_acc: 0.9143\n",
            "Epoch 320/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5264 - acc: 0.9346 - val_loss: 1.5472 - val_acc: 0.9140\n",
            "Epoch 321/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5280 - acc: 0.9331 - val_loss: 1.5644 - val_acc: 0.8967\n",
            "Epoch 322/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5308 - acc: 0.9304 - val_loss: 1.5703 - val_acc: 0.8910\n",
            "Epoch 323/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5276 - acc: 0.9335 - val_loss: 1.5566 - val_acc: 0.9045\n",
            "Epoch 324/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5314 - acc: 0.9297 - val_loss: 1.5561 - val_acc: 0.9051\n",
            "Epoch 325/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5319 - acc: 0.9293 - val_loss: 1.5510 - val_acc: 0.9101\n",
            "Epoch 326/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5284 - acc: 0.9328 - val_loss: 1.5650 - val_acc: 0.8962\n",
            "Epoch 327/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5371 - acc: 0.9240 - val_loss: 1.5559 - val_acc: 0.9051\n",
            "Epoch 328/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5286 - acc: 0.9326 - val_loss: 1.5570 - val_acc: 0.9040\n",
            "Epoch 329/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5273 - acc: 0.9338 - val_loss: 1.5726 - val_acc: 0.8886\n",
            "Epoch 330/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5327 - acc: 0.9284 - val_loss: 1.5548 - val_acc: 0.9063\n",
            "Epoch 331/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5334 - acc: 0.9277 - val_loss: 1.5618 - val_acc: 0.8992\n",
            "Epoch 332/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5290 - acc: 0.9322 - val_loss: 1.5636 - val_acc: 0.8977\n",
            "Epoch 333/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5278 - acc: 0.9333 - val_loss: 1.5571 - val_acc: 0.9040\n",
            "Epoch 334/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5260 - acc: 0.9352 - val_loss: 1.5478 - val_acc: 0.9134\n",
            "Epoch 335/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5288 - acc: 0.9324 - val_loss: 1.5573 - val_acc: 0.9039\n",
            "Epoch 336/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5283 - acc: 0.9328 - val_loss: 1.5477 - val_acc: 0.9134\n",
            "Epoch 337/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5294 - acc: 0.9317 - val_loss: 1.5537 - val_acc: 0.9077\n",
            "Epoch 338/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5285 - acc: 0.9326 - val_loss: 1.5457 - val_acc: 0.9153\n",
            "Epoch 339/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5289 - acc: 0.9322 - val_loss: 1.5486 - val_acc: 0.9126\n",
            "Epoch 340/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5303 - acc: 0.9308 - val_loss: 1.5598 - val_acc: 0.9013\n",
            "Epoch 341/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5326 - acc: 0.9286 - val_loss: 1.5630 - val_acc: 0.8982\n",
            "Epoch 342/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5261 - acc: 0.9350 - val_loss: 1.5639 - val_acc: 0.8972\n",
            "Epoch 343/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5292 - acc: 0.9319 - val_loss: 1.5790 - val_acc: 0.8822\n",
            "Epoch 344/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5257 - acc: 0.9354 - val_loss: 1.5588 - val_acc: 0.9023\n",
            "Epoch 345/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5253 - acc: 0.9358 - val_loss: 1.5499 - val_acc: 0.9113\n",
            "Epoch 346/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5244 - acc: 0.9367 - val_loss: 1.5437 - val_acc: 0.9174\n",
            "Epoch 347/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5278 - acc: 0.9333 - val_loss: 1.5432 - val_acc: 0.9178\n",
            "Epoch 348/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5236 - acc: 0.9375 - val_loss: 1.5500 - val_acc: 0.9110\n",
            "Epoch 349/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5322 - acc: 0.9289 - val_loss: 1.5560 - val_acc: 0.9051\n",
            "Epoch 350/500\n",
            "94/94 [==============================] - 22s 233ms/step - loss: 1.5275 - acc: 0.9337 - val_loss: 1.5546 - val_acc: 0.9063\n",
            "Epoch 351/500\n",
            "94/94 [==============================] - 22s 235ms/step - loss: 1.5274 - acc: 0.9337 - val_loss: 1.5598 - val_acc: 0.9012\n",
            "Epoch 352/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5308 - acc: 0.9303 - val_loss: 1.5614 - val_acc: 0.8997\n",
            "Epoch 353/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5269 - acc: 0.9342 - val_loss: 1.5528 - val_acc: 0.9083\n",
            "Epoch 354/500\n",
            "94/94 [==============================] - 22s 235ms/step - loss: 1.5239 - acc: 0.9373 - val_loss: 1.5554 - val_acc: 0.9057\n",
            "Epoch 355/500\n",
            "94/94 [==============================] - 22s 232ms/step - loss: 1.5249 - acc: 0.9362 - val_loss: 1.5626 - val_acc: 0.8987\n",
            "Epoch 356/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5255 - acc: 0.9356 - val_loss: 1.5578 - val_acc: 0.9033\n",
            "Epoch 357/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5234 - acc: 0.9377 - val_loss: 1.5481 - val_acc: 0.9130\n",
            "Epoch 358/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5284 - acc: 0.9327 - val_loss: 1.5611 - val_acc: 0.9000\n",
            "Epoch 359/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5271 - acc: 0.9341 - val_loss: 1.5482 - val_acc: 0.9131\n",
            "Epoch 360/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5269 - acc: 0.9342 - val_loss: 1.5639 - val_acc: 0.8972\n",
            "Epoch 361/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5267 - acc: 0.9345 - val_loss: 1.5484 - val_acc: 0.9128\n",
            "Epoch 362/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5256 - acc: 0.9354 - val_loss: 1.5560 - val_acc: 0.9050\n",
            "Epoch 363/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5310 - acc: 0.9301 - val_loss: 1.5488 - val_acc: 0.9124\n",
            "Epoch 364/500\n",
            "94/94 [==============================] - 22s 233ms/step - loss: 1.5278 - acc: 0.9333 - val_loss: 1.5556 - val_acc: 0.9055\n",
            "Epoch 365/500\n",
            "94/94 [==============================] - 22s 232ms/step - loss: 1.5253 - acc: 0.9358 - val_loss: 1.5500 - val_acc: 0.9111\n",
            "Epoch 366/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5254 - acc: 0.9358 - val_loss: 1.5478 - val_acc: 0.9133\n",
            "Epoch 367/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5251 - acc: 0.9360 - val_loss: 1.5442 - val_acc: 0.9169\n",
            "Epoch 368/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5228 - acc: 0.9383 - val_loss: 1.5514 - val_acc: 0.9097\n",
            "Epoch 369/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5289 - acc: 0.9321 - val_loss: 1.5442 - val_acc: 0.9169\n",
            "Epoch 370/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5273 - acc: 0.9338 - val_loss: 1.5483 - val_acc: 0.9129\n",
            "Epoch 371/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5279 - acc: 0.9333 - val_loss: 1.5430 - val_acc: 0.9182\n",
            "Epoch 372/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5266 - acc: 0.9345 - val_loss: 1.5487 - val_acc: 0.9122\n",
            "Epoch 373/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5245 - acc: 0.9366 - val_loss: 1.5543 - val_acc: 0.9068\n",
            "Epoch 374/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5255 - acc: 0.9356 - val_loss: 1.5511 - val_acc: 0.9099\n",
            "Epoch 375/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5288 - acc: 0.9323 - val_loss: 1.5583 - val_acc: 0.9030\n",
            "Epoch 376/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5239 - acc: 0.9373 - val_loss: 1.5435 - val_acc: 0.9176\n",
            "Epoch 377/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5253 - acc: 0.9359 - val_loss: 1.5526 - val_acc: 0.9084\n",
            "Epoch 378/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5331 - acc: 0.9280 - val_loss: 1.5512 - val_acc: 0.9100\n",
            "Epoch 379/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5276 - acc: 0.9336 - val_loss: 1.5508 - val_acc: 0.9103\n",
            "Epoch 380/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5294 - acc: 0.9317 - val_loss: 1.5472 - val_acc: 0.9137\n",
            "Epoch 381/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5336 - acc: 0.9275 - val_loss: 1.5550 - val_acc: 0.9062\n",
            "Epoch 382/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5247 - acc: 0.9364 - val_loss: 1.5483 - val_acc: 0.9128\n",
            "Epoch 383/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5241 - acc: 0.9370 - val_loss: 1.5453 - val_acc: 0.9159\n",
            "Epoch 384/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5273 - acc: 0.9339 - val_loss: 1.5594 - val_acc: 0.9018\n",
            "Epoch 385/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5288 - acc: 0.9323 - val_loss: 1.5553 - val_acc: 0.9060\n",
            "Epoch 386/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5315 - acc: 0.9296 - val_loss: 1.5856 - val_acc: 0.8755\n",
            "Epoch 387/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5233 - acc: 0.9379 - val_loss: 1.5430 - val_acc: 0.9180\n",
            "Epoch 388/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5262 - acc: 0.9350 - val_loss: 1.5467 - val_acc: 0.9144\n",
            "Epoch 389/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5271 - acc: 0.9340 - val_loss: 1.5497 - val_acc: 0.9114\n",
            "Epoch 390/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5306 - acc: 0.9306 - val_loss: 1.5463 - val_acc: 0.9148\n",
            "Epoch 391/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5246 - acc: 0.9364 - val_loss: 1.5500 - val_acc: 0.9111\n",
            "Epoch 392/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5242 - acc: 0.9369 - val_loss: 1.5486 - val_acc: 0.9126\n",
            "Epoch 393/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5232 - acc: 0.9378 - val_loss: 1.5493 - val_acc: 0.9116\n",
            "Epoch 394/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5281 - acc: 0.9330 - val_loss: 1.5609 - val_acc: 0.9002\n",
            "Epoch 395/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5260 - acc: 0.9351 - val_loss: 1.5573 - val_acc: 0.9038\n",
            "Epoch 396/500\n",
            "94/94 [==============================] - 22s 232ms/step - loss: 1.5305 - acc: 0.9306 - val_loss: 1.5565 - val_acc: 0.9047\n",
            "Epoch 397/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5261 - acc: 0.9350 - val_loss: 1.5716 - val_acc: 0.8894\n",
            "Epoch 398/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5273 - acc: 0.9338 - val_loss: 1.5559 - val_acc: 0.9053\n",
            "Epoch 399/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5268 - acc: 0.9343 - val_loss: 1.5464 - val_acc: 0.9147\n",
            "Epoch 400/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5275 - acc: 0.9336 - val_loss: 1.5508 - val_acc: 0.9105\n",
            "Epoch 401/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5243 - acc: 0.9368 - val_loss: 1.5499 - val_acc: 0.9114\n",
            "Epoch 402/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5310 - acc: 0.9301 - val_loss: 1.5587 - val_acc: 0.9023\n",
            "Epoch 403/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5264 - acc: 0.9346 - val_loss: 1.5461 - val_acc: 0.9151\n",
            "Epoch 404/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5238 - acc: 0.9373 - val_loss: 1.5613 - val_acc: 0.8999\n",
            "Epoch 405/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5240 - acc: 0.9371 - val_loss: 1.5460 - val_acc: 0.9149\n",
            "Epoch 406/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5213 - acc: 0.9398 - val_loss: 1.5562 - val_acc: 0.9049\n",
            "Epoch 407/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5218 - acc: 0.9393 - val_loss: 1.5472 - val_acc: 0.9140\n",
            "Epoch 408/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5270 - acc: 0.9341 - val_loss: 1.5570 - val_acc: 0.9042\n",
            "Epoch 409/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5305 - acc: 0.9306 - val_loss: 1.5480 - val_acc: 0.9131\n",
            "Epoch 410/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5293 - acc: 0.9318 - val_loss: 1.5485 - val_acc: 0.9126\n",
            "Epoch 411/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5260 - acc: 0.9351 - val_loss: 1.5444 - val_acc: 0.9169\n",
            "Epoch 412/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5277 - acc: 0.9334 - val_loss: 1.5554 - val_acc: 0.9060\n",
            "Epoch 413/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5284 - acc: 0.9327 - val_loss: 1.5657 - val_acc: 0.8956\n",
            "Epoch 414/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5235 - acc: 0.9376 - val_loss: 1.5495 - val_acc: 0.9116\n",
            "Epoch 415/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5240 - acc: 0.9371 - val_loss: 1.5522 - val_acc: 0.9089\n",
            "Epoch 416/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5212 - acc: 0.9399 - val_loss: 1.5441 - val_acc: 0.9169\n",
            "Epoch 417/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5212 - acc: 0.9399 - val_loss: 1.5539 - val_acc: 0.9072\n",
            "Epoch 418/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5281 - acc: 0.9330 - val_loss: 1.5506 - val_acc: 0.9106\n",
            "Epoch 419/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5271 - acc: 0.9340 - val_loss: 1.5442 - val_acc: 0.9169\n",
            "Epoch 420/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5239 - acc: 0.9372 - val_loss: 1.5417 - val_acc: 0.9196\n",
            "Epoch 421/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5234 - acc: 0.9377 - val_loss: 1.5528 - val_acc: 0.9083\n",
            "Epoch 422/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5223 - acc: 0.9388 - val_loss: 1.5503 - val_acc: 0.9107\n",
            "Epoch 423/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5457 - val_acc: 0.9154\n",
            "Epoch 424/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5255 - acc: 0.9356 - val_loss: 1.5621 - val_acc: 0.8990\n",
            "Epoch 425/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5243 - acc: 0.9369 - val_loss: 1.5500 - val_acc: 0.9112\n",
            "Epoch 426/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5238 - acc: 0.9374 - val_loss: 1.5593 - val_acc: 0.9018\n",
            "Epoch 427/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5240 - acc: 0.9371 - val_loss: 1.5501 - val_acc: 0.9111\n",
            "Epoch 428/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5232 - acc: 0.9380 - val_loss: 1.5534 - val_acc: 0.9076\n",
            "Epoch 429/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5250 - acc: 0.9361 - val_loss: 1.5568 - val_acc: 0.9043\n",
            "Epoch 430/500\n",
            "94/94 [==============================] - 22s 233ms/step - loss: 1.5261 - acc: 0.9350 - val_loss: 1.5613 - val_acc: 0.8998\n",
            "Epoch 431/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5280 - acc: 0.9331 - val_loss: 1.5534 - val_acc: 0.9077\n",
            "Epoch 432/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5276 - acc: 0.9335 - val_loss: 1.5498 - val_acc: 0.9112\n",
            "Epoch 433/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5248 - acc: 0.9362 - val_loss: 1.5447 - val_acc: 0.9163\n",
            "Epoch 434/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5232 - acc: 0.9380 - val_loss: 1.5597 - val_acc: 0.9014\n",
            "Epoch 435/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5269 - acc: 0.9343 - val_loss: 1.5785 - val_acc: 0.8827\n",
            "Epoch 436/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5316 - acc: 0.9295 - val_loss: 1.5479 - val_acc: 0.9133\n",
            "Epoch 437/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5225 - acc: 0.9387 - val_loss: 1.5473 - val_acc: 0.9137\n",
            "Epoch 438/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5230 - acc: 0.9381 - val_loss: 1.5520 - val_acc: 0.9090\n",
            "Epoch 439/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5255 - acc: 0.9356 - val_loss: 1.5491 - val_acc: 0.9122\n",
            "Epoch 440/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5234 - acc: 0.9378 - val_loss: 1.5496 - val_acc: 0.9115\n",
            "Epoch 441/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5247 - acc: 0.9364 - val_loss: 1.5463 - val_acc: 0.9147\n",
            "Epoch 442/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5212 - acc: 0.9399 - val_loss: 1.5481 - val_acc: 0.9129\n",
            "Epoch 443/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5192 - acc: 0.9420 - val_loss: 1.5417 - val_acc: 0.9193\n",
            "Epoch 444/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5201 - acc: 0.9411 - val_loss: 1.5425 - val_acc: 0.9186\n",
            "Epoch 445/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5221 - acc: 0.9391 - val_loss: 1.5417 - val_acc: 0.9193\n",
            "Epoch 446/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5208 - acc: 0.9403 - val_loss: 1.5429 - val_acc: 0.9182\n",
            "Epoch 447/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5216 - acc: 0.9396 - val_loss: 1.5596 - val_acc: 0.9016\n",
            "Epoch 448/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5235 - acc: 0.9376 - val_loss: 1.5573 - val_acc: 0.9038\n",
            "Epoch 449/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5256 - acc: 0.9356 - val_loss: 1.5497 - val_acc: 0.9114\n",
            "Epoch 450/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5238 - acc: 0.9373 - val_loss: 1.5533 - val_acc: 0.9079\n",
            "Epoch 451/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5225 - acc: 0.9386 - val_loss: 1.5490 - val_acc: 0.9120\n",
            "Epoch 452/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5228 - acc: 0.9384 - val_loss: 1.5528 - val_acc: 0.9082\n",
            "Epoch 453/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5216 - acc: 0.9395 - val_loss: 1.5452 - val_acc: 0.9159\n",
            "Epoch 454/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5214 - acc: 0.9398 - val_loss: 1.5425 - val_acc: 0.9187\n",
            "Epoch 455/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5224 - acc: 0.9387 - val_loss: 1.5496 - val_acc: 0.9114\n",
            "Epoch 456/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5250 - acc: 0.9361 - val_loss: 1.5558 - val_acc: 0.9053\n",
            "Epoch 457/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5260 - acc: 0.9351 - val_loss: 1.5536 - val_acc: 0.9076\n",
            "Epoch 458/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5228 - acc: 0.9383 - val_loss: 1.5448 - val_acc: 0.9163\n",
            "Epoch 459/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5207 - acc: 0.9404 - val_loss: 1.5513 - val_acc: 0.9097\n",
            "Epoch 460/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5203 - acc: 0.9408 - val_loss: 1.5489 - val_acc: 0.9122\n",
            "Epoch 461/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5224 - acc: 0.9388 - val_loss: 1.5562 - val_acc: 0.9049\n",
            "Epoch 462/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5226 - acc: 0.9385 - val_loss: 1.5577 - val_acc: 0.9036\n",
            "Epoch 463/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5236 - acc: 0.9375 - val_loss: 1.5542 - val_acc: 0.9069\n",
            "Epoch 464/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5215 - acc: 0.9397 - val_loss: 1.5600 - val_acc: 0.9012\n",
            "Epoch 465/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5218 - acc: 0.9394 - val_loss: 1.5585 - val_acc: 0.9026\n",
            "Epoch 466/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5245 - acc: 0.9365 - val_loss: 1.5519 - val_acc: 0.9093\n",
            "Epoch 467/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5247 - acc: 0.9365 - val_loss: 1.5505 - val_acc: 0.9107\n",
            "Epoch 468/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5275 - acc: 0.9336 - val_loss: 1.5669 - val_acc: 0.8942\n",
            "Epoch 469/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5247 - acc: 0.9364 - val_loss: 1.5504 - val_acc: 0.9107\n",
            "Epoch 470/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5250 - acc: 0.9362 - val_loss: 1.5541 - val_acc: 0.9071\n",
            "Epoch 471/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5365 - acc: 0.9246 - val_loss: 1.5689 - val_acc: 0.8920\n",
            "Epoch 472/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5321 - acc: 0.9291 - val_loss: 1.5756 - val_acc: 0.8855\n",
            "Epoch 473/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5265 - acc: 0.9346 - val_loss: 1.5649 - val_acc: 0.8962\n",
            "Epoch 474/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5306 - acc: 0.9306 - val_loss: 1.5490 - val_acc: 0.9121\n",
            "Epoch 475/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5254 - acc: 0.9357 - val_loss: 1.5577 - val_acc: 0.9034\n",
            "Epoch 476/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5316 - acc: 0.9295 - val_loss: 1.5527 - val_acc: 0.9083\n",
            "Epoch 477/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5262 - acc: 0.9349 - val_loss: 1.5509 - val_acc: 0.9102\n",
            "Epoch 478/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5283 - acc: 0.9329 - val_loss: 1.5587 - val_acc: 0.9023\n",
            "Epoch 479/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5274 - acc: 0.9337 - val_loss: 1.5538 - val_acc: 0.9073\n",
            "Epoch 480/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5247 - acc: 0.9364 - val_loss: 1.5464 - val_acc: 0.9147\n",
            "Epoch 481/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5242 - acc: 0.9369 - val_loss: 1.5568 - val_acc: 0.9043\n",
            "Epoch 482/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5244 - acc: 0.9367 - val_loss: 1.5460 - val_acc: 0.9153\n",
            "Epoch 483/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5255 - acc: 0.9357 - val_loss: 1.5582 - val_acc: 0.9029\n",
            "Epoch 484/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5190 - acc: 0.9421 - val_loss: 1.5524 - val_acc: 0.9087\n",
            "Epoch 485/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5247 - acc: 0.9365 - val_loss: 1.5604 - val_acc: 0.9007\n",
            "Epoch 486/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5222 - acc: 0.9389 - val_loss: 1.5446 - val_acc: 0.9165\n",
            "Epoch 487/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5210 - acc: 0.9402 - val_loss: 1.5473 - val_acc: 0.9138\n",
            "Epoch 488/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5210 - acc: 0.9401 - val_loss: 1.5484 - val_acc: 0.9128\n",
            "Epoch 489/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5226 - acc: 0.9385 - val_loss: 1.5432 - val_acc: 0.9179\n",
            "Epoch 490/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5241 - acc: 0.9371 - val_loss: 1.5435 - val_acc: 0.9178\n",
            "Epoch 491/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5200 - acc: 0.9411 - val_loss: 1.5485 - val_acc: 0.9126\n",
            "Epoch 492/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5220 - acc: 0.9391 - val_loss: 1.5466 - val_acc: 0.9146\n",
            "Epoch 493/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5228 - acc: 0.9383 - val_loss: 1.5459 - val_acc: 0.9152\n",
            "Epoch 494/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5202 - acc: 0.9409 - val_loss: 1.5450 - val_acc: 0.9161\n",
            "Epoch 495/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5179 - acc: 0.9433 - val_loss: 1.5432 - val_acc: 0.9180\n",
            "Epoch 496/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5219 - acc: 0.9392 - val_loss: 1.5494 - val_acc: 0.9118\n",
            "Epoch 497/500\n",
            "94/94 [==============================] - 22s 232ms/step - loss: 1.5204 - acc: 0.9408 - val_loss: 1.5631 - val_acc: 0.8981\n",
            "Epoch 498/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5209 - acc: 0.9402 - val_loss: 1.5436 - val_acc: 0.9175\n",
            "Epoch 499/500\n",
            "94/94 [==============================] - 22s 231ms/step - loss: 1.5188 - acc: 0.9423 - val_loss: 1.5466 - val_acc: 0.9147\n",
            "Epoch 500/500\n",
            "94/94 [==============================] - 22s 230ms/step - loss: 1.5192 - acc: 0.9419 - val_loss: 1.5434 - val_acc: 0.9178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5l2PjBPUvVJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I27AdU-kyQ_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "74f3ffbb-3f7d-4244-e5e9-bdaaa10b9163"
      },
      "source": [
        "evaluation = model.evaluate( x_test, y_test )\n",
        "print( f'loss: {evaluation[0]:.2f}, acc: {evaluation[1]*100:.2f}%' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 1.5467 - acc: 0.9144\n",
            "loss: 1.55, acc: 91.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GOphUGLyQ_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save( model_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0WnDgZSyQ_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b2133378-98dd-45fd-8592-37e423818874"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot( history.history['acc'] )\n",
        "plt.plot( history.history['val_acc'])\n",
        "plt.xlabel( 'epochs' )\n",
        "plt.ylabel( 'acc' )\n",
        "plt.legend( ['acc', 'val_acc'] )\n",
        "plt.title( model_name )\n",
        "\n",
        "plt.savefig( model_name + '.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}